[
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/index.htm", "title": "Data Structures and Algorithms (DSA) Tutorial", "content": "Data structures and algorithms (DSA) are two important aspects of any programming language. Every programming language has its own data structures and different types of algorithms to handle these data structures.\nData Structures are used to organise and store data to use it in an effective way when performing data operations.\nAlgorithm is a step-by-step procedure, which defines a set of instructions to be executed in a certain order to get the desired output. Algorithms are generally created independent of underlying languages, i.e. an algorithm can be implemented in more than one programming language.\nAlmost every enterprise application uses various types of data structures in one or the other way. So, as a programmer, data structures and algorithms are really important aspects of day-to-day programming.\nThis tutorial will give you a great understanding on Data Structures needed to understand the complexity of enterprise level applications and need of algorithms, and data structures.\nAs applications are getting complex and data rich, there are three common problems that applications face now-a-days.\n − Consider an inventory of 1 million(10\n) items of a store. If the application is to search an item, it has to search an item in 1 million(10\n) items every time slowing down the search. As data grows, search will become slower.\n − Processor speed although being very high, falls limited if the data grows to billion records.\n − As thousands of users can search data simultaneously on a web server, even the fast server fails while searching the data.\nTo solve the above-mentioned problems, data structures come to rescue. Data can be organized in a data structure in such a way that all items may not be required to be searched, and the required data can be searched almost instantly.\nThe basic steps to learn DSA is as follows:\nTime and Space complexities are the measures of the amount of time required to execute the code (Time Complexity) and amount of space required to execute the code (Space Complexity).\nHere we learn different types of data structures like Array, Stack, Queye, Linked List et.\nOnce you have good undertanding about various data sturtcures then you can start learning associated algorithms to process the data stored in these data structures. These algorithms include searching, sorting, and other different algorithms.\nFrom the data structure point of view, following are some important categories of algorithms −\n − Algorithm to search an item in a data structure.\n − Algorithm to sort items in a certain order.\n − Algorithm to insert item in a data structure.\n −  Algorithm to update an existing item in a data structure.\n − Algorithm to delete an existing item from a data structure.\nThe following computer problems can be solved using Data Structures −\nThis tutorial has been designed for Computer Science Students as well as Software Professionals who are willing to learn data Structures and Algorithm (DSA) Programming in simple and easy steps.\nAfter completing this tutorial you will be at intermediate level of expertise from where you can take yourself to higher level of expertise.\nIn this tutorial, we will work with data structures and algorithms in four different programming languages: C, C++, Java, Python. So, we provide Online Compilers for each of these languages to execute the given code. Doing so, we are aiming to compromise the need for local setup for the compilers.\nBefore proceeding with this tutorial, you should have a basic understanding of C programming language, text editor, and execution of programs, etc.\nThis Data Structures Algorithms tutorial helps you prepare for technical interviews and certification exams. We have provided various quizzes and assignments to check your learning level. Given quizzes have multiple choice type of questions and their answers with short explanation.\nFollowing is a sample quiz, try to attempt any of the given answers:\n - A complete graph can have \nAt maximum, a complete graph can have n\n spanning trees. \nStart your online quiz  \n.\nProfessionals in DSA are in high demands as more and more organizations rely on them to solve complex problems and make data-driven decisions. You can earn competitive salaries, and the specific pay can vary based on your location, experience, and job role.\nMany top companies are actively recruiting experts in DSA, and they offer roles such as Software Engineer, Data Scientist, Machine Learning Engineer, and more. These companies need individuals who can solve complex problems, analyse data, and create algorithms to drive their business forward. Here is the list of few such companies −\nThese are just a few examples, and the demand for DSA professionals is continually growing across various sectors. By developing expertise in these areas, you can open up a wide range of career opportunities in some of the world's leading companies.\nTo get started, there are user-friendly tutorials and resources available to help you master DSA. These materials are designed to prepare you for technical interviews and certification exams, and you can learn at your own pace, anytime and anywhere.\nThere are many Frequently Asked Questions (FAQs) on Data Structures and Algorithms due to the complex nature of this concept. In this section, we will try to answer some of them briefly.\nA data structure is a collection of similar or different data types, and is used to store and modify data using programming languages. And, an algorithm is defined as a set of instructions that must be followed to solve a problem.\nData Structures and Algorithms is a study of such data structures and the algorithms that use them.\nThe best programming language to work with data structures is C++, due to its efficiency and abundant resources for data structures. Despite that, any programming language can be the best pick to work with data structures if you are fluent in it.\nHere are the summarized list of tips which you can follow to start learning Data Structures.\nA datatype is a type of value a variable holds. These values can be numeric, string, characters, etc. An array is defined as a collection of similar type of values stored together. Hence, it is more likely to be a data structures storing values of same datatype.\nData Structures organize the data used in algorithms. They are the foundation of computations performed using algorithms. Hence, learning data structures is recommended first, as it becomes easier to understand the concept of Algorithms with all the prior knowledge.\nNot only in software development, but we can observe the use of data structures in our day-to-day life as well. For instance, piling up plates and removing them one-by-one is the simpler example on how stack data structure organizes its data. Similarly, queueing up to buy movie tickets has the same mechanism as inserting and deleting the data from a queue.\nIn software development, developing navigation maps using graph data structure is also a common real life application.\nMachine Learning and Deep Learning work with mathematical computations and large sets of data. Organizing this data properly becomes crucial in order to process these data-sets for training and deploying suitable models on them. Hence, having a deep knowledge in Data Structures and Algorithms is important while working with Machine Learning and Deep Learning.\nA datatype defines the type of value stored in a variable. This decides the type of operations performed and functions called on these values. Whereas, a data structure is a collection of similar or different types of data, which is used to organize and manipulate data in a program.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/floyd_warshall_algorithm.htm", "title": "Floyd Warshall Algorithm", "content": "The Floyd-Warshall algorithm is a graph algorithm that is deployed to find the shortest path between all the vertices present in a weighted graph. This algorithm is different from other shortest path algorithms; to describe it simply, this algorithm uses each vertex in the graph as a pivot to check if it provides the shortest way to travel from one point to another.\nFloyd-Warshall algorithm works on both directed and undirected weighted graphs unless these graphs do not contain any negative cycles in them. By negative cycles, it is meant that the sum of all the edges in the graph must not lead to a negative number.\nSince, the algorithm deals with overlapping sub-problems  the path found by the vertices acting as pivot are stored for solving the next steps  it uses the dynamic programming approach.\nFloyd-Warshall algorithm is one of the methods in All-pairs shortest path algorithms and it is solved using the Adjacency Matrix representation of graphs.\nConsider a graph, \n where \n is the set of all vertices present in the graph and E is the set of all the edges in the graph. The graph, \n, is represented in the form of an adjacency matrix, \n, that contains all the weights of every edge connecting two vertices.\n − Construct an adjacency matrix \n with all the costs of edges present in the graph. If there is no path between two vertices, mark the value as ∞.\n − Derive another adjacency matrix \n from \n keeping the first row and first column of the original adjacency matrix intact in \n. And for the remaining values, say \n, if \n then replace \n with \n. Otherwise, do not change the values. Here, in this step, \n (first vertex acting as pivot).\n − Repeat \n for all the vertices in the graph by changing the \n value for every pivot vertex until the final matrix is achieved.\n − The final adjacency matrix obtained is the final solution with all the shortest paths.\nConsider the following directed weighted graph \n. Find the shortest paths between all the vertices of the graphs using the Floyd-Warshall algorithm.\nConstruct an adjacency matrix \n with all the distances as values.\n$$A=\\begin{matrix}\n0 &  5&  \\infty &  6&  \\infty \\\\\n\\infty &  0&  1&  \\infty&  7\\\\\n3 &  \\infty&  0&  4&  \\infty\\\\\n\\infty &  \\infty&  2&  0&  3\\\\\n 2&  \\infty&  \\infty&  5&  0\\\\\n\\end{matrix}$$\nConsidering the above adjacency matrix as the input, derive another matrix A\n by keeping only first rows and columns intact. Take \n, and replace all the other values by \n.\n$$A=\\begin{matrix}\n0 &  5&  \\infty &  6&  \\infty \\\\\n\\infty &  &  &  &  \\\\\n3&  &  &  &  \\\\\n\\infty&  &  &  &  \\\\\n2&  &  &  &  \\\\\n\\end{matrix}$$\n$$A_{1}=\\begin{matrix}\n0 &  5&  \\infty &  6&  \\infty \\\\\n\\infty &  0&  1&  \\infty&  7\\\\\n3 &  8&  0&  4&  \\infty\\\\\n\\infty &  \\infty&  2&  0&  3\\\\\n 2&  7&  \\infty&  5&  0\\\\\n\\end{matrix}$$\nConsidering the above adjacency matrix as the input, derive another matrix A\n by keeping only first rows and columns intact. Take \n, and replace all the other values by \n.\n$$A_{2}=\\begin{matrix}\n &  5&  &  &  \\\\\n\\infty &  0&  1&  \\infty&  7\\\\\n &  8&  &  &  \\\\\n &  \\infty&  &  &  \\\\\n &  7&  &  &  \\\\\n\\end{matrix}$$\n$$A_{2}=\\begin{matrix}\n0 &  5&  6&  6& 12 \\\\\n\\infty  &  0&  1&  \\infty&  7\\\\\n3 &  8&  0&  4&  15\\\\\n\\infty &  \\infty&  2&  0&  3\\\\\n2 &  7&  8&  5& 0 \\\\\n\\end{matrix}$$\nConsidering the above adjacency matrix as the input, derive another matrix \n by keeping only first rows and columns intact. Take \n, and replace all the other values by \n.\n$$A_{3}=\\begin{matrix}\n &  &  6&  &  \\\\\n &  &  1&  &  \\\\\n3 &  8&  0&  4&  15\\\\\n &  &  2&  &  \\\\\n &  &  8&  &  \\\\\n\\end{matrix}$$\n$$A_{3}=\\begin{matrix}\n0 &  5&  6&  6& 12 \\\\\n4  &  0&  1&  5&  7\\\\\n3 &  8&  0&  4&  15\\\\\n5 &  10&  2&  0&  3\\\\\n2 &  7&  8&  5& 0 \\\\\n\\end{matrix}$$\nConsidering the above adjacency matrix as the input, derive another matrix \n by keeping only first rows and columns intact. Take \n, and replace all the other values by \n.\n$$A_{4}=\\begin{matrix}\n& & &   6&  \\\\\n& & &   5&  \\\\\n& & &   4&  \\\\\n5 &  10&  2&  0&  3\\\\\n& & &  5&  \\\\\n\\end{matrix}$$\n$$A_{4}=\\begin{matrix}\n0 &  5&  6&  6& 9 \\\\\n4  &  0&  1&  5&  7\\\\\n3 &  8&  0&  4&  7\\\\\n5 &  10&  2&  0&  3\\\\\n2 &  7&  7&  5& 0 \\\\\n\\end{matrix}$$\nConsidering the above adjacency matrix as the input, derive another matrix \n by keeping only first rows and columns intact. Take \n, and replace all the other values by \n.\n$$A_{5}=\\begin{matrix}\n& & & & 9 \\\\\n& & & &  7\\\\ \n& & & &  7\\\\\n& & & &  3\\\\\n2 &  7&  7&  5& 0 \\\\\n\\end{matrix}$$\n$$A_{5}=\\begin{matrix}\n0 &  5&  6&  6& 9 \\\\\n4  &  0&  1&  5&  7\\\\\n3 &  8&  0&  4&  7\\\\\n5 &  10&  2&  0&  3\\\\\n2 &  7&  7&  5& 0 \\\\\n\\end{matrix}$$\nFrom the pseudocode above, the Floyd-Warshall algorithm operates using three for loops to find the shortest distance between all pairs of vertices within a graph. Therefore, the \n of the Floyd-Warshall algorithm is \n, where n is the number of vertices in the graph. The \n of the algorithm is \n.\nFollowing is the implementation of Floyd Warshall Algorithm to find the shortest path in a graph using cost adjacency matrix -\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/matrix_chain_multiplication.htm", "title": "Matrix Chain Multiplication Algorithm", "content": "Matrix Chain Multiplication is an algorithm that is applied to determine the lowest cost way for multiplying matrices. The actual multiplication is done using the standard way of multiplying the matrices, i.e., it follows the basic rule that the number of rows in one matrix must be equal to the number of columns in another matrix. Hence, multiple scalar multiplications must be done to achieve the product.\nTo brief it further, consider matrices A, B, C, and D, to be multiplied; hence, the multiplication is done using the standard matrix multiplication. There are multiple combinations of the matrices found while using the standard approach since matrix multiplication is associative. For instance, there are five ways to multiply the four matrices given above −\n(A(B(CD)))\n(A((BC)D))\n((AB)(CD))\n((A(BC))D)\n(((AB)C)D)\nNow, if the size of matrices A, B, C, and D are \n respectively, then the number of scalar multiplications performed will be \n. But the cost of the matrices change based on the rows and columns present in it. Suppose, the values of l, m, n, p, q are 5, 10, 15, 20, 25 respectively, the cost of (A(B(CD))) is 5  100  25 = 12,500; however, the cost of (A((BC)D)) is 10  25  37 = 9,250.\nSo, dynamic programming approach of the matrix chain multiplication is adopted in order to find the combination with the lowest cost.\nMatrix chain multiplication algorithm is only applied to find the minimum cost way to multiply a sequence of matrices. Therefore, the input taken by the algorithm is the sequence of matrices while the output achieved is the lowest cost parenthesization.\nCount the number of parenthesizations. Find the number of ways in which the input matrices can be multiplied using the formulae −\n$$P(n)=\\left\\{\\begin{matrix}\n1 &  if\\: n=1\\\\\n\\sum_{k=1}^{n-1} P(k)P(n-k)&  if\\: n\\geq 2\\\\\n\\end{matrix}\\right.$$\n$$P(n)=\\left\\{\\begin{matrix}\n\\frac{2(n-1)C_{n-1}}{n} & if\\: n\\geq 2 \\\\\n1 & if\\: n= 1\\\\\n\\end{matrix}\\right.$$\nOnce the parenthesization is done, the optimal substructure must be devised as the first step of dynamic programming approach so the final product achieved is optimal. In matrix chain multiplication, the optimal substructure is found by dividing the sequence of matrices \n into two parts \n. It must be ensured that the parts are divided in such a way that optimal solution is achieved.\nUsing the formula, $C[i,j]=\\left\\{\\begin{matrix}\n0 &  if \\: i=j\\\\\n\\displaystyle \\min_{ i\\leq k< j}\\begin{cases}\nC [i,k]+C[k+1,j]+d_{i-1}d_{k}d_{j}\n\\end{cases} &if \\: i< j  \\\\\n\\end{matrix}\\right.$  find the lowest cost parenthesization of the sequence of matrices by constructing cost tables and corresponding \n values table.\nOnce the lowest cost is found, print the corresponding parenthesization as the output.\nPseudocode to find the lowest cost of all the possible parenthesizations −\nPseudocode to print the optimal output parenthesization −\nThe application of dynamic programming formula is slightly different from the theory; to understand it better let us look at few examples below.\nA sequence of matrices A, B, C, D with dimensions 5  10, 10  15, 15  20, 20  25 are set to be multiplied. Find the lowest cost parenthesization to multiply the given matrices using matrix chain multiplication.\nGiven matrices and their corresponding dimensions are −\nFind the count of parenthesization of the 4 matrices, i.e. n = 4.\nUsing the formula, $P\\left ( n \\right )=\\left\\{\\begin{matrix}\n1 &  if\\: n=1\\\\\n\\sum_{k=1}^{n-1}P(k)P(n-k) & if\\: n\\geq 2  \\\\\n\\end{matrix}\\right.$\nSince n = 4  2, apply the second case of the formula −\n$$P\\left ( n \\right )=\\sum_{k=1}^{n-1}P(k)P(n-k)$$\n$$P\\left ( 4 \\right )=\\sum_{k=1}^{3}P(k)P(4-k)$$\n$$P\\left ( 4 \\right )=P(1)P(3)+P(2)P(2)+P(3)P(1)$$\nIf P(1) = 1 and P(2) is also equal to 1, P(4) will be calculated based on the P(3) value. Therefore, P(3) needs to determined first.\n$$P\\left ( 3 \\right )=P(1)P(2)+P(2)P(1)$$\n$$=1+1=2$$\nTherefore,\n$$P\\left ( 4 \\right )=P(1)P(3)+P(2)P(2)+P(3)P(1)$$\n$$=2+1+2=5$$\nAmong these 5 combinations of parenthesis, the matrix chain multiplicatiion algorithm must find the lowest cost parenthesis.\nThe table above is known as a \n, where all the cost values calculated from the different combinations of parenthesis are stored.\nAnother table is also created to store the \n values obtained at the minimum cost of each combination.\nApplying the dynamic programming approach formula find the costs of various parenthesizations,\n$$C[i,j]=\\left\\{\\begin{matrix}\n0 &  if \\: i=j\\\\\n\\displaystyle \\min_{ i\\leq k< j}\\begin{cases}\nC [i,k]+C\\left [ k+1,j \\right ]+d_{i-1}d_{k}d_{j}\n\\end{cases} &if \\: i< j  \\\\\n\\end{matrix}\\right.$$\n$C\\left [ 1,1 \\right ]=0$\n$C\\left [ 2,2 \\right ]=0$\n$C\\left [ 3,3 \\right ]=0$\n$C\\left [ 4,4 \\right ]=0$\nApplying the dynamic approach formula only in the upper triangular values of the cost table, since i < j always.\n$C[1,2]=\\displaystyle \\min_{ 1\\leq k< 2}\\begin{Bmatrix}\nC[1,1]+C[2,2]+d_{0}d_{1}d_{2} \\end{Bmatrix}$\n$C[1,2]=0+0+\\left ( 5\\times 10\\times 15 \\right )$\n$C[1,2]=750$\n$C[2,3]=\\displaystyle \\min_{ 2\\leq k< 3}\\begin{Bmatrix}\nC[2,2]+C[3,3]+d_{1}d_{2}d_{3} \\end{Bmatrix}$\n$C[2,3]=0+0+\\left ( 10\\times 15\\times 20 \\right )$\n$C[2,3]=3000$\n$C[3,4]=\\displaystyle \\min_{ 3\\leq k< 4}\\begin{Bmatrix}\nC[3,3]+C[4,4]+d_{2}d_{3}d_{4} \\end{Bmatrix}$\n$C[3,4]=0+0+\\left ( 15\\times 20\\times 25 \\right )$\n$C[3,4]=7500$\nFind the values of [1, 3] and [2, 4] in this step. The cost table is always filled diagonally step-wise.\n$C[2,4]=\\displaystyle \\min_{ 2\\leq k< 4}\\begin{Bmatrix}\nC[2,2]+C[3,4]+d_{1}d_{2}d_{4},C[2,3] +C[4,4]+d_{1}d_{3}d_{4}\\end{Bmatrix}$\n$C[2,4]=\\displaystyle min\\left\\{ ( 0 + 7500 + (10 \\times  15 \\times 20)), (3000 + 5000)\\right\\}$\n$C[2,4]=8000$\n$C[1,3]=\\displaystyle \\min_{ 1\\leq k< 3}\\begin{Bmatrix}\nC[1,1]+C[2,3]+d_{0}d_{1}d_{3},C[1,2] +C[3,3]+d_{0}d_{2}d_{3}\\end{Bmatrix}$\n$C[1,3]=min\\left\\{ ( 0 + 3000 + 1000), (1500+0+750)\\right\\}$\n$C[1,3]=2250$\nNow compute the final element of the cost table to compare the lowest cost parenthesization.\n$C[1,4]=\\displaystyle \\min_{ 1\\leq k< 4}\\begin{Bmatrix}\nC[1,1]+C[2,4]+d_{0}d_{1}d_{4},C[1,2] +C[3,4]+d_{1}d_{2}d_{4},C[1,3]+C[4,4] +d_{1}d_{3}d_{4}\\end{Bmatrix}$\n$C[1,4]=min\\left\\{0+8000+1250,750+7500+1875,2200+0+2500\\right\\}$\n$C[1,4]=4700$\nNow that all the values in cost table are computed, the final step is to parethesize the sequence of matrices. For that, \n table needs to be constructed with the minimum value of k corresponding to every parenthesis.\nBased on the lowest cost values from the cost table and their corresponding k values, let us add parenthesis on the sequence of matrices.\nThe lowest cost value at [1, 4] is achieved when k = 3, therefore, the first parenthesization must be done at 3.\nThe lowest cost value at [1, 3] is achieved when k = 2, therefore the next parenthesization is done at 2.\nThe lowest cost value at [1, 2] is achieved when k = 1, therefore the next parenthesization is done at 1. But the parenthesization needs at least two matrices to be multiplied so we do not divide further.\nSince, the sequence cannot be parenthesized further, the final solution of matrix chain multiplication is ((AB)C)(D).\nFollowing is the final implementation of Matrix Chain Multiplication Algorithm to calculate the minimum number of ways several matrices can be multiplied using dynamic programming −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/dynamic_programming.htm", "title": "Dynamic Programming", "content": "Dynamic programming approach is similar to divide and conquer in breaking down the problem into smaller and yet smaller possible sub-problems. But unlike divide and conquer, these sub-problems are not solved independently. Rather, results of these smaller sub-problems are remembered and used for similar or overlapping sub-problems.\nMostly, dynamic programming algorithms are used for solving optimization problems. Before solving the in-hand sub-problem, dynamic algorithm will try to examine the results of the previously solved sub-problems. The solutions of sub-problems are combined in order to achieve the best optimal final solution. This paradigm is thus said to be using Bottom-up approach.\nSo we can conclude that −\nThe problem should be able to be divided into smaller overlapping sub-problem.\nFinal optimum solution can be achieved by using an optimum solution of smaller sub-problems.\nDynamic algorithms use memorization.\nHowever, in a problem, two main properties can suggest that the given problem can be solved using Dynamic Programming. They are −\nSimilar to Divide-and-Conquer approach, Dynamic Programming also combines solutions to sub-problems. It is mainly used where the solution of one sub-problem is needed repeatedly. The computed solutions are stored in a table, so that these dont have to be re-computed. Hence, this technique is needed where overlapping sub-problem exists.\nFor example, Binary Search does not have overlapping sub-problem. Whereas recursive program of Fibonacci numbers have many overlapping sub-problems.\nA given problem has Optimal Substructure Property, if the optimal solution of the given problem can be obtained using optimal solutions of its sub-problems.\nFor example, the Shortest Path problem has the following optimal substructure property −\nIf a node x lies in the shortest path from a source node \n to destination node \n, then the shortest path from \n to \n is the combination of the shortest path from \n to x, and the shortest path from x to \n.\nThe standard All Pair Shortest Path algorithms like Floyd-Warshall and Bellman-Ford are typical examples of Dynamic Programming.\nDynamic Programming algorithm is designed using the following four steps −\nCharacterize the structure of an optimal solution.\nRecursively define the value of an optimal solution.\nCompute the value of an optimal solution, typically in a bottom-up fashion.\nConstruct an optimal solution from the computed information.\nIn contrast to greedy algorithms, where local optimization is addressed, dynamic algorithms are motivated for an overall optimization of the problem.\nIn contrast to divide and conquer algorithms, where solutions are combined to achieve an overall solution, dynamic algorithms use the output of a smaller sub-problem and then try to optimize a bigger sub-problem. Dynamic algorithms use memorization to remember the output of already solved sub-problems.\nThe following computer problems can be solved using dynamic programming approach −\nDynamic programming can be used in both top-down and bottom-up manner. And of course, most of the times, referring to the previous solution output is cheaper than re-computing in terms of CPU cycles.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/optimal_merge_pattern_algorithm.htm", "title": "Optimal Merge Pattern Algorithm", "content": "Merge a set of sorted files of different length into a single sorted file. We need to find an optimal solution, where the resultant file will be generated in minimum time.\nIf the number of sorted files are given, there are many ways to merge them into a single sorted file. This merge can be performed pair wise. Hence, this type of merging is called as \n.\nAs, different pairings require different amounts of time, in this strategy we want to determine an optimal way of merging many files together. At each step, two shortest sequences are merged.\nTo merge a \n and a \n requires possibly \n record moves, the obvious choice being, merge the two smallest files together at each step.\nTwo-way merge patterns can be represented by binary merge trees. Let us consider a set of \n sorted files \n. Initially, each element of this is considered as a single node binary tree. To find this optimal solution, the following algorithm is used.\nFollowing is the pseudocode of the Optimal Merge Pattern Algorithm −\nAt the end of this algorithm, the weight of the root node represents the optimal cost.\nLet us consider the given files, f\n, f\n, f\n, f\n and f\n with 20, 30, 10, 5 and 30 number of elements respectively.\nIf merge operations are performed according to the provided sequence, then\n   =>  20 + 30 = 50\n   =>  50 + 10 = 60\n   =>  60 +  5 = 65\n   =>  65 + 30 = 95\nHence, the total number of operations is\n50 + 60 + 65 + 95 = 270\nNow, the question arises is there any better solution?\nSorting the numbers according to their size in an ascending order, we get the following sequence −\nHence, merge operations can be performed on this sequence\n   =>   5 + 10 = 15\n   =>  15 + 20 = 35\n   =>  35 + 30 = 65\n   =>  65 + 30 = 95\nTherefore, the total number of operations is\n15 + 35 + 65 + 95 = 210\nObviously, this is better than the previous one.\nIn this context, we are now going to solve the problem using this algorithm.\nHence, the solution takes 15 + 35 + 60 + 95 = 205 number of comparisons.\nFollowing are the implementations of the above approach in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/job_sequencing_with_deadline.htm", "title": "Job Sequencing with Deadline", "content": "Job scheduling algorithm is applied to schedule the jobs on a single processor to maximize the profits.\nThe greedy approach of the job scheduling algorithm states that, Given n number of jobs with a starting time and ending time, they need to be scheduled in such a way that maximum profit is received within the maximum deadline.\nSet of jobs with deadlines and profits are taken as an input with the job scheduling algorithm and scheduled subset of jobs with maximum profit are obtained as the final output.\nConsider the following tasks with their deadlines and profits. Schedule the tasks in such a way that they produce maximum profit after being executed −\nFind the maximum deadline value, dm, from the deadlines given.\nArrange the jobs in descending order of their profits.\nThe maximum deadline, d\n, is 4. Therefore, all the tasks must end before 4.\nChoose the job with highest profit, J4. It takes up 3 parts of the maximum deadline.\nTherefore, the next job must have the time period 1.\nTotal Profit = 100.\nThe next job with highest profit is J5. But the time taken by J5 is 4, which exceeds the deadline by 3. Therefore, it cannot be added to the output set.\nThe next job with highest profit is J2. The time taken by J5 is 2, which also exceeds the deadline by 1. Therefore, it cannot be added to the output set.\nThe next job with higher profit is J3. The time taken by J3 is 1, which does not exceed the given deadline. Therefore, J3 is added to the output set.\nSince, the maximum deadline is met, the algorithm comes to an end. The output set of jobs scheduled within the deadline are \n with the maximum profit of \n.\nFollowing is the final implementation of Job sequencing Algorithm using Greedy Approach −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/fractional_knapsack_problem.htm", "title": "Fractional Knapsack Problem", "content": "The knapsack problem states that − given a set of items, holding weights and profit values, one must determine the subset of the items to be added in a knapsack such that, the total weight of the items must not exceed the limit of the knapsack and its total profit value is maximum.\nIt is one of the most popular problems that take greedy approach to be solved. It is called as the \n.\nTo explain this problem a little easier, consider a test with 12 questions, 10 marks each, out of which only 10 should be attempted to get the maximum mark of 100. The test taker now must calculate the highest profitable questions  the one that hes confident in  to achieve the maximum mark. However, he cannot attempt all the 12 questions since there will not be any extra marks awarded for those attempted answers. This is the most basic real-world application of the knapsack problem.\nThe weights (Wi) and profit values (Pi) of the items to be added in the knapsack are taken as an input for the fractional knapsack algorithm and the subset of the items added in the knapsack without exceeding the limit and with maximum profit is achieved as the output.\nConsider all the items with their weights and profits mentioned respectively.\nCalculate P\n/W\n of all the items and sort the items in descending order based on their P\n/W\n values.\nWithout exceeding the limit, add the items into the knapsack.\nIf the knapsack can still store some weight, but the weights of other items exceed the limit, the fractional part of the next time can be added.\nHence, giving it the name fractional knapsack problem.\nFor the given set of items and the knapsack capacity of 10 kg, find the subset of the items to be added in the knapsack such that the profit is maximum.\nGiven, n = 5\nCalculate P\n/W\n for all the items\nArrange all the items in descending order based on P\n/W\nWithout exceeding the knapsack capacity, insert the items in the knapsack with maximum profit.\nHowever, the knapsack can still hold 4 kg weight, but the next item having 5 kg weight will exceed the capacity. Therefore, only 4 kg weight of the 5 kg will be added in the knapsack.\nHence, the knapsack holds the weights = [(1 * 1) + (1 * 3) + (1 * 2) + (4/5 * 5)] = 10, with maximum profit of [(1 * 8) + (1 * 15) + (1 * 10) + (4/5 * 20)] = 37.\nFollowing is the final implementation of Fractional Knapsack Algorithm using Greedy Approach −\nFew of the many real-world applications of the knapsack problem are −\nCutting raw materials without losing too much material\nPicking through the investments and portfolios\nSelecting assets of asset-backed securitization\nGenerating keys for the Merkle-Hellman algorithm\nCognitive Radio Networks\nPower Allocation\nNetwork selection for mobile nodes\nCooperative wireless communication\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/map_colouring_algorithm.htm", "title": "Map Colouring Algorithm", "content": "Map colouring problem states that given a graph G {V, E} where V and E are the set of vertices and edges of the graph, all vertices in V need to be coloured in such a way that no two adjacent vertices must have the same colour.\nThe real-world applications of this algorithm are  assigning mobile radio frequencies, making schedules, designing Sudoku, allocating registers etc.\nWith the map colouring algorithm, a graph G and the colours to be added to the graph are taken as an input and a coloured graph with no two adjacent vertices having the same colour is achieved.\nInitiate all the vertices in the graph.\nSelect the node with the highest degree to colour it with any colour.\nChoose the colour to be used on the graph with the help of the selection colour function so that no adjacent vertex is having the same colour.\nCheck if the colour can be added and if it does, add it to the solution set.\nRepeat the process from step 2 until the output set is ready.\nFind degrees of all the vertices −\nChoose the vertex with the highest degree to colour first, i.e., A and choose a colour using selection colour function. Check if the colour can be added to the vertex and if yes, add it to the solution set.\nSelect any vertex with the next highest degree from the remaining vertices and colour it using selection colour function.\nD and E both have the next highest degree 3, so choose any one between them, say D.\nD is adjacent to A, therefore it cannot be coloured in the same colour as A. Hence, choose a different colour using selection colour function.\nThe next highest degree vertex is E, hence choose E.\nE is adjacent to both A and D, therefore it cannot be coloured in the same colours as A and D. Choose a different colour using selection colour function.\nThe next highest degree vertices are B and C. Thus, choose any one randomly.\nB is adjacent to both A and E, thus not allowing to be coloured in the colours of A and E but it is not adjacent to D, so it can be coloured with Ds colour.\nThe next and the last vertex remaining is C, which is adjacent to both A and D, not allowing it to be coloured using the colours of A and D. But it is not adjacent to E, so it can be coloured in Es colour.\nFollowing is the complete implementation of Map Colouring Algorithm in various programming languages where a graph is coloured in such a way that no two adjacent vertices have same colour.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/dijkstras_shortest_path_algorithm.htm", "title": "Dijkstras Shortest Path Algorithm", "content": "Dijkstras shortest path algorithm is similar to that of Prims algorithm as they both rely on finding the shortest path locally to achieve the global solution. However, unlike prims algorithm, the dijkstras algorithm does not find the minimum spanning tree; it is designed to find the shortest path in the graph from one vertex to other remaining vertices in the graph. Dijkstras algorithm can be performed on both directed and undirected graphs.\nSince the shortest path can be calculated from single source vertex to all the other vertices in the graph, Dijkstras algorithm is also called \n. The output obtained is called \n.\nIn this chapter, we will learn about the greedy approach of the dijkstras algorithm.\nThe dijkstras algorithm is designed to find the shortest path between two vertices of a graph. These two vertices could either be adjacent or the farthest points in the graph. The algorithm starts from the source. The inputs taken by the algorithm are the graph G {V, E}, where V is the set of vertices and E is the set of edges, and the source vertex S. And the output is the shortest path spanning tree.\nDeclare two arrays − \n[] to store the distances from the \nsource vertex to the other vertices in graph and \n[] \nto store the visited vertices.\nSet distance[S] to 0 and distance[v] = ∞, where v represents \nall the other vertices in the graph.\nAdd S to the visited[] array and find the adjacent vertices of S \nwith the minimum distance.\nThe adjacent vertex to S, say A, has the minimum distance and \nis not in the visited array yet. A is picked and added to the visited \narray and the distance of A is changed from ∞ to the assigned distance \nof A, say d\n, where d\n < ∞.\nRepeat the process for the adjacent vertices of the visited \nvertices until the shortest path spanning tree is formed.\nTo understand the dijkstras concept better, let us analyze the algorithm with the help of an example graph −\nInitialize the distances of all the vertices as ∞, except the source node S.\nNow that the source vertex S is visited, add it into the visited array.\nThe vertex S has three adjacent vertices with various distances and the vertex with minimum distance among them all is A. Hence, A is visited and the dist[A] is changed from ∞ to 6.\nThere are two vertices visited in the visited array, therefore, the adjacent vertices must be checked for both the visited vertices.\nVertex S has two more adjacent vertices to be visited yet: D and E. Vertex A has one adjacent vertex B.\nCalculate the distances from S to D, E, B and select the minimum distance −\nCalculate the distances of the adjacent vertices  S, A, E  of all the visited arrays and select the vertex with minimum distance.\nRecalculate the distances of unvisited vertices and if the distances minimum than existing distance is found, replace the value in the distance array.\ndist[C] = minimum (12, 11) = 11\ndist[B] = minimum (15,23) = 15\nThe remaining unvisited vertex in the graph is B with the minimum distance 15, is added to the output spanning tree.\nThe shortest path spanning tree is obtained as an output using the dijkstras algorithm.\nThe program implements the dijkstras shortest path problem that takes the cost adjacency matrix as the input and prints the shortest path as the output along with the minimum cost.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/kruskals_spanning_tree_algorithm.htm", "title": "Kruskals Minimal Spanning Tree Algorithm", "content": "Kruskal's minimal spanning tree algorithm is one of the efficient methods to find the minimum spanning tree of a graph. A minimum spanning tree is a subgraph that connects all the vertices present in the main graph with the least possible edges and minimum cost (sum of the weights assigned to each edge).\nThe algorithm first starts from the forest  which is defined as a subgraph containing only vertices of the main graph  of the graph, adding the least cost edges later until the minimum spanning tree is created without forming cycles in the graph.\nKruskal's algorithm has easier implementation than prims algorithm, but has higher complexity.\nThe inputs taken by the kruskals algorithm are the graph G {V, E}, where V is the set of vertices and E is the set of edges, and the source vertex S and the minimum spanning tree of graph G is obtained as an output.\nSort all the edges in the graph in an ascending order and store it in an array \n[].\nConstruct the forest of the graph on a plane with all the vertices in it.\nSelect the least cost edge from the edge[] array and add it into the forest of the graph. Mark the vertices visited by adding them into the visited[] array.\nRepeat the steps 2 and 3 until all the vertices are visited without having any cycles forming in the graph\nWhen all the vertices are visited, the minimum spanning tree is formed.\n\nCalculate the minimum cost of the output spanning tree formed.\nConstruct a minimum spanning tree using kruskals algorithm for the graph given below −\nAs the first step, sort all the edges in the given graph in an ascending order and store the values in an array.\nThen, construct a forest of the given graph on a single plane.\nFrom the list of sorted edge costs, select the least cost edge and add it onto the forest in output graph.\nSimilarly, the next least cost edge is B → A = 6; so we add it onto the output graph.\nThe next least cost edge is C → F = 9; add it onto the output graph.\nThe next edge to be added onto the output graph is F → E = 10.\nThe next edge from the least cost array is B → C = 11, hence we add it in the output graph.\nThe last edge from the least cost array to be added in the output graph is F → G = 12.\nThe obtained result is the minimum spanning tree of the given graph with cost = 53.\nThe final program implements the Kruskals minimum spanning tree problem that takes the cost adjacency matrix as the input and prints the shortest path as the output along with the minimum cost.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/prims_spanning_tree_algorithm.htm", "title": "Prims Minimal Spanning Tree", "content": "Prim's minimal spanning tree algorithm is one of the efficient methods to find the minimum spanning tree of a graph. A minimum spanning tree is a sub graph that connects all the vertices present in the main graph with the least possible edges and minimum cost (sum of the weights assigned to each edge).\nThe algorithm, similar to any shortest path algorithm, begins from a vertex that is set as a root and walks through all the vertices in the graph by determining the least cost adjacent edges.\nTo execute the prim's algorithm, the inputs taken by the algorithm are the graph G {V, E}, where V is the set of vertices and E is the set of edges, and the source vertex S. A minimum spanning tree of graph G is obtained as an output.\nDeclare an array \n[] to store the visited vertices and firstly, add the arbitrary root, say S, to the visited array.\nCheck whether the adjacent vertices of the last visited vertex are present in the \n[] array or not.\nIf the vertices are not in the \n[] array, compare the cost of edges and add the least cost edge to the output spanning tree.\nThe adjacent unvisited vertex with the least cost edge is added into the \n[] array and the least cost edge is added to the minimum spanning tree output.\nSteps 2 and 4 are repeated for all the unvisited vertices in the graph to obtain the full minimum spanning tree output for the given graph.\nCalculate the cost of the minimum spanning tree obtained.\nFind the minimum spanning tree using prims method (greedy approach) for the graph given below with S as the arbitrary root.\nCreate a visited array to store all the visited vertices into it.\nThe arbitrary root is mentioned to be S, so among all the edges that are connected to S we need to find the least cost edge.\nSince B is the last visited, check for the least cost edge that is connected to the vertex B.\nHence, B → A is the edge added to the spanning tree.\nSince A is the last visited, check for the least cost edge that is connected to the vertex A.\nBut A → B is already in the spanning tree, check for the next least cost edge. Hence, A → E is added to the spanning tree.\nSince E is the last visited, check for the least cost edge that is connected to the vertex E.\nTherefore, E → D is added to the spanning tree.\nSince D is the last visited, check for the least cost edge that is connected to the vertex D.\nTherefore, D → C is added to the spanning tree.\nThe minimum spanning tree is obtained with the minimum cost = 46\nThe final program implements Prims minimum spanning tree problem that takes the cost adjacency matrix as the input and prints the spanning tree as the output along with the minimum cost.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/travelling_salesman_problem.htm", "title": "Travelling Salesman Problem (Greedy Approach)", "content": "The travelling salesman problem is a graph computational problem where the salesman needs to visit all cities (represented using nodes in a graph) in a list just once and the distances (represented using edges in the graph) between all these cities are known. The solution that is needed to be found for this problem is the shortest possible route in which the salesman visits all the cities and returns to the origin city.\nIf you look at the graph below, considering that the salesman starts from the vertex a, they need to travel through all the remaining vertices b, c, d, e, f and get back to a while making sure that the cost taken is minimum.\nThere are various approaches to find the solution to the travelling salesman problem: naive approach, greedy approach, dynamic programming approach, etc. In this tutorial we will be learning about solving travelling salesman problem using greedy approach.\nAs the definition for greedy approach states, we need to find the best optimal solution locally to figure out the global optimal solution. The inputs taken by the algorithm are the graph G {V, E}, where V is the set of vertices and E is the set of edges. The shortest path of graph G starting from one vertex returning to the same vertex is obtained as the output.\nTravelling salesman problem takes a graph G {V, E} as an input and declare another graph as the output (say G) which will record the path the salesman is going to take from one node to another.\nThe algorithm begins by sorting all the edges in the input graph G from the least distance to the largest distance.\nThe first edge selected is the edge with least distance, and one of the two vertices (say A and B) being the origin node (say A).\nThen among the adjacent edges of the node other than the origin node (B), find the least cost edge and add it onto the output graph.\nContinue the process with further nodes making sure there are no cycles in the output graph and the path reaches back to the origin node A.\nHowever, if the origin is mentioned in the given problem, then the solution must always start from that node only. Let us look at some example problems to understand this better.\nConsider the following graph with six cities and the distances between them −\nFrom the given graph, since the origin is already mentioned, the solution must always start from that node. Among the edges leading from A, A → B has the shortest distance.\nThen, B → C has the shortest and only edge between, therefore it is included in the output graph.\nTheres only one edge between C → D, therefore it is added to the output graph.\nTheres two outward edges from D. Even though, D → B has lower distance than D → E, B is already visited once and it would form a cycle if added to the output graph. Therefore, D → E is added into the output graph.\nTheres only one edge from e, that is E → F. Therefore, it is added into the output graph.\nAgain, even though F → C has lower distance than F → A, F → A is added into the output graph in order to avoid the cycle that would form and C is already visited once.\nThe shortest path that originates and ends at A is A → B → C → D → E → F → A\nThe cost of the path is: 16 + 21 + 12 + 15 + 16 + 34 = 114.\nEven though, the cost of path could be decreased if it originates from other nodes but the question is not raised with respect to that.\nThe complete implementation of Travelling Salesman Problem using Greedy Approach is given below −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/greedy_algorithms.htm", "title": "Greedy Algorithms", "content": "Among all the algorithmic approaches, the simplest and straightforward approach is the Greedy method. In this approach, the decision is taken on the basis of current available information without worrying about the effect of the current decision in future.\nGreedy algorithms build a solution part by part, choosing the next part in such a way, that it gives an immediate benefit. This approach never reconsiders the choices taken previously. This approach is mainly used to solve optimization problems. Greedy method is easy to implement and quite efficient in most of the cases. Hence, we can say that Greedy algorithm is an algorithmic paradigm based on heuristic that follows local optimal choice at each step with the hope of finding global optimal solution.\nIn many problems, it does not produce an optimal solution though it gives an approximate (near optimal) solution in a reasonable time.\nGreedy algorithms have the following five components −\n − A solution is created from this set.\n − Used to choose the best candidate to be added to the solution.\n − Used to determine whether a candidate can be used to contribute to the solution.\n − Used to assign a value to a solution or a partial solution.\n − Used to indicate whether a complete solution has been reached.\nGreedy approach is used to solve many problems, such as\nFinding the shortest path between two vertices using Dijkstra's algorithm.\nFinding the minimal spanning tree in a graph using Prim's /Kruskal's algorithm, etc.\nThe Counting Coins problem is to count to a desired value by choosing the least possible coins and the greedy approach forces the algorithm to pick the largest possible coin. If we are provided coins of 1, 2, 5 and 10 and we are asked to count 18 then the greedy procedure will be −\n1 − Select one 10 coin, the remaining count is 8\n2 − Then select one 5 coin, the remaining count is 3\n3 − Then select one 2 coin, the remaining count is 1\n4 − And finally, the selection of one 1 coins solves the problem\nThough, it seems to be working fine, for this count we need to pick only 4 coins. But if we slightly change the problem then the same approach may not be able to produce the same optimum result.\nFor the currency system, where we have coins of 1, 7, 10 value, counting coins for value 18 will be absolutely optimum but for count like 15, it may use more coins than necessary. For example, the greedy approach will use 10 + 1 + 1 + 1 + 1 + 1, total 6 coins. Whereas the same problem could be solved by using only 3 coins (7 + 7 + 1)\nHence, we may conclude that the greedy approach picks an immediate optimized solution and may fail where global optimization is a major concern.\nIn many problems, Greedy algorithm fails to find an optimal solution, moreover it may produce a worst solution. Problems like Travelling Salesman and Knapsack cannot be solved using this approach.\nMost networking algorithms use the greedy approach. Here is a list of few of them −\nWe will discuss these examples elaborately in the further chapters of this tutorial.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/karatsuba_algorithm.htm", "title": "Karatsuba Algorithm", "content": "The \n is used by the system to perform fast multiplication on two n-digit numbers, i.e. the system compiler takes lesser time to compute the product than the time-taken by a normal multiplication.\nThe usual multiplication approach takes n\n computations to achieve the final product, since the multiplication has to be performed between all digit combinations in both the numbers and then the sub-products are added to obtain the final product. This approach of multiplication is known as \n.\nTo understand this multiplication better, let us consider two 4-digit integers: \n and \n, and find the product using Naive approach.\nSo, \nIn this method of naive multiplication, given the number of digits in both numbers is 4, there are 16 single-digit  single-digit multiplications being performed. Thus, the time complexity of this approach is O(4\n) since it takes 4\n steps to calculate the final product.\nBut when the value of n keeps increasing, the time complexity of the problem also keeps increasing. Hence, Karatsuba algorithm is adopted to perform faster multiplications.\nThe main idea of the Karatsuba Algorithm is to reduce multiplication of multiple sub problems to multiplication of three sub problems. Arithmetic operations like additions and subtractions are performed for other computations.\nFor this algorithm, two n-digit numbers are taken as the input and the product of the two number is obtained as the output.\n − In this algorithm we assume that n is a power of 2.\n − If n = 1 then we use multiplication tables to find P = XY.\n − If n > 1, the n-digit numbers are split in half and represent the number using the formulae −\nwhere, X\n, X\n, Y\n, Y\n each have n/2 digits.\n − Take a variable Z = W  (U + V),\nwhere,\n − Then, the product P is obtained after substituting the values in the formula −\n − Recursively call the algorithm by passing the sub problems (X\n, Y\n), (X\n, Y\n) and (X\n + X\n, Y\n + Y\n) separately. Store the returned values in variables U, V and W respectively.\nLet us solve the same problem given above using Karatsuba method, 1456  6533 −\nThe Karatsuba method takes the divide and conquer approach by dividing the problem into multiple sub-problems and applies recursion to make the multiplication simpler.\nAssuming that n is the power of 2, rewrite the n-digit numbers in the form of −\nThat gives us,\nFirst let us try simplifying the mathematical expression, we get,\nThe above expression is the simplified version of the given multiplication problem, since multiplying two double-digit numbers can be easier to solve rather than multiplying two four-digit numbers.\nHowever, that holds true for the human mind. But for the system compiler, the above expression still takes the same time complexity as the normal naive multiplication. Since it has 4 double-digit  double-digit multiplications, the time complexity taken would be −\nThus, the calculation needs to be simplified further.\nSince n is not equal to 1, the algorithm jumps to step 3.\nThat gives us,\nCalculate Z = W  (U + V) −\nThe final product,\nThe sub-problems can be further divided into smaller problems; therefore, the algorithm is again called recursively.\nX\n and Y\n are passed as parameters X and Y.\nSo now, X = 14, Y = 65\nCalculate Z = W  (U + V) −\nX\n and Y\n are passed as parameters X and Y.\nSo now, X = 56, Y = 33\nCalculate Z = W  (U + V) −\nX\n + X\n and Y\n + Y\n are passed as parameters X and Y.\nSo now, X = 70, Y = 98\nCalculate Z = W  (U + V) −\nThe final product,\nSubstituting the values in equation,\nThe Karatsuba algorithm is a recursive algorithm; since it calls smaller instances of itself during execution.\nAccording to the algorithm, it calls itself only thrice on n/2-digit numbers in order to achieve the final product of two n-digit numbers.\nNow, if T(n) represents the number of digit multiplications required while performing the multiplication,\nT(n) = 3T(n/2)\nThis equation is a simple recurrence relation which can be solved as −\nRecurrence relations can be solved using the \n, since we have a dividing function in the form of −\nSince f(n) represents work done outside the recursion, which are addition and subtraction arithmetic operations in Karatsuba, these arithmetic operations do not contribute to time complexity.\nCheck the relation between a and b\n.\nAccording to masters theorem, apply case 1.\nThe time complexity of Karatsuba algorithm for fast multiplication is \n.\nIn the complete implementation of Karatsuba Algorithm, we are trying to multiply two higher-valued numbers. Here, since the \n data type accepts decimals upto 18 places, we take the inputs as \n values. The Karatsuba function is called recursively until the final product is obtained.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/strassens_matrix_multiplication_algorithm.htm", "title": "Strassens Matrix Multiplication", "content": "Strassen's Matrix Multiplication is the divide and conquer approach to solve the matrix multiplication problems. The usual matrix multiplication method multiplies each row with each column to achieve the product matrix. The time complexity taken by this approach is \n, since it takes two loops to multiply. Strassens method was introduced to reduce the time complexity from \n to \n.\nFirst, we will discuss Naive method and its complexity. Here, we are calculating Z=X  Y. Using Naive method, two matrices (\n and \n) can be multiplied if the order of these matrices are \n  \n and \n  \n and the resultant matrix will be of order \n  \n. The following pseudocode describes the Naive multiplication −\nHere, we assume that integer operations take \n time. There are three \n loops in this algorithm and one is nested in other. Hence, the algorithm takes \n time to execute.\nIn this context, using Strassens Matrix multiplication algorithm, the time consumption can be improved a little bit.\nStrassens Matrix multiplication can be performed only on \n where \n is a \n. Order of both of the matrices are \n.\nDivide \n, \n and \n into four \n matrices as represented below −\n$Z = \\begin{bmatrix}I & J \\\\K & L \\end{bmatrix}$ \n and \nUsing Strassens Algorithm compute the following −\n$$M_{1} \\: \\colon= (A+C) \\times (E+F)$$\n$$M_{2} \\: \\colon= (B+D) \\times (G+H)$$\n$$M_{3} \\: \\colon= (A-D) \\times (E+H)$$\n$$M_{4} \\: \\colon= A \\times (F-H)$$\n$$M_{5} \\: \\colon= (C+D) \\times (E)$$\n$$M_{6} \\: \\colon= (A+B) \\times (H)$$\n$$M_{7} \\: \\colon= D \\times (G-E)$$\nThen,\n$$I \\: \\colon= M_{2} + M_{3} - M_{6} - M_{7}$$\n$$J \\: \\colon= M_{4} + M_{6}$$\n$$K \\: \\colon= M_{5} + M_{7}$$\n$$L \\: \\colon= M_{1} - M_{3} - M_{4} - M_{5}$$\n$$T(n)=\\begin{cases}c & if\\:n= 1\\\\7\\:x\\:T(\\frac{n}{2})+d\\:x\\:n^2 & otherwise\\end{cases} \\:where\\: c\\: and \\:d\\:are\\: constants$$\nUsing this recurrence relation, we get $T(n) = O(n^{log7})$\nHence, the complexity of Strassens matrix multiplication algorithm is $O(n^{log7})$.\nLet us look at the implementation of Strassen's Matrix Multiplication in various programming languages: C, C++, Java, Python.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/max_min_problem.htm", "title": "Max-Min Problem", "content": "Let us consider a simple problem that can be solved by divide and conquer technique.\nThe Max-Min Problem in algorithm analysis is finding the maximum and minimum value in an array.\nTo find the maximum and minimum numbers in a given array \n of size \n, the following algorithm can be used. First we are representing the \n and then we will present \n.\nNaive method is a basic method to solve any problem. In this method, the maximum and minimum number can be found separately. To find the maximum and minimum numbers, the following straightforward algorithm can be used.\nFollowing are the implementations of the above approach in various programming languages −\nThe number of comparison in Naive method is \n.\nThe number of comparisons can be reduced using the divide and conquer approach. Following is the technique.\nIn this approach, the array is divided into two halves. Then using recursive approach maximum and minimum numbers in each halves are found. Later, return the maximum of two maxima of each half and the minimum of two minima of each half.\nIn this given problem, the number of elements in an array is $y - x + 1$, where \n is greater than or equal to \n.\n$\\mathbf{\\mathit{Max - Min(x, y)}}$ will return the maximum and minimum values of an array $\\mathbf{\\mathit{numbers[x...y]}}$.\nFollowing are implementations of the above approach in various programming languages −\nLet \n be the number of comparisons made by $\\mathbf{\\mathit{Max - Min(x, y)}}$, where the number of elements $n = y - x + 1$.\nIf \n represents the numbers, then the recurrence relation can be represented as\n$$T(n) = \\begin{cases}T\\left(\\lfloor\\frac{n}{2}\\rfloor\\right)+T\\left(\\lceil\\frac{n}{2}\\rceil\\right)+2 & for\\: n>2\\\\1 & for\\:n = 2 \\\\0 & for\\:n = 1\\end{cases}$$\nLet us assume that \n is in the form of power of \n. Hence, \n where \n is height of the recursion tree.\nSo,\n$$T(n) = 2.T (\\frac{n}{2}) + 2 = 2.\\left(\\begin{array}{c}2.T(\\frac{n}{4}) + 2\\end{array}\\right) + 2 ..... = \\frac{3n}{2} - 2$$\nCompared to Nave method, in divide and conquer approach, the number of comparisons is less. However, using the asymptotic notation both of the approaches are represented by \n.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/dsa_discussion.htm", "title": "Discuss Data Structures & Algorithms", "content": "Data Structures are the programmatic way of storing data so that data can be used efficiently. Almost every enterprise application uses various types of data structures in one or the other way. This tutorial will give you a great understanding on Data Structures needed to understand the complexity of enterprise level applications and need of algorithms, and data structures.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/dsa_useful_resources.htm", "title": "Data Structure - Useful Resources", "content": "The following resources contain additional information on Data Structure. Please use them to get more in-depth knowledge on this.\n\n    54 Lectures \n    \n\n\n \n\n    52 Lectures \n    \n\n\n \n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/dsa_quick_guide.htm", "title": "Data Structures & Algorithms - Quick Guide", "content": "Data Structure is a systematic way to organize data in order to use it efficiently. Following terms are the foundation terms of a data structure.\n − Each data structure has an interface. Interface represents the set of operations that a data structure supports. An interface only provides the list of supported operations, type of parameters they can accept and return type of these operations.\n − Implementation provides the internal representation of a data structure. Implementation also provides the definition of the algorithms used in the operations of the data structure.\n − Data structure implementation should implement its interface correctly.\n − Running time or the execution time of operations of data structure must be as small as possible.\n − Memory usage of a data structure operation should be as little as possible.\nAs applications are getting complex and data rich, there are three common problems that applications face now-a-days.\n − Consider an inventory of 1 million(10\n) items of a store. If the application is to search an item, it has to search an item in 1 million(10\n) items every time slowing down the search. As data grows, search will become slower.\n − Processor speed although being very high, falls limited if the data grows to billion records.\n − As thousands of users can search data simultaneously on a web server, even the fast server fails while searching the data.\nTo solve the above-mentioned problems, data structures come to rescue. Data can be organized in a data structure in such a way that all items may not be required to be searched, and the required data can be searched almost instantly.\nThere are three cases which are usually used to compare various data structure's execution time in a relative manner.\n − This is the scenario where a particular data structure operation takes maximum time it can take. If an operation's worst case time is ƒ(n) then this operation will not take more than ƒ(n) time where ƒ(n) represents function of n.\n − This is the scenario depicting the average execution time of an operation of a data structure. If an operation takes ƒ(n) time in execution, then m operations will take mƒ(n) time.\n − This is the scenario depicting the least possible execution time of an operation of a data structure. If an operation takes ƒ(n) time in execution, then the actual operation may take time as the random number which would be maximum as ƒ(n).\n − Data are values or set of values.\n − Data item refers to single unit of values.\n − Data items that are divided into sub items are called as Group Items.\n − Data items that cannot be divided are called as Elementary Items.\n − An entity is that which contains certain attributes or properties, which may be assigned values.\n − Entities of similar attributes form an entity set.\n − Field is a single elementary unit of information representing an attribute of an entity.\n − Record is a collection of field values of a given entity.\n − File is a collection of records of the entities in a given entity set.\nIf you are still willing to set up your environment for C programming language, you need the following two tools available on your computer, (a) Text Editor and (b) The C Compiler.\nThis will be used to type your program. Examples of few editors include Windows Notepad, OS Edit command, Brief, Epsilon, EMACS, and vim or vi.\nThe name and the version of the text editor can vary on different operating systems. For example, Notepad will be used on Windows, and vim or vi can be used on Windows as well as Linux or UNIX.\nThe files you create with your editor are called source files and contain program source code. The source files for C programs are typically named with the extension \"\n\".\nBefore starting your programming, make sure you have one text editor in place and you have enough experience to write a computer program, save it in a file, compile it, and finally execute it.\nThe source code written in the source file is the human readable source for your program. It needs to be \"compiled\", to turn into machine language so that your CPU can actually execute the program as per the given instructions.\nThis C programming language compiler will be used to compile your source code into a final executable program. We assume you have the basic knowledge about a programming language compiler.\nMost frequently used and free available compiler is GNU C/C&plus;&plus; compiler. Otherwise, you can have compilers either from HP or Solaris if you have respective Operating Systems (OS).\nThe following section guides you on how to install GNU C/C&plus;&plus; compiler on various OS. We are mentioning C/C&plus;&plus; together because GNU GCC compiler works for both C and C&plus;&plus; programming languages.\nIf you are using \n, then check whether GCC is installed on your system by entering the following command from the command line −\nIf you have GNU compiler installed on your machine, then it should print a message such as the following −\nIf GCC is not installed, then you will have to install it yourself using the detailed instructions available at \nThis tutorial has been written based on Linux and all the given examples have been compiled on Cent OS flavor of Linux system.\nIf you use Mac OS X, the easiest way to obtain GCC is to download the Xcode development environment from Apple's website and follow the simple installation instructions. Once you have Xcode setup, you will be able to use GNU compiler for C/C&plus;&plus;.\nXcode is currently available at \nTo install GCC on Windows, you need to install MinGW. To install MinGW, go to the MinGW homepage, \n, and follow the link to the MinGW download page. Download the latest version of the MinGW installation program, which should be named MinGW-<version>.exe.\nWhile installing MinWG, at a minimum, you must install gcc-core, gcc-g&plus;&plus;, binutils, and the MinGW runtime, but you may wish to install more.\nAdd the bin subdirectory of your MinGW installation to your \n environment variable, so that you can specify these tools on the command line by their simple names.\nWhen the installation is complete, you will be able to run gcc, g&plus;&plus;, ar, ranlib, dlltool, and several other GNU tools from the Windows command line.\nThis chapter explains the basic terms related to data structure.\nData Definition defines a particular data with the following characteristics.\n − Definition should define a single concept.\n − Definition should be able to be mapped to some data element.\n − Definition should be unambiguous.\n − Definition should be understandable.\nData Object represents an object having a data.\nData type is a way to classify various types of data such as integer, string, etc. which determines the values that can be used with the corresponding type of data, the type of operations that can be performed on the corresponding type of data. There are two data types −\nThose data types for which a language has built-in support are known as Built-in Data types. For example, most of the languages provide the following built-in data types.\nThose data types which are implementation independent as they can be implemented in one or the other way are known as derived data types. These data types are normally built by the combination of primary or built-in data types and associated operations on them. For example −\nThe data in the data structures are processed by certain operations. The particular data structure chosen largely depends on the frequency of the operation that needs to be performed on the data structure.\n are introduced in order to store, organize and manipulate data in programming languages. They are designed in a way that makes accessing and processing of the data a little easier and simpler. These data structures are not confined to one particular programming language; they are just pieces of code that structure data in the memory.\nData types are often confused as a type of data structures, but it is not precisely correct even though they are referred to as Abstract Data Types. Data types represent the nature of the data while data structures are just a collection of similar or different data types in one.\nThere are usually just two types of data structures −\nLinear\nNon-Linear\nThe data is stored in linear data structures sequentially. These are rudimentary structures since the elements are stored one after the other without applying any mathematical operations.\nLinear data structures are usually easy to implement but since the memory allocation might become complicated, time and space complexities increase. Few examples of linear data structures include −\nArrays\nLinked Lists\nStacks\nQueues\nBased on the data storage methods, these linear data structures are divided into two sub-types. They are − \n and \n data structures.\nIn Static Linear Data Structures, the memory allocation is not scalable. Once the entire memory is used, no more space can be retrieved to store more data. Hence, the memory is required to be reserved based on the size of the program. This will also act as a drawback since reserving more memory than required can cause a wastage of memory blocks.\nThe best example for static linear data structures is an array.\nIn Dynamic linear data structures, the memory allocation can be done dynamically when required. These data structures are efficient considering the space complexity of the program.\nFew examples of dynamic linear data structures include: linked lists, stacks and queues.\nNon-Linear data structures store the data in the form of a hierarchy. Therefore, in contrast to the linear data structures, the data can be found in multiple levels and are difficult to traverse through.\nHowever, they are designed to overcome the issues and limitations of linear data structures. For instance, the main disadvantage of linear data structures is the memory allocation. Since the data is allocated sequentially in linear data structures, each element in these data structures uses one whole memory block. However, if the data uses less memory than the assigned block can hold, the extra memory space in the block is wasted. Therefore, non-linear data structures are introduced. They decrease the space complexity and use the memory optimally.\nFew types of non-linear data structures are −\nGraphs\nTrees\nTries\nMaps\nArray is a type of linear data structure that is defined as a collection of elements with same or different data types. They exist in both single dimension and multiple dimensions. These data structures come into picture when there is a necessity to store multiple elements of similar nature together at one place.\nThe difference between an array index and a memory address is that the array index acts like a key value to label the elements in the array. However, a memory address is the starting address of free memory available.\nFollowing are the important terms to understand the concept of Array.\n − Each item stored in an array is called an element.\n − Each location of an element in an array has a numerical index, which is used to identify the element.\nCreating an array in \n and \n programming languages −\nCreating an array in \n programming language −\nArrays are used as solutions to many problems from the small sorting problems to more complex problems like travelling salesperson problem. There are many data structures other than arrays that provide efficient time and space complexity for these problems, so what makes using arrays better? The answer lies in the random access lookup time.\nArrays provide \n random access lookup time. That means, accessing the 1\n index of the array and the 1000\n index of the array will both take the same time. This is due to the fact that array comes with a pointer and an offset value. The pointer points to the right location of the memory and the offset value shows how far to look in the said memory.\nTherefore, in an array with 6 elements, to access the 1st element, array is pointed towards the 0th index. Similarly, to access the 6\n element, array is pointed towards the 5\n index.\nArrays are represented as a collection of buckets where each bucket stores one element. These buckets are indexed from 0 to n-1, where n is the size of that particular array. For example, an array with size 10 will have buckets indexed from 0 to 9.\nThis indexing will be similar for the multidimensional arrays as well. If it is a 2-dimensional array, it will have sub-buckets in each bucket. Then it will be indexed as array_name[m][n], where m and n are the sizes of each level in the array.\nAs per the above illustration, following are the important points to be considered.\nIndex starts with 0.\nArray length is 9 which means it can store 9 elements.\nEach element can be accessed via its index. For example, we can fetch an element at index 6 as 23.\nThe basic operations in the Arrays are insertion, deletion, searching, display, traverse, and update. These operations are usually performed to either modify the data in the array or to report the status of the array.\nFollowing are the basic operations supported by an array.\n − print all the array elements one by one.\n − Adds an element at the given index.\n − Deletes an element at the given index.\n − Searches an element using the given index or by the value.\n − Updates an element at the given index.\n − Displays the contents of the array.\nIn C, when an array is initialized with size, then it assigns defaults values to its elements in following order.\nIn the insertion operation, we are adding one or more elements to the array. Based on the requirement, a new element can be added at the beginning, end, or any given index of array. This is done using input statements of the programming languages.\nFollowing is an algorithm to insert elements into a Linear Array until we reach the end of the array −\nHere, we see a practical implementation of insertion operation, where we add data at the end of the array −\nFor other variations of array insertion operation, \n.\nIn this array operation, we delete an element from the particular index of an array. This deletion operation takes place as we assign the value in the consequent index to the current index.\nConsider LA is a linear array with N elements and K is a positive integer such that K<=N. Following is the algorithm to delete an element available at the K\n position of LA.\nFollowing are the implementations of this operation in various programming languages −\nSearching an element in the array using a key; The key element sequentially compares every value in the array to check if the key is present in the array or not.\nConsider LA is a linear array with N elements and K is a positive integer such that K<=N. Following is the algorithm to find an element with a value of ITEM using sequential search.\nFollowing are the implementations of this operation in various programming languages −\nThis operation traverses through all the elements of an array. We use loop statements to carry this out.\nFollowing is the algorithm to traverse through all the elements present in a Linear Array −\nFollowing are the implementations of this operation in various programming languages −\nUpdate operation refers to updating an existing element from the array at a given index.\nConsider LA is a linear array with N elements and K is a positive integer such that K<=N. Following is the algorithm to update an element available at the Kth position of LA.\nFollowing are the implementations of this operation in various programming languages −\nThis operation displays all the elements in the entire array using a print statement.\nFollowing are the implementations of this operation in various programming languages −\nIf arrays accommodate similar types of data types, linked lists consist of elements with different data types that are also arranged sequentially.\nA linked list is a collection of nodes connected together via links. These nodes consist of the data to be stored and a pointer to the address of the next node within the linked list. In the case of arrays, the size is limited to the definition, but in linked lists, there is no defined size. Any amount of data can be stored in it and can be deleted from it.\nThere are three types of linked lists −\n − The nodes only point to the address of the next node in the list.\n − The nodes point to the addresses of both previous and next nodes.\n − The last node in the list will point to the first node in the list. It can either be singly linked or doubly linked.\nLinked list can be visualized as a chain of nodes, where every node points to the next node.\nAs per the above illustration, following are the important points to be considered.\nLinked List contains a link element called first (head).\nEach link carries a data field(s) and a link field called next.\nEach link is linked with its next link using its next link.\nLast link carries a link as null to mark the end of the list.\nFollowing are the various types of linked list.\nSingly linked lists contain two buckets in one node; one bucket holds the data and the other bucket holds the address of the next node of the list. Traversals can be done in one direction only as there is only a single link between two nodes of the same list.\nDoubly Linked Lists contain three buckets in one node; one bucket holds the data and the other buckets hold the addresses of the previous and next nodes in the list. The list is traversed twice as the nodes in the list are connected to each other from both sides.\nCircular linked lists can exist in both singly linked list and doubly linked list.\nSince the last node and the first node of the circular linked list are connected, the traversal in this linked list will go on forever until it is broken.\nThe basic operations in the linked lists are insertion, deletion, searching, display, and deleting an element at a given key. These operations are performed on Singly Linked Lists as given below −\n − Adds an element at the beginning of the list.\n − Deletes an element at the beginning of the list.\n − Displays the complete list.\n − Searches an element using the given key.\n − Deletes an element using the given key.\nAdding a new node in linked list is a more than one step activity. We shall learn this with diagrams here. First, create a node using the same structure and find the location where it has to be inserted.\nImagine that we are inserting a node B (NewNode), between A (LeftNode) and C (RightNode). Then point B.next to C −\nIt should look like this −\nNow, the next node at the left should point to the new node.\nThis will put the new node in the middle of the two. The new list should look like this −\nInsertion in linked list can be done in three different ways. They are explained as follows −\nIn this operation, we are adding an element at the beginning of the list.\nFollowing are the implementations of this operation in various programming languages −\nIn this operation, we are adding an element at the ending of the list.\nFollowing are the implementations of this operation in various programming languages −\nIn this operation, we are adding an element at any position within the list.\nFollowing are the implementations of this operation in various programming languages −\nDeletion is also a more than one step process. We shall learn with pictorial representation. First, locate the target node to be removed, by using searching algorithms.\nThe left (previous) node of the target node now should point to the next node of the target node −\nThis will remove the link that was pointing to the target node. Now, using the following code, we will remove what the target node is pointing at.\nWe need to use the deleted node. We can keep that in memory otherwise we can simply deallocate memory and wipe off the target node completely.\nSimilar steps should be taken if the node is being inserted at the beginning of the list. While inserting it at the end, the second last node of the list should point to the new node and the new node will point to NULL.\nDeletion in linked lists is also performed in three different ways. They are as follows −\nIn this deletion operation of the linked, we are deleting an element from the beginning of the list. For this, we point the head to the second node.\nFollowing are the implementations of this operation in various programming languages −\nIn this deletion operation of the linked, we are deleting an element from the ending of the list.\nFollowing are the implementations of this operation in various programming languages −\nIn this deletion operation of the linked, we are deleting an element at any position of the list.\nFollowing are the implementations of this operation in various programming languages −\nThis operation is a thorough one. We need to make the last node to be pointed by the head node and reverse the whole linked list.\nFirst, we traverse to the end of the list. It should be pointing to NULL. Now, we shall make it point to its previous node −\nWe have to make sure that the last node is not the last node. So we'll have some temp node, which looks like the head node pointing to the last node. Now, we shall make all left side nodes point to their previous nodes one by one.\nExcept the node (first node) pointed by the head node, all nodes should point to their predecessor, making them their new successor. The first node will point to NULL.\nWe'll make the head node point to the new first node by using the temp node.\nStep by step process to reverse a linked list is as follows −\nFollowing are the implementations of this operation in various programming languages −\nSearching for an element in the list using a key element. This operation is done in the same way as array search; comparing every element in the list with the key element given.\nFollowing are the implementations of this operation in various programming languages −\nThe traversal operation walks through all the elements of the list in an order and displays the elements in that order.\nFollowing are the implementations of this operation in various programming languages −\nDoubly Linked List is a variation of Linked list in which navigation is possible in both ways, either forward and backward easily as compared to Single Linked List. Following are the important terms to understand the concept of doubly linked list.\n − Each link of a linked list can store a data called an element.\n − Each link of a linked list contains a link to the next link called Next.\n − Each link of a linked list contains a link to the previous link called Prev.\n − A Linked List contains the connection link to the first link called First and to the last link called Last.\nAs per the above illustration, following are the important points to be considered.\nDoubly Linked List contains a link element called first and last.\nEach link carries a data field(s) and a link field called next.\nEach link is linked with its next link using its next link.\nEach link is linked with its previous link using its previous link.\nThe last link carries a link as null to mark the end of the list.\nFollowing are the basic operations supported by a list.\n − Adds an element at the beginning of the list.\n − Deletes an element at the beginning of the list.\n − Adds an element at the end of the list.\n − Deletes an element from the end of the list.\n − Adds an element after an item of the list.\n − Deletes an element from the list using the key.\n − Displays the complete list in a forward manner.\n − Displays the complete list in a backward manner.\nIn this operation, we create a new node with three compartments, one containing the data, the others containing the address of its previous and next nodes in the list. This new node is inserted at the beginning of the list.\nFollowing are the implementations of this operation in various programming languages −\nThis deletion operation deletes the existing first nodes in the doubly linked list. The head is shifted to the next node and the link is removed.\nFollowing are the implementations of this operation in various programming languages −\nIn this insertion operation, the new input node is added at the end of the doubly linked list; if the list is not empty. The head will be pointed to the new node, if the list is empty.\nFollowing are the implementations of this operation in various programming languages −\n is a variation of Linked list in which the first element points to the last element and the last element points to the first element. Both Singly Linked List and Doubly Linked List can be made into a circular linked list.\nIn singly linked list, the next pointer of the last node points to the first node.\nIn doubly linked list, the next pointer of the last node points to the first node and the previous pointer of the first node points to the last node making the circular in both directions.\nAs per the above illustration, following are the important points to be considered.\nThe last link's next points to the first link of the list in both cases of singly as well as doubly linked list.\nThe first link's previous points to the last of the list in case of doubly linked list.\nFollowing are the important operations supported by a circular list.\n − Inserts an element at the start of the list.\n − Deletes an element from the start of the list.\n − Displays the list.\nThe insertion operation of a circular linked list only inserts the element at the start of the list. This differs from the usual singly and doubly linked lists as there is no particular starting and ending points in this list. The insertion is done either at the start or after a particular node (or a given position) in the list.\nFollowing are the implementations of this operation in various programming languages −\nThe Deletion operation in a Circular linked list removes a certain node from the list. The deletion operation in this type of lists can be done at the beginning, or a given position, or at the ending.\nFollowing are the implementations of this operation in various programming languages −\nThe Display List operation visits every node in the list and prints them all in the output.\nFollowing are the implementations of this operation in various programming languages −\nA stack is an Abstract Data Type (ADT), that is popularly used in most programming languages. It is named stack because it has the similar operations as the real-world stacks, for example  a pack of cards or a pile of plates, etc.\nThe stack follows the LIFO (Last in - First out) structure where the last element inserted would be the first element deleted.\nA Stack ADT allows all data operations at one end only. At any given time, we can only access the top element of a stack.\nThe following diagram depicts a stack and its operations −\nA stack can be implemented by means of Array, Structure, Pointer, and Linked List. Stack can either be a fixed size one or it may have a sense of dynamic resizing. Here, we are going to implement stack using arrays, which makes it a fixed size stack implementation.\nStack operations usually are performed for initialization, usage and, de-initialization of the stack ADT.\nThe most fundamental operations in the stack ADT include: push(), pop(), peek(), isFull(), isEmpty(). These are all built-in operations to carry out data manipulation and to check the status of the stack.\nStack uses pointers that always point to the topmost element within the stack, hence called as the \n pointer.\npush() is an operation that inserts elements into the stack. The following is an algorithm that describes the push() operation in a simpler way.\nFollowing are the implementations of this operation in various programming languages −\n − In Java we have used to built-in method \n to perform this operation.\n is a data manipulation operation which removes elements from the stack. The following pseudo code describes the pop() operation in a simpler way.\nFollowing are the implementations of this operation in various programming languages −\n − In Java we are using the built-in method pop().\nThe \n is an operation retrieves the topmost element within the stack, without deleting it. This operation is used to check the status of the stack with the help of the top pointer.\nFollowing are the implementations of this operation in various programming languages −\n operation checks whether the stack is full. This operation is used to check the status of the stack with the help of top pointer.\nFollowing are the implementations of this operation in various programming languages −\nThe \n operation verifies whether the stack is empty. This operation is used to check the status of the stack with the help of top pointer.\nFollowing are the implementations of this operation in various programming languages −\nThe way to write arithmetic expression is known as a \n. An arithmetic expression can be written in three different but equivalent notations, i.e., without changing the essence or output of an expression. These notations are −\nThese notations are named as how they use operator in expression. We shall learn the same here in this chapter.\nWe write expression in \n notation, e.g. a - b &plus; c, where operators are used \n-between operands. It is easy for us humans to read, write, and speak in infix notation but the same does not go well with computing devices. An algorithm to process infix notation could be difficult and costly in terms of time and space consumption.\nIn this notation, operator is \ned to operands, i.e. operator is written ahead of operands. For example, \n. This is equivalent to its infix notation \n. Prefix notation is also known as \n.\nThis notation style is known as \n. In this notation style, the operator is \ned to the operands i.e., the operator is written after the operands. For example, \n. This is equivalent to its infix notation \n.\nThe following table briefly tries to show the difference in all three notations −\nAs we have discussed, it is not a very efficient way to design an algorithm or program to parse infix notations. Instead, these infix notations are first converted into either postfix or prefix notations and then computed.\nTo parse any arithmetic expression, we need to take care of operator precedence and associativity also.\nWhen an operand is in between two different operators, which operator will take the operand first, is decided by the precedence of an operator over others. For example −\nAs multiplication operation has precedence over addition, b * c will be evaluated first. A table of operator precedence is provided later.\nAssociativity describes the rule where operators with the same precedence appear in an expression. For example, in expression a &plus; b  c, both &plus; and  have the same precedence, then which part of the expression will be evaluated first, is determined by associativity of those operators. Here, both &plus; and  are left associative, so the expression will be evaluated as \n.\nPrecedence and associativity determines the order of evaluation of an expression. Following is an operator precedence and associativity table (highest to lowest) −\nThe above table shows the default behavior of operators. At any point of time in expression evaluation, the order can be altered by using parenthesis. For example −\nIn \n, the expression part \n*\n will be evaluated first, with multiplication as precedence over addition. We here use parenthesis for \n to be evaluated first, like \n.\nWe shall now look at the algorithm on how to evaluate postfix notation −\nQueue, like Stack, is also an abstract data structure. The thing that makes queue different from stack is that a queue is open at both its ends. Hence, it follows FIFO (First-In-First-Out) structure, i.e. the data item inserted first will also be accessed first. The data is inserted into the queue through one end and deleted from it using the other end.\nA real-world example of queue can be a single-lane one-way road, where the vehicle enters first, exits first. More real-world examples can be seen as queues at the ticket windows and bus-stops.\nSimilar to the stack ADT, a queue ADT can also be implemented using arrays, linked lists, or pointers. As a small example in this tutorial, we implement queues using a one-dimensional array.\nQueue operations also include initialization of a queue, usage and permanently deleting the data from the memory.\nThe most fundamental operations in the queue ADT include: enqueue(), dequeue(), peek(), isFull(), isEmpty(). These are all built-in operations to carry out data manipulation and to check the status of the queue.\nQueue uses two pointers − \n and \n. The front pointer accesses the data from the front end (helping in enqueueing) while the rear pointer accesses data from the rear end (helping in dequeuing).\nThe \n is a data manipulation operation that is used to insert elements into the stack. The following algorithm describes the enqueue() operation in a simpler way.\nFollowing are the implementations of this operation in various programming languages −\nThe \n is a data manipulation operation that is used to remove elements from the stack. The following algorithm describes the dequeue() operation in a simpler way.\nFollowing are the implementations of this operation in various programming languages −\nThe peek() is an operation which is used to retrieve the frontmost element in the queue, without deleting it. This operation is used to check the status of the queue with the help of the pointer.\nFollowing are the implementations of this operation in various programming languages −\nThe isFull() operation verifies whether the stack is full.\nFollowing are the implementations of this operation in various programming languages −\nThe isEmpty() operation verifies whether the stack is empty. This operation is used to check the status of the stack with the help of top pointer.\nFollowing are the implementations of this operation in various programming languages −\nIn this chapter, the algorithm implementation of the Queue data structure is performed in four programming languages.\nA graph is an abstract data type (ADT) that consists of a set of objects that are connected to each other via links. These objects are called \n and the links are called \n.\nUsually, a graph is represented as G = {V, E}, where G is the graph space, V is the set of vertices and E is the set of edges. If E is empty, the graph is known as a \n.\nBefore we proceed further, let's familiarize ourselves with some important terms −\n − Each node of the graph is represented as a vertex. In the following example, the labelled circle represents vertices. Thus, A to G are vertices. We can represent them using an array as shown in the following image. Here A can be identified by index 0. B can be identified using index 1 and so on.\n − Edge represents a path between two vertices or a line between two vertices. In the following example, the lines from A to B, B to C, and so on represents edges. We can use a two-dimensional array to represent an array as shown in the following image. Here AB can be represented as 1 at row 0, column 1, BC as 1 at row 1, column 2 and so on, keeping other combinations as 0.\n − Two node or vertices are adjacent if they are connected to each other through an edge. In the following example, B is adjacent to A, C is adjacent to B, and so on.\n − Path represents a sequence of edges between the two vertices. In the following example, ABCD represents a path from A to D.\nThe primary operations of a graph include creating a graph with vertices and edges, and displaying the said graph. However, one of the most common and popular operation performed using graphs are Traversal, i.e. visiting every vertex of the graph in a specific order.\nThere are two types of traversals in Graphs −\nDepth First Search Traversal\nBreadth First Search Traversal\nDepth First Search is a traversal algorithm that visits all the vertices of a graph in the decreasing order of its depth. In this algorithm, an arbitrary node is chosen as the starting point and the graph is traversed back and forth by marking unvisited adjacent nodes until all the vertices are marked.\nThe DFS traversal uses the stack data structure to keep track of the unvisited nodes.\nBreadth First Search is a traversal algorithm that visits all the vertices of a graph present at one level of the depth before moving to the next level of depth. In this algorithm, an arbitrary node is chosen as the starting point and the graph is traversed by visiting the adjacent vertices on the same depth level and marking them until there is no vertex left.\nThe DFS traversal uses the queue data structure to keep track of the unvisited nodes.\nWhile representing graphs, we must carefully depict the elements (vertices and edges) present in the graph and the relationship between them. Pictorially, a graph is represented with a finite set of nodes and connecting links between them. However, we can also represent the graph in other most commonly used ways, like −\nAdjacency Matrix\nAdjacency List\nThe Adjacency Matrix is a VV matrix where the values are filled with either 0 or 1. If the link exists between Vi and Vj, it is recorded 1; otherwise, 0.\nFor the given graph below, let us construct an adjacency matrix −\nThe adjacency matrix is −\nThe adjacency list is a list of the vertices directly connected to the other vertices in the graph.\nThe adjacency list is −\nThere are two basic types of graph −\nDirected Graph\nUndirected Graph\nDirected graph, as the name suggests, consists of edges that possess a direction that goes either away from a vertex or towards the vertex. Undirected graphs have edges that are not directed at all.\nA \n is a subset of an undirected graph that contains all the vertices of the graph connected with the minimum number of edges in the graph. Precisely, the edges of the spanning tree is a subset of the edges in the original graph.\nIf all the vertices are connected in a graph, then there exists at least one spanning tree. In a graph, there may exist more than one spanning tree.\nA spanning tree does not have any cycle.\nAny vertex can be reached from any other vertex.\nIn the following graph, the highlighted edges form a spanning tree.\nA \n is a subset of edges of a connected weighted undirected graph that connects all the vertices together with the minimum possible total edge weight. To derive an MST, Prims algorithm or Kruskals algorithm can be used. Hence, we will discuss Prims algorithm in this chapter.\nAs we have discussed, one graph may have more than one spanning tree. If there are n number of vertices, the spanning tree should have  number of edges. In this context, if each edge of the graph is associated with a weight and there exists more than one spanning tree, we need to find the minimum spanning tree of the graph.\nMoreover, if there exist any duplicate weighted edges, the graph may have multiple minimum spanning tree.\nIn the above graph, we have shown a spanning tree though its not the minimum spanning tree. The cost of this spanning tree is \n.\nThe shortest path in a graph is defined as the minimum cost route from one vertex to another. This is most commonly seen in weighted directed graphs but are also applicable to undirected graphs.\nA popular real-world application of finding the shortest path in a graph is a map. Navigation is made easier and simpler with the various shortest path algorithms where destinations are considered vertices of the graph and routes are the edges. The two common shortest path algorithms are −\nDijkstras Shortest Path Algorithm\nBellman Fords Shortest Path Algorithm\nFollowing are the implementations of this operation in various programming languages −\nDepth First Search (DFS) algorithm traverses a graph in a depthward motion and uses a stack to remember to get the next vertex to start a search, when a dead end occurs in any iteration.\nAs in the example given above, DFS algorithm traverses from S to A to D to G to E to B first, then to F and lastly to C. It employs the following rules.\n − Visit the adjacent unvisited vertex. Mark it as visited. Display it. Push it in a stack.\n − If no adjacent vertex is found, pop up a vertex from the stack. (It will pop up all the vertices from the stack, which do not have adjacent vertices.)\n − Repeat Rule 1 and Rule 2 until the stack is empty.\nAs \n does not have any unvisited adjacent node so we keep popping the stack until we find a node that has an unvisited adjacent node. In this case, there's none and we keep popping until the stack is empty.\nBreadth First Search (BFS) algorithm traverses a graph in a breadthward motion and uses a queue to remember to get the next vertex to start a search, when a dead end occurs in any iteration.\nAs in the example given above, BFS algorithm traverses from A to B to E to F first then to C and G lastly to D. It employs the following rules.\n − Visit the adjacent unvisited vertex. Mark it as visited. Display it. Insert it in a queue.\n − If no adjacent vertex is found, remove the first vertex from the queue.\n − Repeat Rule 1 and Rule 2 until the queue is empty.\nAt this stage, we are left with no unmarked (unvisited) nodes. But as per the algorithm we keep on dequeuing in order to get all unvisited nodes. When the queue gets emptied, the program is over.\nA spanning tree is a subset of Graph G, which has all the vertices covered with minimum possible number of edges. Hence, a spanning tree does not have cycles and it cannot be disconnected..\nBy this definition, we can draw a conclusion that every connected and undirected Graph G has at least one spanning tree. A disconnected graph does not have any spanning tree, as it cannot be spanned to all its vertices.\nWe found three spanning trees off one complete graph. A complete undirected graph can have maximum \n number of spanning trees, where \n is the number of nodes. In the above addressed example, \n hence \n spanning trees are possible.\nWe now understand that one graph can have more than one spanning tree. Following are a few properties of the spanning tree connected to graph G −\nA connected graph G can have more than one spanning tree.\nAll possible spanning trees of graph G, have the same number of edges and vertices.\nThe spanning tree does not have any cycle (loops).\nRemoving one edge from the spanning tree will make the graph disconnected, i.e. the spanning tree is \n.\nAdding one edge to the spanning tree will create a circuit or loop, i.e. the spanning tree is \n.\nSpanning tree has \n edges, where \n is the number of nodes (vertices).\nFrom a complete graph, by removing maximum \n edges, we can construct a spanning tree.\nA complete graph can have maximum \n number of spanning trees.\nThus, we can conclude that spanning trees are a subset of connected Graph G and disconnected graphs do not have spanning tree.\nSpanning tree is basically used to find a minimum path to connect all nodes in a graph. Common application of spanning trees are −\nLet us understand this through a small example. Consider, city network as a huge graph and now plans to deploy telephone lines in such a way that in minimum lines we can connect to all city nodes. This is where the spanning tree comes into picture.\nIn a weighted graph, a minimum spanning tree is a spanning tree that has minimum weight than all other spanning trees of the same graph. In real-world situations, this weight can be measured as distance, congestion, traffic load or any arbitrary value denoted to the edges.\nWe shall learn about two most important spanning tree algorithms here −\nBoth are greedy algorithms.\nA tree is a non-linear abstract data type with a hierarchy-based structure. It consists of nodes (where the data is stored) that are connected via links. The tree data structure stems from a single node called a root node and has subtrees connected to the root.\nFollowing are the important terms with respect to tree.\n − Path refers to the sequence of nodes along the edges of a tree.\n − The node at the top of the tree is called root. There is only one root per tree and one path from the root node to any node.\n − Any node except the root node has one edge upward to a node called parent.\n − The node below a given node connected by its edge downward is called its child node.\n − The node which does not have any child node is called the leaf node.\n − Subtree represents the descendants of a node.\n − Visiting refers to checking the value of a node when control is on the node.\n − Traversing means passing through nodes in a specific order.\n − Level of a node represents the generation of a node. If the root node is at level 0, then its next child node is at level 1, its grandchild is at level 2, and so on.\n − Key represents a value of a node based on which a search operation is to be carried out for a node.\nThere are three types of trees −\nGeneral Trees\nBinary Trees\nBinary Search Trees\nGeneral trees are unordered tree data structures where the root node has minimum 0 or maximum n subtrees.\nThe General trees have no constraint placed on their hierarchy. The root node thus acts like the superset of all the other subtrees.\nBinary Trees are general trees in which the root node can only hold up to maximum 2 subtrees: left subtree and right subtree. Based on the number of children, binary trees are divided into three types.\nA full binary tree is a binary tree type where every node has either 0 or 2 child nodes.\nA complete binary tree is a binary tree type where all the leaf nodes must be on the same level. However, root and internal nodes in a complete binary tree can either have 0, 1 or 2 child nodes.\nA perfect binary tree is a binary tree type where all the leaf nodes are on the same level and every node except leaf nodes have 2 children.\nBinary Search Trees possess all the properties of Binary Trees including some extra properties of their own, based on some constraints, making them more efficient than binary trees.\nThe data in the Binary Search Trees (BST) is always stored in such a way that the values in the left subtree are always less than the values in the root node and the values in the right subtree are always greater than the values in the root node, i.e. left subtree < root node  right subtree.\nBinary Search Trees are more efficient than Binary Trees since time complexity for performing various operations reduces.\nSince the order of keys is based on just the parent node, searching operation becomes simpler.\nThe alignment of BST also favors Range Queries, which are executed to find values existing between two keys. This helps in the Database Management System.\nThe main disadvantage of Binary Search Trees is that if all elements in nodes are either greater than or lesser than the root node,\n. Simply put, the tree becomes slanted to one side completely.\nThis \n will make the tree a linked list rather than a BST, since the worst case time complexity for searching operation becomes O(n).\nTo overcome this issue of skewness in the Binary Search Trees, the concept of \n was introduced.\nConsider a Binary Search Tree with m as the height of the left subtree and n as the height of the right subtree. If the value of (m-n) is equal to 0,1 or -1, the tree is said to be a \n.\nThe trees are designed in a way that they self-balance once the height difference exceeds 1. Binary Search Trees use rotations as self-balancing algorithms. There are four different types of rotations: Left Left, Right Right, Left Right, Right Left.\nThere are various types of self-balancing binary search trees −\nAVL Trees\nRed Black Trees\nB Trees\nB+ Trees\nSplay Trees\nPriority Search Trees\nTraversal is a process to visit all the nodes of a tree and may print their values too. Because, all nodes are connected via edges (links) we always start from the root (head) node. That is, we cannot randomly access a node in a tree. There are three ways which we use to traverse a tree −\nIn-order Traversal\nPre-order Traversal\nPost-order Traversal\nGenerally, we traverse a tree to search or locate a given item or key in the tree or to print all the values it contains.\nIn this traversal method, the left subtree is visited first, then the root and later the right sub-tree. We should always remember that every node may represent a subtree itself.\nIf a binary tree is traversed \n, the output will produce sorted key values in an ascending order.\nWe start from \n, and following in-order traversal, we move to its left subtree \n.\n is also traversed in-order. The process goes on until all the nodes are visited. The output of in-order traversal of this tree will be −\nUntil all nodes are traversed −\n − Recursively traverse left subtree.\n − Visit root node.\n − Recursively traverse right subtree.\nFollowing are the implementations of this operation in various programming languages −\nIn this traversal method, the root node is visited first, then the left subtree and finally the right subtree.\nWe start from \n, and following pre-order traversal, we first visit \n itself and then move to its left subtree \n. \n is also traversed pre-order. The process goes on until all the nodes are visited. The output of pre-order traversal of this tree will be −\nUntil all nodes are traversed −\n − Visit root node.\n − Recursively traverse left subtree.\n − Recursively traverse right subtree.\nFollowing are the implementations of this operation in various programming languages −\nIn this traversal method, the root node is visited last, hence the name. First we traverse the left subtree, then the right subtree and finally the root node.\nWe start from \n, and following pre-order traversal, we first visit the left subtree \n. \n is also traversed post-order. The process goes on until all the nodes are visited. The output of post-order traversal of this tree will be \nUntil all nodes are traversed −\n − Recursively traverse left subtree.\n − Recursively traverse right subtree.\n − Visit root node.\nFollowing are the implementations of this operation in various programming languages −\nTo check the C implementation of tree traversing, please \nTraversal is a process to visit all the nodes of a tree and may print their values too. Because, all nodes are connected via edges (links) we always start from the root (head) node. That is, we cannot randomly access a node in a tree. There are three ways which we use to traverse a tree −\nIn-order Traversal\nPre-order Traversal\nPost-order Traversal\nWe shall now see the implementation of tree traversal in C programming language here using the following binary tree −\nFollowing are the implementations of this operation in various programming languages −\nA Binary Search Tree (BST) is a tree in which all the nodes follow the below-mentioned properties −\nThe left sub-tree of a node has a key less than or equal to its parent node's key.\nThe right sub-tree of a node has a key greater than or equal to its parent node's key.\nThus, BST divides all its sub-trees into two segments; the left sub-tree and the right sub-tree and can be defined as −\nBST is a collection of nodes arranged in a way where they maintain BST properties. Each node has a key and an associated value. While searching, the desired key is compared to the keys in BST and if found, the associated value is retrieved.\nFollowing is a pictorial representation of BST −\nWe observe that the root node key (27) has all less-valued keys on the left sub-tree and the higher valued keys on the right sub-tree.\nFollowing are the basic operations of a tree −\n − Searches an element in a tree.\n − Inserts an element in a tree.\n − Traverses a tree in a pre-order manner.\n − Traverses a tree in an in-order manner.\n − Traverses a tree in a post-order manner.\nDefine a node that stores some data, and references to its left and right child nodes.\nWhenever an element is to be searched, start searching from the root node. Then if the data is less than the key value, search for the element in the left subtree. Otherwise, search for the element in the right subtree. Follow the same algorithm for each node.\nFollowing are the implementations of this operation in various programming languages −\nWhenever an element is to be inserted, first locate its proper location. Start searching from the root node, then if the data is less than the key value, search for the empty location in the left subtree and insert the data. Otherwise, search for the empty location in the right subtree and insert the data.\nFollowing are the implementations of this operation in various programming languages −\nThe inorder traversal operation in a Binary Search Tree visits all its nodes in the following order −\nFirstly, we traverse the left child of the root node/current node, if any.\nNext, traverse the current node.\nLastly, traverse the right child of the current node, if any.\nFollowing are the implementations of this operation in various programming languages −\nThe preorder traversal operation in a Binary Search Tree visits all its nodes. However, the root node in it is first printed, followed by its left subtree and then its right subtree.\nFollowing are the implementations of this operation in various programming languages −\nLike the other traversals, postorder traversal also visits all the nodes in a Binary Search Tree and displays them. However, the left subtree is printed first, followed by the right subtree and lastly, the root node.\nFollowing are the implementations of this operation in various programming languages −\nFollowing are the implementations of this operation in various programming languages −\nThe first type of self-balancing binary search tree to be invented is the AVL tree. The name AVL tree is coined after its inventor's names − Adelson-Velsky and Landis.\nIn AVL trees, the difference between the heights of left and right subtrees, known as the \n, must be at most one. Once the difference exceeds one, the tree automatically executes the balancing algorithm until the difference becomes one again.\nThere are usually four cases of rotation in the balancing algorithm of AVL trees: LL, RR, LR, RL.\nLL rotation is performed when the node is inserted into the right subtree leading to an unbalanced tree. This is a single left rotation to make the tree balanced again −\nThe node where the unbalance occurs becomes the left child and the newly added node becomes the right child with the middle node as the parent node.\nRR rotation is performed when the node is inserted into the left subtree leading to an unbalanced tree. This is a single right rotation to make the tree balanced again −\nThe node where the unbalance occurs becomes the right child and the newly added node becomes the left child with the middle node as the parent node.\nLR rotation is the extended version of the previous single rotations, also called a double rotation. It is performed when a node is inserted into the right subtree of the left subtree. The LR rotation is a combination of the left rotation followed by the right rotation. There are multiple steps to be followed to carry this out.\nConsider an example with A as the root node, B as the left child of A and C as the right child of B.\nSince the unbalance occurs at A, a left rotation is applied on the child nodes of A, i.e. B and C.\nAfter the rotation, the C node becomes the left child of A and B becomes the left child of C.\nThe unbalance still persists, therefore a right rotation is applied at the root node A and the left child C.\nAfter the final right rotation, C becomes the root node, A becomes the right child and B is the left child.\nRL rotation is also the extended version of the previous single rotations, hence it is called a double rotation and it is performed if a node is inserted into the left subtree of the right subtree. The RL rotation is a combination of the right rotation followed by the left rotation. There are multiple steps to be followed to carry this out.\nConsider an example with A as the root node, B as the right child of A and C as the left child of B.\nSince the unbalance occurs at A, a right rotation is applied on the child nodes of A, i.e. B and C.\nAfter the rotation, the C node becomes the right child of A and B becomes the right child of C.\nThe unbalance still persists, therefore a left rotation is applied at the root node A and the right child C.\nAfter the final left rotation, C becomes the root node, A becomes the left child and B is the right child.\nThe basic operations performed on the AVL Tree structures include all the operations performed on a binary search tree, since the AVL Tree at its core is actually just a binary search tree holding all its properties. Therefore, basic operations performed on an AVL Tree are − \n and \n.\nThe data is inserted into the AVL Tree by following the Binary Search Tree property of insertion, i.e. the left subtree must contain elements less than the root value and right subtree must contain all the greater elements. However, in AVL Trees, after the insertion of each element, the balance factor of the tree is checked; if it does not exceed 1, the tree is left as it is. But if the balance factor exceeds 1, a balancing algorithm is applied to readjust the tree such that balance factor becomes less than or equal to 1 again.\nThe following steps are involved in performing the insertion operation of an AVL Tree −\n − Create a node\n − Check if the tree is empty\n − If the tree is empty, the new node created will become the root node of the AVL Tree.\n − If the tree is not empty, we perform the Binary Search Tree insertion operation and check the balancing factor of the node in the tree.\n − Suppose the balancing factor exceeds 1, we apply suitable rotations on the said node and resume the insertion from Step 4.\nLet us understand the insertion operation by constructing an example AVL tree with 1 to 7 integers.\nStarting with the first element 1, we create a node and measure the balance, i.e., 0.\nSince both the binary search property and the balance factor are satisfied, we insert another element into the tree.\nThe balance factor for the two nodes are calculated and is found to be -1 (Height of left subtree is 0 and height of the right subtree is 1). Since it does not exceed 1, we add another element to the tree.\nNow, after adding the third element, the balance factor exceeds 1 and becomes 2. Therefore, rotations are applied. In this case, the RR rotation is applied since the imbalance occurs at two right nodes.\nThe tree is rearranged as −\nSimilarly, the next elements are inserted and rearranged using these rotations. After rearrangement, we achieve the tree as −\nFollowing are the implementations of this operation in various programming languages −\nDeletion in the AVL Trees take place in three different scenarios −\n − If the node to be deleted is a leaf node, then it is deleted without any replacement as it does not disturb the binary search tree property. However, the balance factor may get disturbed, so rotations are applied to restore it.\n − If the node to be deleted has one child, replace the value in that node with the value in its child node. Then delete the child node. If the balance factor is disturbed, rotations are applied.\n − If the node to be deleted has two child nodes, find the inorder successor of that node and replace its value with the inorder successor value. Then try to delete the inorder successor node. If the balance factor exceeds 1 after deletion, apply balance algorithms.\nUsing the same tree given above, let us perform deletion in three scenarios −\nDeleting element 7 from the tree above −\nSince the element 7 is a leaf, we normally remove the element without disturbing any other node in the tree\nDeleting element 6 from the output tree achieved −\nHowever, element 6 is not a leaf node and has one child node attached to it. In this case, we replace node 6 with its child node: node 5.\nThe balance of the tree becomes 1, and since it does not exceed 1 the tree is left as it is. If we delete the element 5 further, we would have to apply the left rotations; either LL or LR since the imbalance occurs at both 1-2-4 and 3-2-4.\nThe balance factor is disturbed after deleting the element 5, therefore we apply LL rotation (we can also apply the LR rotation here).\nOnce the LL rotation is applied on path 1-2-4, the node 3 remains as it was supposed to be the right child of node 2 (which is now occupied by node 4). Hence, the node is added to the right subtree of the node 2 and as the left child of the node 4.\nDeleting element 2 from the remaining tree −\nAs mentioned in scenario 3, this node has two children. Therefore, we find its inorder successor that is a leaf node (say, 3) and replace its value with the inorder successor.\nThe balance of the tree still remains 1, therefore we leave the tree as it is without performing any rotations.\nFollowing are the implementations of this operation in various programming languages −\nIn the following implementation, we consider the inputs in ascending order and store them in AVL Trees by calculating the balance factor and applying rotations.\nFollowing are the implementations of this operation in various programming languages −\nRed-Black Trees are another type of the Balanced Binary Search Trees with two coloured nodes: Red and Black. It is a self-balancing binary search tree that makes use of these colours to maintain the balance factor during the insertion and deletion operations. Hence, during the Red-Black Tree operations, the memory uses 1 bit of storage to accommodate the colour information of each node\nIn Red-Black trees, also known as RB trees, there are different conditions to follow while assigning the colours to the nodes.\nThe root node is always black in colour.\nNo two adjacent nodes must be red in colour.\nEvery path in the tree (from the root node to the leaf node) must have the same amount of black coloured nodes.\nEven though AVL trees are more balanced than RB trees, with the balancing algorithm in AVL trees being stricter than that of RB trees, multiple and faster insertion and deletion operations are made more efficient through RB trees.\nThe operations on Red-Black Trees include all the basic operations usually performed on a Binary Search Tree. Some of the basic operations of an RB Tree include −\nInsertion\nDeletion\nSearch\nInsertion operation of a Red-Black tree follows the same insertion algorithm of a binary search tree. The elements are inserted following the binary search property and as an addition, the nodes are color coded as red and black to balance the tree according to the red-black tree properties.\nFollow the procedure given below to insert an element into a red-black tree by maintaining both binary search tree and red black tree properties.\n − Check whether the tree is empty; make the current node as the root and color the node black if it is empty.\n − But if the tree is not empty, we create a new node and color it red. Here we face two different cases −\nIf the parent of the new node is a black colored node, we exit the operation and tree is left as it is.\nIf the parent of this new node is red and the color of the parents sibling is either black or if it does not exist, we apply a suitable rotation and recolor accordingly.\nIf the parent of this new node is red and color of the parents sibling is red, recolor the parent, the sibling and grandparent nodes to black. The grandparent is recolored only if it is \n the root node; if it is the root node recolor only the parent and the sibling.\nLet us construct an RB Tree for the first 7 integer numbers to understand the insertion operation in detail −\nThe tree is checked to be empty so the first node added is a root and is colored black.\nNow, the tree is not empty so we create a new node and add the next integer with color red,\nThe nodes do not violate the binary search tree and RB tree properties, hence we move ahead to add another node.\nThe tree is not empty; we create a new red node with the next integer to it. But the parent of the new node is not a black colored node,\nThe tree right now violates both the binary search tree and RB tree properties; since parents sibling is NULL, we apply a suitable rotation and recolor the nodes.\nNow that the RB Tree property is restored, we add another node to the tree −\nThe tree once again violates the RB Tree balance property, so we check for the parents sibling node color, red in this case, so we just recolor the parent and the sibling.\nWe next insert the element 5, which makes the tree violate the RB Tree balance property once again.\nAnd since the sibling is NULL, we apply suitable rotation and recolor.\nNow, we insert element 6, but the RB Tree property is violated and one of the insertion cases need to be applied −\nThe parents sibling is red, so we recolor the parent, parents sibling and the grandparent nodes since the grandparent is not the root node.\nNow, we add the last element, 7, but the parent node of this new node is red.\nSince the parents sibling is NULL, we apply suitable rotations (RR rotation)\nThe final RB Tree is achieved.\nThe deletion operation on red black tree must be performed in such a way that it must restore all the properties of a binary search tree and a red black tree. Follow the steps below to perform the deletion operation on the red black tree −\nFirstly, we perform deletion based on the binary search tree properties.\n − If either the node to be deleted or the nodes parent is red, just delete it.\n − If the node is a double black, just remove the double black (double black occurs when the node to be deleted is a black colored leaf node, as it adds up the NULL nodes which are considered black colored nodes too)\n − If the double blacks sibling node is also a black node and its child nodes are also black in color, follow the steps below −\nRemove double black\nRecolor its parent to black (if the parent is a red node, it becomes black; if the parent is already a black node, it becomes double black)\nRecolor the parents sibling with red\nIf double black node still exists, we apply other cases.\n − If the double black nodes sibling is red, we perform the following steps −\nSwap the colors of the parent node and the parents sibling node.\nRotate parent node in the double blacks direction\nReapply other cases that are suitable.\n − If the double blacks sibling is a black node but the siblings child node that is closest to the double black is red, follows the steps below −\nSwap the colors of double blacks sibling and the siblings child in question\nRotate the sibling node is the opposite direction of double black (i.e. if the double black is a right child apply left rotations and vice versa)\nApply case 6.\n − If the double blacks sibling is a black node but the siblings child node that is farther to the double black is red, follows the steps below −\nSwap the colors of double blacks parent and sibling nodes\nRotate the parent in double blacks direction (i.e. if the double black is a right child apply right rotations and vice versa)\nRemove double black\nChange the color of red child node to black.\nConsidering the same constructed Red-Black Tree above, let us delete few elements from the tree.\nDelete elements 4, 5, 3 from the tree.\nTo delete the element 4, let us perform the binary search deletion first.\nAfter performing the binary search deletion, the RB Tree property is not disturbed, therefore the tree is left as it is.\nThen, we delete the element 5 using the binary search deletion\nBut the RB property is violated after performing the binary search deletion, i.e., all the paths in the tree do not hold same number of black nodes; so we swap the colors to balance the tree.\nThen, we delete the node 3 from the tree obtained −\nApplying binary search deletion, we delete node 3 normally as it is a leaf node. And we get a double node as 3 is a black colored node.\nWe apply case 3 deletion as double blacks sibling node is black and its child nodes are also black. Here, we remove the double black, recolor the double blacks parent and sibling.\nAll the desired nodes are deleted and the RB Tree property is maintained.\nThe search operation in red-black tree follows the same algorithm as that of a binary search tree. The tree is traversed and each node is compared with the key element to be searched; if found it returns a successful search. Otherwise, it returns an unsuccessful search.\nB trees are extended binary search trees that are specialized in m-way searching, since the order of B trees is m. Order of a tree is defined as the maximum number of children a node can accommodate. Therefore, the height of a b tree is relatively smaller than the height of AVL tree and RB tree.\nThey are general form of a Binary Search Tree as it holds more than one key and two children.\nThe various properties of B trees include −\nEvery node in a B Tree will hold a maximum of m children and (m-1) keys, since the order of the tree is m.\nEvery node in a B tree, except root and leaf, can hold at least m/2 children\nThe root node must have no less than two children.\nAll the paths in a B tree must end at the same level, i.e. the leaf nodes must be at the same level.\nA B tree always maintains sorted data.\nB trees are also widely used in disk access, minimizing the disk access time since the height of a b tree is low.\n − A disk access is the memory access to the computer disk where the information is stored and disk access time is the time taken by the system to access the disk memory.\nThe operations supported in B trees are Insertion, deletion and searching with the time complexity of \n for every operation.\nThe insertion operation for a B Tree is done similar to the Binary Search Tree but the elements are inserted into the same node until the maximum keys are reached. The insertion is done using the following procedure −\n − Calculate the maximum $\\mathrm{\\left ( m-1 \\right )}$ and minimum $\\mathrm{\\left ( \\left \\lceil \\frac{m}{2}\\right \\rceil-1 \\right )}$ number of keys a node can hold, where m is denoted by the order of the B Tree.\n − The data is inserted into the tree using the binary search insertion and once the keys reach the maximum number, the node is split into half and the median key becomes the internal node while the left and right keys become its children.\n − All the leaf nodes must be on the same level.\nThe keys, 5, 3, 21, 9, 13 are all added into the node according to the binary search property but if we add the key 22, it will violate the maximum key property. Hence, the node is split in half, the median key is shifted to the parent node and the insertion is then continued.\nAnother hiccup occurs during the insertion of 11, so the node is split and median is shifted to the parent.\nWhile inserting 16, even if the node is split in two parts, the parent node also overflows as it reached the maximum keys. Hence, the parent node is split first and the median key becomes the root. Then, the leaf node is split in half the median of leaf node is shifted to its parent.\nThe final B tree after inserting all the elements is achieved.\nThe deletion operation in a B tree is slightly different from the deletion operation of a Binary Search Tree. The procedure to delete a node from a B tree is as follows −\n − If the key to be deleted is in a leaf node and the deletion does not violate the minimum key property, just delete the node.\n − If the key to be deleted is in a leaf node but the deletion violates the minimum key property, borrow a key from either its left sibling or right sibling. In case if both siblings have exact minimum number of keys, merge the node in either of them.\n − If the key to be deleted is in an internal node, it is replaced by a key in either left child or right child based on which child has more keys. But if both child nodes have minimum number of keys, theyre merged together.\n − If the key to be deleted is in an internal node violating the minimum keys property, and both its children and sibling have minimum number of keys, merge the children. Then merge its sibling with its parent.\nFollowing is functional C++ code snippet of the deletion operation in B Trees −\nThe B+ trees are extensions of B trees designed to make the insertion, deletion and searching operations more efficient.\nThe properties of B+ trees are similar to the properties of B trees, except that the B trees can store keys and records in all internal nodes and leaf nodes while B+ trees store records in leaf nodes and keys in internal nodes. One profound property of the B+ tree is that all the leaf nodes are connected to each other in a single linked list format and a data pointer is available to point to the data present in disk file. This helps fetch the records in equal numbers of disk access.\nSince the size of main memory is limited, B+ trees act as the data storage for the records that couldnt be stored in the main memory. For this, the internal nodes are stored in the main memory and the leaf nodes are stored in the secondary memory storage.\nEvery node in a B+ Tree, except root, will hold a maximum of \n children and (m-1) keys, and a minimum of $\\mathrm{\\left \\lceil \\frac{m}{2}\\right \\rceil}$ children and $\\mathrm{\\left \\lceil \\frac{m-1}{2}\\right \\rceil}$ keys, since the order of the tree is \n.\nThe root node must have no less than two children and at least one search key.\nAll the paths in a B tree must end at the same level, i.e. the leaf nodes must be at the same level.\nA B+ tree always maintains sorted data.\nThe operations supported in B+ trees are Insertion, deletion and searching with the time complexity of \n for every operation.\nThey are almost similar to the B tree operations as the base idea to store data in both data structures is same. However, the difference occurs as the data is stored only in the leaf nodes of a B+ trees, unlike B trees.\nThe insertion to a B+ tree starts at a leaf node.\n − Calculate the maximum and minimum number of keys to be added onto the B+ tree node.\n − Insert the elements one by one accordingly into a leaf node until it exceeds the maximum key number.\n − The node is split into half where the left child consists of minimum number of keys and the remaining keys are stored in the right child.\n − But if the internal node also exceeds the maximum key property, the node is split in half where the left child consists of the minimum keys and remaining keys are stored in the right child. However, the smallest number in the right child is made the parent.\n − If both the leaf node and internal node have the maximum keys, both of them are split in the similar manner and the smallest key in the right child is added to the parent node.\nThe deletion operation in a B+ tree, we need to consider the redundancy in the data structure.\n − If the key is present in a leaf node which has more than minimal number of keys, without its copy present in the internal nodes, simple delete it.\n − If the key is present in a leaf node with exactly minimal number of keys and a copy of it is not present in the internal nodes, borrow a key from its sibling node and delete the desired key.\n − If the key present in the leaf node has its copy in the internal nodes, there are multiple scenarios possible −\nMore than minimal keys present in both leaf node and internal node: simply delete the key and add the inorder successor to the internal node only.\nExact minimum number of keys present in the leaf node: delete the node and borrow a node from its immediate sibling and replace its value in internal node as well.\nIf the copy of the leaf node to be delete is in its grandparent, delete the node and remove the empty space. The grandparent is filled with the inorder successor of the deleted node.\n − If the key to be deleted is in a node violating the minimum keys property, both its parent and sibling have minimum number of keys, delete the key and merge its sibling with its parent.\nSplay trees are the altered versions of the Binary Search Trees, since it contains all the operations of BSTs, like insertion, deletion and searching, followed by another extended operation called \n.\nFor instance, a value A is supposed to be inserted into the tree. If the tree is empty, add A to the root of the tree and exit; but if the tree is not empty, use binary search insertion operation to insert the element and then perform splaying on the new node.\nSimilarly, after searching an element in the splay tree, the node consisting of the element must be splayed as well.\n Splaying, in simpler terms, is just a process to bring an operational node to the root. There are six types of rotations for it.\nZig rotation\nZag rotation\nZig-Zig rotation\nZag-Zag rotation\nZig-Zag rotation\nZag-Zig rotation\nThe zig rotations are performed when the operational node is either the root node or the left child node of the root node. The node is rotated towards its right.\nAfter the shift, the tree will look like −\nThe zag rotations are also performed when the operational node is either the root node or the right child nod of the root node. The node is rotated towards its left.\nThe operational node becomes the root node after the shift −\nThe zig-zig rotations are performed when the operational node has both parent and a grandparent. The node is rotated two places towards its right.\nThe first rotation will shift the tree to one position right −\nThe second right rotation will once again shift the node for one position. The final tree after the shift will look like this −\nThe zag-zag rotations are also performed when the operational node has both parent and a grandparent. The node is rotated two places towards its left.\nAfter the first rotation, the tree will look like −\nThen the final tree after the second rotation is given as follows. However, the operational node is still not the root so the splaying is considered incomplete. Hence, other suitable rotations are again applied in this case until the node becomes the root.\nThe zig-zag rotations are performed when the operational node has both a parent and a grandparent. But the difference is the grandparent, parent and child are in LRL format. The node is rotated first towards its right followed by left.\nAfter the first rotation, the tree is  −\nThe final tree after the second rotation −\nThe zag-zig rotations are also performed when the operational node has both parent and grandparent. But the difference is the grandparent, parent and child are in RLR format. The node is rotated first towards its left followed by right.\nFirst rotation is performed, the tree is obtained as −\nAfter second rotation, the final tree is given as below. However, the operational node is not the root node yet so one more rotation needs to be performed to make the said node as the root.\nA splay contains the same basic operations that a Binary Search Tree provides with: Insertion, Deletion, and Search. However, after every operation there is an additional operation that differs them from Binary Search tree operations: Splaying. We have learned about Splaying already so let us understand the procedures of the other operations.\nThe insertion operation in a Splay tree is performed in the exact same way insertion in a binary search tree is performed. The procedure to perform the insertion in a splay tree is given as follows −\nCheck whether the tree is empty; if yes, add the new node and exit\nIf the tree is not empty, add the new node to the existing tree using the binary search insertion.\nThen, suitable splaying is chosen and applied on the newly added node.\nFollowing are the implementations of this operation in various programming languages −\nThe deletion operation in a splay tree is performed as following −\nApply splaying operation on the node to be deleted.\nOnce, the node is made the root, delete the node.\nNow, the tree is split into two trees, the left subtree and the right subtree; with their respective first nodes as the root nodes: say root_left and root_right.\nIf root_left is a NULL value, then the root_right will become the root of the tree. And vice versa.\nBut if both root_left and root_right are not NULL values, then select the maximum value from the left subtree and make it the new root by connecting the subtrees.\nFollowing are the implementations of this operation in various programming languages −\nThe search operation in a Splay tree follows the same procedure of the Binary Search Tree operation. However, after the searching is done and the element is found, splaying is applied on the node searched. If the element is not found, then unsuccessful search is prompted.\nFollowing are the implementations of this operation in various programming languages −\nA trie is a type of a multi-way search tree, which is fundamentally used to retrieve specific keys from a string or a set of strings. It stores the data in an ordered efficient way since it uses pointers to every letter within the alphabet.\nThe trie data structure works based on the common prefixes of strings. The root node can have any number of nodes considering the amount of strings present in the set. The root of a trie does not contain any value except the pointers to its child nodes.\nThere are three types of trie data structures −\nStandard Tries\nCompressed Tries\nSuffix Tries\nThe real-world applications of trie include − autocorrect, text prediction, sentiment analysis and data sciences.\nThe trie data structures also perform the same operations that tree data structures perform. They are −\nInsertion\nDeletion\nSearch\nThe insertion operation in a trie is a simple approach. The root in a trie does not hold any value and the insertion starts from the immediate child nodes of the root, which act like a key to their child nodes. However, we observe that each node in a trie represents a singlecharacter in the input string. Hence the characters are added into the tries one by one while the links in the trie act as pointers to the next level nodes.\nThe deletion operation in a trie is performed using the bottom-up approach. The element is searched for in a trie and deleted, if found. However, there are some special scenarios that need to be kept in mind while performing the deletion operation.\n − The key is unique − in this case, the entire key path is deleted from the node. (Unique key suggests that there is no other path that branches out from one path).\n − The key is not unique − the leaf nodes are updated. For example, if the key to be deleted is \n but it is a prefix of another key \n; we delete the see and change the Boolean values of t, h and e as false.\n − The key to be deleted already has a prefix − the values until the prefix are deleted and the prefix remains in the tree. For example, if the key to be deleted is \n but there is another key present \n; so we delete a, r, and t until only he remains.\nSearching in a trie is a rather straightforward approach. We can only move down the levels of trie based on the key node (the nodes where insertion operation starts at). Searching is done until the end of the path is reached. If the element is found, search is successful; otherwise, search is prompted unsuccessful.\nHeap is a special case of balanced binary tree data structure where the root-node key is compared with its children and arranged accordingly. If \n has child node \n then −\nkey(α) ≥ key(β)\nAs the value of parent is greater than that of child, this property generates \n. Based on this criteria, a heap can be of two types −\n − Where the value of the root node is less than or equal to either of its children.\n − Where the value of the root node is greater than or equal to either of its children.\nBoth trees are constructed using the same input and order of arrival.\nWe shall use the same example to demonstrate how a Max Heap is created. The procedure to create Min Heap is similar but we go for min values instead of max values.\nWe are going to derive an algorithm for max heap by inserting one element at a time. At any point of time, heap must maintain its property. While insertion, we also assume that we are inserting a node in an already heapified tree.\n − In Min Heap construction algorithm, we expect the value of the parent node to be less than that of the child node.\nLet's understand Max Heap construction by an animated illustration. We consider the same input sample that we used earlier.\nLet us derive an algorithm to delete from max heap. Deletion in Max (or Min) Heap always happens at the root to remove the Maximum (or minimum) value.\nSome computer programming languages allow a module or function to call itself. This technique is known as recursion. In recursion, a function \n either calls itself directly or calls a function \n that in turn calls the original function \n. The function \n is called recursive function.\n − a function calling itself.\n − a function that calls another function which in turn calls it again.\nA recursive function can go infinite like a loop. To avoid infinite running of recursive function, there are two properties that a recursive function must have −\n − There must be at least one base criteria or condition, such that, when this condition is met the function stops calling itself recursively.\n − The recursive calls should progress in such a way that each time a recursive call is made it comes closer to the base criteria.\nMany programming languages implement recursion by means of \n. Generally, whenever a function (\n) calls another function (\n) or itself as callee, the caller function transfers execution control to the callee. This transfer process may also involve some data to be passed from the caller to the callee.\nThis implies, the caller function has to suspend its execution temporarily and resume later when the execution control returns from the callee function. Here, the caller function needs to start exactly from the point of execution where it puts itself on hold. It also needs the exact same data values it was working on. For this purpose, an activation record (or stack frame) is created for the caller function.\nThis activation record keeps the information about local variables, formal parameters, return address and all information passed to the caller function.\nOne may argue why to use recursion, as the same task can be done with iteration. The first reason is, recursion makes a program more readable and because of latest enhanced CPU systems, recursion is more efficient than iterations.\nIn case of iterations, we take number of iterations to count the time complexity. Likewise, in case of recursion, assuming everything is constant, we try to figure out the number of times a recursive call is being made. A call made to a function is Ο(1), hence the (n) number of times a recursive call is made makes the recursive function Ο(n).\nSpace complexity is counted as what amount of extra space is required for a module to execute. In case of iterations, the compiler hardly requires any extra space. The compiler keeps updating the values of variables used in the iterations. But in case of recursion, the system needs to store activation record each time a recursive call is made. Hence, it is considered that space complexity of recursive function may go higher than that of a function with iteration.\nTower of Hanoi, is a mathematical puzzle which consists of three towers (pegs) and more than one rings is as depicted −\nThese rings are of different sizes and stacked upon in an ascending order, i.e. the smaller one sits over the larger one. There are other variations of the puzzle where the number of disks increase, but the tower count remains the same.\nThe mission is to move all the disks to some another tower without violating the sequence of arrangement. A few rules to be followed for Tower of Hanoi are −\nFollowing is an animated representation of solving a Tower of Hanoi puzzle with three disks.\nTower of Hanoi puzzle with n disks can be solved in minimum \n steps. This presentation shows that a puzzle with 3 disks has taken \n steps.\nTo write an algorithm for Tower of Hanoi, first we need to learn how to solve this problem with lesser amount of disks, say → 1 or 2. We mark three towers with name, \n, \n and \n (only to help moving the disks). If we have only one disk, then it can easily be moved from source to destination peg.\nIf we have 2 disks −\nSo now, we are in a position to design an algorithm for Tower of Hanoi with more than two disks. We divide the stack of disks in two parts. The largest disk (n\n disk) is in one part and all other (n-1) disks are in the second part.\nOur ultimate aim is to move disk \n from source to destination and then put all other (n1) disks onto it. We can imagine to apply the same in a recursive way for all given set of disks.\nThe steps to follow are −\nA recursive algorithm for Tower of Hanoi can be driven as follows −\nFibonacci series generates the subsequent number by adding two previous numbers. Fibonacci series starts from two numbers  \n. The initial values of F\n & F\n can be taken 0, 1 or 1, 1 respectively.\nFibonacci series satisfies the following conditions −\nHence, a Fibonacci series can look like this −\nF\n = 0 1 1 2 3 5 8 13\nor, this −\nF\n = 1 1 2 3 5 8 13 21\nFor illustration purpose, Fibonacci of F\n is displayed as −\nFirst we try to draft the iterative algorithm for Fibonacci series.\nLet us learn how to create a recursive algorithm Fibonacci series. The base criteria of recursion.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/cycle_sort_interview_questions_and_answers.htm", "title": "Cycle Sort Interview Questions and Answers", "content": "Cycle Sort is a comparison-based, in-place, and unstable \n. We prefer to use this algorithm when the given numbers range from 1 to n. This range of questions covers the most important and popular questions of the cycle sort. The following are the most asked interview questions on the cycle sort algorithm.\nThere are two approaches to performing a cycle sort. Both approaches are given below:\nThis approach is the preferable method when given numbers ranging from 1 to n; then, we are sorting elements by the use of one loop. The following are method 1 interview questions:\nCycle sort is the in-place sorting technique used to sort the array in one go. We use this algorithm for less usage of memory, which will be helpful in efficient sorting.\nWhen we have less memory space to use and we have to perform the whole functionality of the algorithm in one loop, then we use a cycle sort.\nCycle sort counts how many items in the array are smaller than the current element. If 3 elements are smaller than the current element, it belongs in position 3 (using 0-based indexing).\nCycle sort takes n-1 swaps for n elements. Swapping needs each element except one needs to move exactly once.\nThe time complexity of both algorithms is slow O(n). Cycle Sort takes less memory space. \n is moving everything one by one to its place, while cycle sort creates efficient chains of movements.\nWhen the elements are sorted, then it is the best case. It takes time complexity of O(n). It takes no additional memory writes because everything is already in place.\nWe use this approach when numbers are given in the range of 1 to n; then, we compare each element with all other elements of an array. In this, numbers are given randomly. Method 2 explanations are given below:\nCycle sort is the in-place sorting technique used to sort the array by comparing two elements. If one element is more than the second, then its position will be maintained otherwise, it will be swapped. We use this algorithm for less usage of memory, which will be helpful in efficient sorting.\nCycle sort counts how many items in the array are smaller than the current element. If 3 elements are smaller than the current element, it belongs in position 3 (using 0-based indexing).\nCycle sort takes n-1 swaps for n elements. Swapping needs each element except one, which needs to move exactly once.\nCycle sort takes O(n) time because it checks every element of an array. But it makes no additional memory writes because everything is already in place.\nSome questions are common in both the approaches that are given below:\nThe space complexity of the cycle sort is O(1). It takes only auxiliary space, and it is also an in-place algorithm.\nCycle sort takes less memory, but it needs too many comparisons. For example, saving petrol but taking the longest route possible with the overall trip is still slow.\nCycle Sort is not a stable algorithm because when duplicate elements appear, it changes their original order. It doesn't take care of maintaining the sequence of identical values.\nIt places them one after another. If it sees multiple 5s, it puts them in consecutive positions after figuring out how many elements are smaller than 5.\nFirstly, we check if an element is already in its correct position before starting a new cycle. If it is, then we move to the next element without performing any writes. This type of case helps to maintain the algorithm's efficiency and minimum memory writes.\nA cycle is like a game of musical chairs. Element A goes to its correct spot, displacing element B, which goes to its spot, displacing element C, and so on, until an element lands in A's original position, completing the cycle.\nCycle sort will sort in descending order when we count larger elements to determine position instead of counting smaller elements.\nIf we talk about the writes, n-1 is the best case. This consistency in writing is Cycle Sort's main strength.\nInsertion sort is used to sort the whole array by sorting one element at a time by repeatedly inserting and shifting elements. Cycle sort finds the exact final position of each element before placing it there.\nCycle sort is not adaptive because it can't work faster on partially sorted arrays and always performs the same number of comparisons.\nUse binary search instead of counting one by one to find where elements belong, reducing comparison time.\nWe can't use cycle sort with linked lists because it needs to pick an element non-contagiously around the array. Linked lists don't support quick random access like a cycle sort.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/radix_sort_interview_questions_and_answers.htm", "title": "Radix Sort Interview Questions and Answers", "content": "is the \n that sorts the elements by processing the digits one by one. It takes either the largest element or the smallest element of an array based on the processing of the digits. The following are the most common problems on radix sort.\nFollowing are the top interview questions on the radix sort:\n is a sorting algorithm that sorts elements by processing their digits one by one. It starts the sorting by selecting the least or highest digit number in the array.\nRadix sort is better when the range of numbers isn't too large and the number of digits is small.\n can handle the floating-point numbers. It is also needed for decimals and negative numbers.\n can run in near-linear time when the number of digits is small.\nRadix Sort uses a \n or \n to sort individual digits. However, these data structures are rarely used.\n is a stable algorithm because it maintains the original order of equal elements.\n uses numbers as integers or floating-point and strings that can be broken into components.\n needs extra memory, isn't as cache-friendly, and doesn't work well for all data types.\n common approach is to separate positive and negative numbers, sort them separately, and then merge them.\n refers to the base of the numbering system. For example, it uses 10 for decimal and 2 for binary.\n can be parallelized efficiently because digits are processed independently.\n works in one pass for a known range, while Radix Sort repeatedly sorts digits.\n characters from right to left, just like sorting numbers by digits.\n best, worst, and average cases are all O(d(n+k)) because every digit must be processed.\n, but it does need to know the largest number of digits in the input.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/bucket_sort_interview_questions_and_answers.htm", "title": "Bucket Sort Interview Questions and Answers", "content": "is a non-comparison-based \n. It is used to sort the elements that are present in the \n range. We prefer to use this algorithm when numbers are uniformly distributed. This page covers the most important interview questions on bucket sort.\nThe following are the top questions on the bucket sort:\nBucket sort is a distribution-based sorting technique used to distribute the elements into the range of buckets. After the distribution of all elements, it sorts each bucket by using some stable algorithm like \n, \n, etc. When all buckets are sorted individually, then data are transferred into an \n, and after a transfer, all the buckets are emptied. Now, we have the array in sorted form.\nThe time complexity of bucket sort in all the cases is given below:\nThe space complexity of bucket sort is O(n+k), where n is the number of input elements and k is the number of buckets.\nBucket sort is most efficient when we follow the following steps:\nWe can use the optimal number of buckets by selecting the buckets according to the number of the elements. A common rule of thumb is to use n buckets for reasonable performance and also take care of available memory. We can use uniformly distributed data for getting the linear time complexity.\nWe prefer to use the stable algorithm insertion sort because it performs well with smaller datasets and is efficient for nearly sorted data. We can also use other algorithms like quicksort, mergesort, etc.\nBucket sort skips the empty bucket during concatenation by the implementation of checks in it.\nWe have to use a data structure that handles the sparse nature of buckets. Skipping does not affect the correctness of an algorithm, but efficiency will be affected.\nIf the input follows the uniform distribution, then it gives the best performance. But, if the distribution is skewed, performance can be degraded because more elements cluster in fewer buckets. When all the elements are in the same bucket, then it leads to the worst-case scenario.\nWhen all the elements are present in a single bucket, then we have to perform an insertion sort for the internal sorting of the bucket. It leads to the worst case scenario, which takes the time of O(n).\nWe have to find the minimum value in the array and add its absolute value to all elements to make them non-negative. Then, perform \n on the adjusted values. After sorting, subtract the same absolute value from each element to restore the original range.\nBucket sort can be stable if elements are inserted into the buckets in their original order, and when we concatenate, it preserves the order of the buckets. It ensures that equal elements maintain their relative order from the input array.\nThe differences between \n, \n, and \n are given below:\nWe can use bucket sort for sorting strings and non-integer values; for that, we have to define a mapping function that can change the character to numeric values by ASCII. After converting, the elements are distributed into the buckets and perform the bucket sort. After sorting, it will again be converted into characters by ASCII.\nBucket sort performs poorly when the data is highly skewed or clustered, all elements are present in a single bucket. When the range of values is very high compared to other elements. When the memory is less, it will be unpredictable due to extra space needed for the buckets.\nWe can parallelize the bucket sort by distributing elements into buckets in parallel, and sorting it by using individual buckets paralelly. We also have to use separate processes and use parallel merge for the final concatenation stage. These things make it highly suitable for distributed systems and parallel computing environments.\nFor implementing the external memory version, we have to scan the data once to know about the value of the elements and create buckets like a separate file on disk. In stream data, each element goes to its appropriate bucket file and is sorted individually by using insertion sort. At the end, we have to concatenate the sorted bucket files in sequence. By this method, we can sort larger datasets than available \n.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/bubble_sort_interview_questions.htm", "title": "Bubble Sort Interview Questions and Answers", "content": "is the \n that starts by comparing the first two elements, and at the end, it gets the largest element from the first cycle. This cycle runs until the whole unsorted array is sorted. This article covers all the basic level questions to advanced level interview questions. It helps to learn good topics of bubble sort.\nFollowing are the Bubble Sort interview question answers:\nBubble Sort is used to compare the two elements from an array in which it starts from the first two elements to the last two elements. It compares only adjacent elements and swaps them if they are in the wrong order. After every cycle, you obtain the highest element of the cycle, and this top element is not taken into consideration in the next cycle. The number of cycles is based on the number of elements in the array.\nThe worst and best case time complexity of Bubble Sort is O(N), and the average case time complexity is O(N). The space complexity is O(1), as it requires only a constant amount of additional space.\nBubble Sort is a comparison-based sorting algorithm.\nThe greatest strength of Bubble Sort is that it is easy to understand and implement. It is easy to write and read.\nThe greatest weakness of Bubble Sort is that it has inefficient performance for sorting larger sets of data. When it sorts the large datasets, then it gives the high time complexity.\nBubble Sort is good for sorting data when there are small data sets or data is already sorted. It is good when we need simplicity instead of efficiency.\nBubble Sort is rarely used in practical applications because of its inefficiency with large data sets. However, it can be used for instructional purposes and small data sets where we don't need efficiency.\nThe Bubble Sort algorithm is used to compare adjacent elements and swap them when they are unsorted. This is repeated until the list of elements becomes sorted. Let's understand the workings of the Bubble Sort With the following example:\nStep 1. [5, 2, 9, 3] - Swapping the first two elements that are 5 and 2.\nStep 2. [2, 5, 9, 3] - Now, it can't swap the 5 and 9 because 5 is shorter than 9.\nStep 3. [2, 5, 9, 3] - Now it will switch to the third set, which is 9 and 3.\nStep 4. [2, 5, 3, 9] - Now the first cycle is completed, and this cycle will run till no swaps are left in any cycle.\nWe can optimize bubble sort by putting a flag to check if any swap was made during a pass through the array. If no swap was made, then the array is sorted, and the algorithm can be stopped early.\nAn \"in-place\" sorting algorithm sorts the elements of an array without using extra space. It sorts the elements of an original array without using any additional data structures for storage.\nThe word \"Bubble\" of Bubble Sort tells the sorting process of the list where small elements \"bubble\" up on the top of the list. This shows how small values easily come into place.\nBubble Sort can be used to sort linked lists. It is not the best algorithm for sorting and as well as big linked lists. We have better sorting algorithms for sorting this data structure.\nBubble Sort is known as a stable sorting algorithm. This is because it preserves the relative order of equal elements in the list. In this, two elements with the same value in the sorted array will retain their original order.\nThe inner loop does the comparison and swapping of the next two elements to place the biggest unsorted element at the array end.\nYes, Bubble Sort can sort non-integer data, provided that a proper comparison function is set up for the data type.\nBubble Sort is simple to implement as well as to understand. Hence, it is excellent for educational needs. It's a good candidate for educational environments and small dataset collections where there is no stress about complexity based on its simple nature.\nWhen you have an already sorted array and apply a Bubble Sort algorithm to it, the Bubble Sort will perform a single pass and determine that the array has been sorted.\nAdvantages: Easy to implement and understand, ideal for educational use.\nDisadvantages: Inefficient for big data, not stable, and high time complexity.\nYes, Bubble Sort can sort a partially sorted array more effectively by employing the \"optimized Bubble Sort\" method, which avoids unnecessary comparisons and swaps.\nBubble Sort is the least efficient among the sorting algorithms with a time complexity of O(N), whereas the time complexities of Merge Sort and Quick Sort are O(N log N).\nThe best case for Bubble Sort is when the array is sorted. The time complexity of the best case is O(N). In this, the algorithm checks one time in the array where its no swaps at all.\nThe worst-case scenario of Bubble Sort is when the array is sorted in reverse order. Time complexity in that case is O(N) because the algorithm has to do N swaps through the array, where each pass has up to N comparisons and swaps.\nBubble Sort is recursively called. Usually, it is not common practice. Using bubble sort recursively can head to function calling and the possibility of stack overflow for large arrays.\nBubble Sort is a stable sorting algorithm that keeps duplicate elements in the original relative order. If there are any two same elements, they would not be swapped, which will help to keep their relative position.\nThe outer loop of the Bubble Sort takes care of the passes through the array. Each pass shifts the largest element into its correct position, which is at the end of an array.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/heap_sort_interview_questions.htm", "title": "Heap Sort Interview Questions and Answers", "content": "is the \n that sorts the array with the use of the heapify method. In this set of questions, we provide the best and most important questions regarding interviews. This range can clear the concept from the basic to the advanced level. Heap Sort interview questions give you detailed knowledge about the heaps and heapify processes.\nFollowing are the top heap sort interview question answers:\nThe Heap Sort uses the \n to sort the elements. First, it constructs a maximum heap using the given input. Then, it takes the maximum element and creates a new heap. This process is repeated until the entire array is sorted. The time complexity of the Heap sort is O(n log n).\nThe steps to perform a heap sort are as follows:\nThe functions of the heap sort are to create a maximum heap from the given elements, continually remove the top element from the heap, and reconstruct the heap until every element is sorted.\nMax-heapify and min-heapify are the techniques that use the heap property for max heaps and min heaps, respectively. Max-heapify is applied to make a parent node greater than its child nodes, while min-heapify is applied to make a parent node lesser than its children.\nWe can sort an array with a heapsort without using a heap. For this, we have to take an input array as a binary heap. We also have to use heapify operations to rebuild its elements.\nWe use heap sort when we need stability and consistency because it is an in-place sorting algorithm with O(n log n) time complexity.\nHeapsort is better than mergesort in terms of space complexity because it is an in-place sort, whereas mergesort needs additional memory space for the size of the input.\nHeapsort's worst-case time complexity is O(n log n) because the algorithm needs to search the entire heap to find the proper element to swap with the root node. In this process, the heap is fully unbalanced.\nA binary tree is not as preferred to implement heaps over arrays due to the better memory locality of arrays, which makes it more efficient to access elements. Furthermore, arrays provide easier indexing and storage of elements in a single contiguous memory location, which is essential for effective heap structure management.\nThe heap order property tells the core characteristic of a heap structure. For a min-heap, every parent node contains a value smaller than or equal to its children's values. On the opposite, in a max-heap, each parent node contains a value larger than or equal to its children's values. When the heap is valid, the relationship between parent and child nodes is consistently maintained.\nHeap Sort is not stable because when it is sorting the elements, it never maintains the original relative order of equal elements.\nSpace complexity for heapsort is the amount of memory space to be provided for the data for the sorting process. Space complexity for heapsort is O(n), which means that it will use space memory according to the size of the data set.\nThe selection sort technique is used for sorting a small dataset. It is not a good choice for larger datasets because of its quadratic time complexity.\nHeap Sort uses another array of the same size as the input array. It is needed for sorting with the help of the heap data structure.\nHeap sort is called unstable because it uses a sorting algorithm that may change the positions of equal elements. By using heap sort, equal elements lose their original order after sorting.\nHeap Sort is an internal sorting algorithm because it works with data stored in memory. However, it is not called an internal sort. It is unstable and does not maintain the relative order of equal elements.\nHeap Sort is recently using in real-world applications like priority queues for programming languages and memory allocation. It is also used for interrupt handling and process scheduling for operating systems.\nHeap Sort is better than \n because it is faster for large lists, with a time complexity of O(n log n) instead of O(n). It also sorts in place, using memory efficiently.\nWhen heap sort already has a sorted array, it continues with its process of making the heap and then performs the sort. It will take the same amount of time O(n log n) to sort a sorted array. Heap sort will not utilize any previous order in the data.\nHeapify is used for organizing the heap by placing larger elements at the top and smaller ones at the bottom. When elements are added or removed, heapify again restores its order so that the largest or smallest element can be found quickly.\nHeap Sort uses an iterative approach with a \n. It will help in reducing the need for deep recursive calls. On the opposite, Merge Sort uses recursion for higher memory usage due to \n. When we have a system with limited stack memory Heap Sort is the better choice.\nWe can use Heap Sort for small \n but is not the best choice. The extra effort of building a heap makes its process slower. For handling small datasets simpler methods like Insertion Sorts should be used.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/insertion_sort_interview_questions.htm", "title": "Insertion Sort Interview Questions and Answers", "content": "is the \n that uses the method of one item at a time. It takes an element from an unsorted array and places it into its correct location. This set of interview questions covers the range from easy to advanced level. It covers the important and most asked interview questions on insertion sort.\nFollowing are the top interview questions of insertion sort:\nInsertion Sort is used to sort the list of elements by selecting one element at a time. It starts the sorting by choosing the first element from the unsorted array and placing it in the proper position of the sorted array. It will keep on doing this until the list of elements is fully sorted.\nWe use insertion sort for sorting the array or list by choosing a single element at one time. It is also known as the one element at a time principle that compares every element of an array until it finds the best position to insert. It will continue repeating this process until every element has been placed in sorted order.\nInsertion Sort is used to sort large data sets less efficiently because its worst-case and average time complexity is O(n2). If the array is sorted, then its (n) time complexity. It is the best case. It does not need additional memory to sort, and therefore, the space complexity is O(1).\nIf we have to work with little lists or given partially sorted data sets, then insertion sort is the best option for it. It is good when memory use and code clarity are prioritized over speed owing to its minimalism and tiny space requirement.\nWhen Insertion Sort is sorting an array, then it keeps the relative order of equal elements, so it is known as a stable sorting algorithm. It is also an adaptive algorithm, so it works better with data that has already been partially sorted.\nWe don't prefer to use insertion sort when datasets are large because it can take a long time for the sorting process. When there is mixed data or some patterns of storing the data, it doesn't perform well to make the sorting process faster.\nInsertion Sort is used in everyday situations where we have small sets of things to organize. It is very hard to keep them in the same order or sorted order. For example, You have to sort your hand of cards during a game, and you want to arrange them to play strategically. In this type of situation, we use Insertion Sort.\nThe average time complexity of \n is O(n log n), which is more efficient than Insertion Sort for larger datasets.\nInsertion Sort has constant space complexity and lower overhead, which makes it faster for smaller datasets.\nInsertion Sort is the best algorithm when the data is partially sorted, whether it is in ascending or descending order. When the data is in its original form, it uses less number of swaps and comparisons. When the data are randomly given, then this algorithm is less efficient.\nIn Insertion Sort, the word \"Insertion\" means the fundamental process of picking an element from the unsorted array or list and placing it in the ordered format.\nWe have to initialize a comparison function that decides the object's order with the use of Insertion Sort to sort an array of objects. After that, we have to use Insertion Sort to compare objects using the comparison function, the same as a regular array of elements.\nWhen there is a duplicate key in a list, The algorithm will insert the duplicate key into the list in the correct location. From this, We consider that there will be duplicate keys in the list, but the list will still be sorted correctly.\nInsertion Sort needs (N - 1) passes to sort the array, where N is the number of elements in the input array.\nYes, Insertion Sort is an in-place sorting algorithm because it does not need any extra space to sort the input array.\nYes, Insertion Sort is a stable sorting algorithm because it retains the relative order of equal elements after sorting the array.\nIf the input array is already sorted, then Insertion Sort takes O(N) time to complete all the iterations.\nThe outer loop of Insertion Sort checks each element in the unsorted part of the array, which tells us that the entire array is ready for sorting.\nThe inner loop of Insertion Sort makes comparisons and shifts between the elements in the sorted part of the array until the correct position for the current element is found.\nInsertion Sort is good for partially sorted data because it needs fewer moves and comparisons. It gets close to its best-case time of \n when most elements are already in place.\nWe use Insertion Sort for handling the online sorting very well because when data arrives one by one, it can quickly insert new elements in the right place. By this, we know it is useful for sorting real-time data streams.\nThe worst-case time complexity of both algorithms is \n. But Insertion Sort is faster because it makes fewer swaps. Bubble Sort needs to compare every time and swaps a lot, while Insertion Sort places elements in their correct order.\nInsertion sort is working well with linked lists because inserting elements is easy We have to update pointers, and no need for shifting. However, it does not use the binary search, so it takes time to analyze where to insert an element.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/merge_sort_interview_questions.htm", "title": "Merge Sort Interview Questions and Answers", "content": "is the \n used to divide the array into sub-arrays and then sort those sub-arrays after that, it merges the all-sorted arrays into a single sorted array. This article covers all the important and interview-related question answers that will help you to clear the interview rounds.\nHere are the top interview question answers on the Merge Sort:\nMerge Sort is used when we don't know the size of the data, and also we can apply it to any size of data. But if we have less data, then Quick Sort performs better, and at that time, the data is uniformly distributed.\nWhen we use merge sort with items that have the same value then It can contains their original sequence. Suppose we are sorting cards by number and we have two 7s, the first seven will still come before the second seven in a final sorted list.\nNo, Merge Sort is using extra space. It utilizes extra memory as other arrays for storing data temporarily while sorting and merging stuff.\nIt is the same as the Quick Sort. Merge Sort always maintains the same speed when the data is sorted or not, and It always performs equally.\nMerge Sort works by iterating over your data several times. In each iteration, it merges more and more larger sorted chunks, with the chunks doubling in size each time. To sort n items, it needs logn iterations, and each iteration will need to do n operations, which gives us the overall time complexity of O(n log n).\nIt does this by dividing the problem into smaller and smaller pieces. It first divides the data into smaller and smaller groups until it has individual pieces. Then, it combines these groups back together in order.\nNo, Merge Sort takes the same time whether or not the data is partially sorted. It goes through the same steps each time.\nThough Merge Sort is reliable and stable (with time complexity O(n log n)), it's not the best every time. Its principal disadvantage is that it requires additional memory space. \n would generally be the preferred option, and in certain special situations, \n and \n maybe even more suitable.\nYes, Merge Sort is fine with \n. Because linked list elements are not stored side by side in memory, you simply have to watch out for managing the links between elements.\nMerge Sort is slowest when it has to compare alternate elements, such as when comparing [1,3] and [2,4]. This takes the most comparisons when merging.\nYes, you can do that by making it reuse one temporary array rather than creating a new one every time you merge. It still requires additional space, but this is more efficient.\nIt manages duplicates nicely. During merging, if it encounters equal values, it places the ones on the left first, which maintains them in their original order.\nIt suits linked lists as it only needs to alter the pointers between elements. Quick Sort would have to traverse the list extensively, which is slow using linked lists.\nThere's a two-way Merge Sort, which divides data into pairs, and a multi-way Merge Sort, which can divide into more parts simultaneously. Multi-way is especially helpful when sorting extremely large files.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/selection_sort_interview_questions.htm", "title": "Selection Sort Interview Questions and Answers", "content": "is a \n that compares every array element to find the least element and shifts it into the first index. It repeats this process until the whole array is sorted. This set of questions will provide you with deep learning of Selection Sort. It covers the most important and asked interview questions and answers from basic to advanced level.\nFollowing are the top interview questions on Selection Sort:\nSelection Sort is used to select the lowest element from an unsorted array and swap it with the starting element of an array. By placing one element into its correct location, it completes the one pass. It will take a maximum of n-1 passes to sort the array.\nSelection Sort is used to divide the given array into two sections, which are a sorted subarray and an unsorted subarray. After that, it iterates the unsorted subarray to find the minimum element and swaps it with the first element in that unsorted section. It will continue this process until the entire array is sorted.\nSelection Sort is known as an unstable algorithm since it does not preserve the original order of equal elements.\nNo, the Selection Sort is not adaptive because it fails to preserve the existing arrangement of elements in the array.\nSelection Sort leaves the duplicate elements in the same position where it found them.\nSelection Sort does not maintain the relative order of duplicate values. There are the possibilities of swapping the elements while sorting.\nSelection Sort is not suitable for large sets of data because it has O(N) time complexity, particularly when contrasted with more optimized algorithms such as \n or \n.\nOne optimization that is possible is to minimize the number of swaps by doing them only when needed and not on each iteration.\nYes, Selection Sort can be done recursively, but it's not usually done due to a function call overhead and the possibility of stack overflow with large lists.\nNo, Selection Sort is of constant behavior irrespective of the initial order of the elements and thus becomes less efficient for almost sorted arrays.\nThe best-case time complexity of the Selection Sort is (N).\nYes, Selection Sort can be used to sort linked lists by changing the pointers instead of swapping actual elements.\nSelection Sort is stable when we change the style of swapping the elements. Instead of swapping elements, we can shift elements and insert the minimum element in its correct position to keep the order of equal elements. For choosing the stable algorithm, we generally choose other sorting methods like Merge Sort and Insertion Sort.\nIt's called Selection Sort because it selects the smallest element from the unsorted part and moves it to the correct position.\nNo, It works fine for small lists but is too slow for large datasets.\nWe can make a small improvement in it by selecting both the smallest and largest elements at once. By doing this, the time complexity remains O(N), so it's still slow.\nYes, because it always picks the smallest or largest element in each step.\nWe can make maximum O(N) swaps in the worst case.\nIt's simple and takes fewer swaps, which is useful when swapping is costly.\nIt's too slow because its time complexity is O(N) and doesn't work well on nearly sorted data.\nYes, we can use it with the strings by comparing them alphabetically, just like with numbers.\nQuick Sort is the faster on average datasets O(N log N), while Selection Sort is simpler but much slower for large datasets.\nThe outer loop iterates through all the elements of an array. After that, it selects the smallest element and puts it in the correct place.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/data_structures_algorithms_questions_answers.htm", "title": "DSA Questions & Answers", "content": "has been designed with a special intention of helping students and professionals preparing for various \n and \n. This section provides a useful collection of sample Interview Questions and  Multiple Choice Questions (MCQs) and their answers with appropriate explanations.\nThis section provides a huge collection of Data Structures Algorithms Interview Questions with their answers hidden in a box to challenge you to have a go at them before discovering the correct answer.\nThis section provides a great collection of Data Structures Algorithms Multiple Choice Questions (MCQs) on a single page along with their correct answers and explanation. If you select the right option, it turns green; else red.\nIf you are preparing to appear for a Java and Data Structures Algorithms related certification exam, then this section is a must for you. This section simulates a real online test along with a given timer which challenges you to complete the test within a given time-frame. Finally you can check your overall test score and how you fared among millions of other candidates who attended this online test.\nThis section provides various mock tests that you can download at your local machine and solve offline. Every mock test is supplied with a mock test key to let you verify the final score and grade yourself.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/maximum_bipartite_matching.htm", "title": "Maximum Bipartite Matching", "content": "A \n is a type of \n that has the highest possible number of edges between two sets of vertices.\nA \n is a graph where the set of vertices can be divided into two groups, such that no two vertices within the same group are directly connected. The \n is one where every vertex in the first group is connected to every vertex in the second group.\nIf one group has \n vertices and the other has \n vertices, the total number of edges in the \n is:\n have the following properties:\nConsider the following \n with four vertices:\nIn this example, the graph has two groups of vertices: \n and \n. The \n would have the following edges:\nThere are four edges in total, which is the maximum possible for this graph.\nThere are several algorithms to find the \n in a graph:\nNow, let's look at the steps to find the \n in a graph using the \n:\nLet's look at an example of code to find the \n in a graph using the \n:\n algorithms have several real-world applications:\nThese applications demonstrate the importance of \n algorithms in solving complex optimization problems.\nThe time complexity of finding the \n in a graph using the \n is \n, where \n is the number of vertices and \n is the number of edges in the graph. This algorithm is efficient for sparse graphs with a large number of vertices.\nIn this tutorial, we learned about \n and how to find the \n in a graph using various algorithms. We also explored the properties and applications of \n in real-world scenarios. By understanding these concepts, you can solve a wide range of optimization problems efficiently.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/bellmon_ford_shortest_path.htm", "title": "Bellman-Ford Algorithm", "content": "is a popular algorithm for finding the shortest path from a starting point (or \"source\") to all other points in a graph, even if some edges have negative weights. While its not as fast as \n, it has a big advantage: it can handle graphs with negative edge weights.\nThe way the \n algorithm works is pretty simple. It goes through all the edges of the graph multiple times and tries to improve the shortest path estimates. It \"relaxes\" the edges by updating the distances until they converge to the correct values.\nIt continues to do this for all edges in the graph, ensuring the \n.\nSo, even though it might take a bit longer than \n, its a good tool when \n are involved!\nHeres a step-by-step guide to how the \n algorithm works:\nConsider a simple graph with vertices and edges.\nHere's an example code snippet for the Bellman-Ford Shortest Path algorithm in C, C++, Java, and Python:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nThe Bellman-Ford algorithm is used in various applications, including:\nIn this tutorial, we learned about the \n algorithm. It's code, time complexity, and applications.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/infix_to_postfix.htm", "title": "Convert Infix expression to Postfix expression", "content": "Infix to postfix conversion rearranges math expressions to make them easier for computers to evaluate. In infix notation, operators are between operands (e.g., \"A + B\"). In postfix notation, operators come after operands (e.g., \"A B +\"). We use a stack to convert infix to postfix, ensuring the order of operations is preserved.\nBefore jumping into conversion, lets understand the three common types of expressions:\nPostfix notation is useful because it eliminates the need for parentheses and follows a simple evaluation process using stacks.\nStacks helps us to manage the order of operations (precedence) and parentheses correctly. When scanning an infix expression, we can use a stack to store operators and pop them based on precedence. This way, we can convert infix to postfix without losing the order of operations.\nThe algorithm for converting an infix expression to a postfix expression using a stack is:\nOperators have different levels of precedence, which determine the order of evaluation:\nWhen two operators have the same precedence, we evaluate them based on associativity. Most operators are left-to-right associative, except exponentiation, which is right-to-left.\nLet's convert the infix expression: \nFinal Postfix Expression: \nNow that we have converted infix to postfix, lets see how to evaluate it:\nNow that we already know the algorithm, lets see how to implement it in C, C++, Java and Python:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing are some common applications of postfix notation:\nInfix notation is how we naturally write mathematical expressions, but computers prefer postfix because its easier to process. We can use stack for easy \n conversion. Understanding this concept is crucial for solving problems related to expression evaluation and compiler design.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/dsa_fisher_yates_shuffle_algorithm.htm", "title": "Fisher-Yates Shuffle Algorithm", "content": "The Fisher-Yates Shuffle algorithm shuffles a finite sequence of elements by generating a random permutation. The possibility of every permutation occurring is equally likely. The algorithm is performed by storing the elements of the sequence in a sack and drawing each element randomly from the sack to form the shuffled sequence.\nCoined after Ronald Fisher and Frank Yates, for designing the original method of the shuffle, the algorithm is unbiased. It generates all permutations in same conditions so the output achieved is nowhere influenced. However, the modern version of the Fisher-Yates Algorithm is more efficient than that of the original one.\nThe original method of Shuffle algorithm involved a pen and paper to generate a random permutation of a finite sequence. The algorithm to generate the random permutation is as follows −\n − Write down all the elements in the finite sequence. Declare a separate list to store the output achieved.\n − Choose an element i randomly in the input sequence and add it onto the output list. Mark the element i as visited.\n − Repeat Step 2 until all the element in the finite sequence is visited and added onto the output list randomly.\n − The output list generated after the process terminates is the random permutation generated.\nThe modern algorithm is a slightly modified version of the original fisher-yates shuffle algorithm. The main goal of the modification is to computerize the original algorithm by reducing the time complexity of the original method. The modern method is developed by Richard Durstenfeld and was popularized by Donald E. Knuth.\nTherefore, the modern method makes use of swapping instead of maintaining another output list to store the random permutation generated. The time complexity is reduced to \n rather than \n. The algorithm goes as follows −\n − Write down the elements 1 to n in the finite sequence.\n − Choose an element i randomly in the input sequence and swap it with the last unvisited element in the list.\n − Repeat Step 2 until all the element in the finite sequence is visited and swapped.\n − The list generated after the process terminates is the random permutation sequence.\nShuffling is done from highest index to the lowest index of the array in the following modern method pseudocode.\nShuffling is done from lowest index to the highest index of the array in the following modern method pseudocode.\nTo describe the algorithm better, let us permute the the given finite sequence of the first six letters of the alphabet. Input sequence: A B C D E F.\nThis is called the pen and paper method. We consider an input array with the finite sequence stored and an output array to store the result.\nChoose any element randomly and add it onto the output list after marking it checked. In this case, we choose element C.\nThe next element chosen randomly is E which is marked and added to the output list.\nThe random function then picks the next element A and adds it onto the output array after marking it visited.\nThen F is selected from the remaining elements in the input sequence and added to the output after marking it visited.\nThe next element chosen to add onto the random permutation is D. It is marked and added to the output array.\nThe last element present in the input list would be B, so it is marked and added onto the output list finally.\nIn order to reduce time complexity of the original method, the modern algorithm is introduced. The modern method uses swapping to shuffle the sequences  for example, the algorithm works like shuffling a pack of cards by swapping their places in the original deck. Let us look at an example to understand how modern version of the Fisher-Yates algorithm works.\nConsider first few letters of the alphabet as an input and shuffle them using the modern method.\nRandomly choosing the element D and swapping it with the last unmarked element in the sequence, in this case F.\nFor the next step we choose element B to swap with the last unmarked element E since F had been moved to Ds place after swapping in the previous step.\nWe next swap the element A with F, since it is the last unmarked element in the list.\nThen the element F is swapped with the last unmarked element C.\nThe remaining elements in the sequence could be swapped finally, but since the random function chose E as the element it is left as it is.\nStep 7\nThe remaining element C is left as it is without swapping.\nThe array obtained after swapping is the final output array.\nFollowing are the implementations of the above approach in various programming langauges −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/dsa_kargers_minimum_cut_algorithm.htm", "title": "Kargers Minimum Cut Algorithm", "content": "Considering the real-world applications like image segmentation where objects that are focused by the camera need to be removed from the image. Here, each pixel is considered as a node and the capacity between these pixels is reduced. The algorithm that is followed is the minimum cut algorithm.\n is the removal of minimum number of edges in a graph (directed or undirected) such that the graph is divided into multiple separate graphs or disjoint set of vertices.\nLet us look at an example for a clearer understanding of disjoint sets achieved\nEdges {A, E} and {F, G} are the only ones loosely bonded to be removed easily from the graph. Hence, the minimum cut for the graph would be 2.\nThe resultant graphs after removing the edges A → E and F → G are {A, B, C, D, G} and {E, F}.\n algorithm is a randomized algorithm to find the minimum cut of a graph. It uses the monte carlo approach so it is expected to run within a time constraint and have a minimal error in achieving output. However, if the algorithm is executed multiple times the probability of the error is reduced. The graph used in kargers minimum cut algorithm is undirected graph with no weights.\nThe kargers algorithm merges any two nodes in the graph into one node which is known as a supernode. The edge between the two nodes is contracted and the other edges connecting other adjacent vertices can be attached to the supernode.\n − Choose any random edge [u, v] from the graph G to be contracted.\n\n − Merge the vertices to form a supernode and connect the edges of the other adjacent nodes of the vertices to the supernode formed. Remove the self nodes, if any.\n\n − Repeat the process until theres only two nodes left in the contracted graph.\n\n − The edges connecting these two nodes are the minimum cut edges.\n\nThe algorithm does not always the give the optimal output so the process is repeated multiple times to decrease the probability of error.\nApplying the algorithm on an undirected unweighted graph G {V, E} where V and E are sets of vertices and edges present in the graph, let us find the minimum cut −\nChoose any edge, say A → B, and contract the edge by merging the two vertices into one supernode. Connect the adjacent vertex edges to the supernode. Remove the self loops, if any.\nContract another edge (A, B) → C, so the supernode will become (A, B, C) and the adjacent edges are connected to the newly formed bigger supernode.\nThe node D only has one edge connected to the supernode and one adjacent edge so it will be easier to contract and connect the adjacent edge to the new supernode formed.\nAmong F and E vertices, F is more strongly bonded to the supernode, so the edges connecting F and (A, B, C, D) are contracted.\nSince there are only two nodes present in the graph, the number of edges are the final minimum cut of the graph. In this case, the minimum cut of given graph is \n.\nThe minimum cut of the original graph is 2 (E → D and E → F).\nFollowing are the implementations of the above approach in various programming langauges −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/dsa_randomized_quick_sort_algorithm.htm", "title": "Randomized Quick Sort Algorithm", "content": "Quicksort is a popular sorting algorithm that chooses a pivot element and sorts the input list around that pivot element. To learn more about quick sort, please \nRandomized quick sort is designed to decrease the chances of the algorithm being executed in the worst case time complexity of \n. The worst case time complexity of quick sort arises when the input given is an already sorted list, leading to n(n  1) comparisons. There are two ways to randomize the quicksort −\nRandomly shuffling the inputs: Randomization is done on the input list so that the sorted input is jumbled again which reduces the time complexity. However, this is not usually performed in the randomized quick sort.\nRandomly choosing the pivot element: Making the pivot element a random variable is commonly used method in the randomized quick sort. Here, even if the input is sorted, the pivot is chosen randomly so the worst case time complexity is avoided.\nThe algorithm exactly follows the standard algorithm except it randomizes the pivot selection.\nLet us look at an example to understand how randomized quicksort works in avoiding the worst case time complexity. Since, we are designing randomized algorithms to decrease the occurrence of worst cases in time complexity lets take a sorted list as an input for this example.\nThe sorted input list is 3, 5, 7, 8, 12, 15. We need to apply the quick sort algorithm to sort the list.\nConsidering the worst case possible, if the random pivot chosen is also the highest index number, it compares all the other numbers and another pivot is selected.\nSince 15 is greater than all the other numbers in the list, it wont be swapped, and another pivot is chosen.\nThis time, if the random pivot function chooses 7 as the pivot number −\nNow the pivot divides the list into half so standard quick sort is carried out usually. However, the time complexity is decreased than the worst case.\nIt is to be noted that the worst case time complexity of the quick sort will always remain \n but with randomizations we are decreasing the occurrences of that worst case.\nFollowing are the implementations of the above approach in various programming langauges −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/dsa_randomized_algorithms.htm", "title": "Randomized Algorithms", "content": "Randomized algorithm is a different design approach taken by the standard algorithms where few random bits are added to a part of their logic. They are different from deterministic algorithms; deterministic algorithms follow a definite procedure to get the same output every time an input is passed where randomized algorithms produce a different output every time theyre executed. It is important to note that it is not the input that is randomized, but the logic of the standard algorithm.\nUnlike deterministic algorithms, randomized algorithms consider randomized bits of the logic along with the input that in turn contribute towards obtaining the output.\nHowever, the probability of randomized algorithms providing incorrect output cannot be ruled out either. Hence, the process called \n is performed to reduce the likelihood of these erroneous outputs. Amplification is also an algorithm that is applied to execute some parts of the randomized algorithms multiple times to increase the probability of correctness. However, too much amplification can also exceed the time constraints making the algorithm ineffective.\nRandomized algorithms are classified based on whether they have time constraints as the random variable or deterministic values. They are designed in their two common forms − Las Vegas and Monte Carlo.\n− The Las Vegas method of randomized algorithms never gives incorrect outputs, making the time constraint as the random variable. For example, in string matching algorithms, las vegas algorithms start from the beginning once they encounter an error. This increases the probability of correctness. Eg., Randomized Quick Sort Algorithm.\n− The Monte Carlo method of randomized algorithms focuses on finishing the execution within the given time constraint. Therefore, the running time of this method is deterministic. For example, in string matching, if monte carlo encounters an error, it restarts the algorithm from the same point. Thus, saving time. Eg., Kargers Minimum Cut Algorithm\nThis approach is usually adopted to reduce the time complexity and space complexity. But there might be some ambiguity about how adding randomness will decrease the runtime and memory stored, instead of increasing; we will understand that using the game theory.\nThe basic idea of game theory actually provides with few models that help understand how decision-makers in a game interact with each other. These game theoretical models use assumptions to figure out the decision-making structure of the players in a game. The popular assumptions made by these theoretical models are that the players are both rational and take into account what the opponent player would decide to do in a particular situation of a game. We will apply this theory on randomized algorithms.\nThe zero-sum game is a mathematical representation of the game theory. It has two players where the result is a gain for one player while it is an equivalent loss to the other player. So, the net improvement is the sum of both players status which sums up to zero.\nRandomized algorithms are based on the zero-sum game of designing an algorithm that gives lowest time complexity for all inputs. There are two players in the game; one designs the algorithm and the opponent provides with inputs for the algorithm. The player two needs to give the input in such a way that it will yield the worst time complexity for them to win the game. Whereas, the player one needs to design an algorithm that takes minimum time to execute any input given.\nFor example, consider the quick sort algorithm where the main algorithm starts from selecting the pivot element. But, if the player in zero-sum game chooses the sorted list as an input, the standard algorithm provides the worst case time complexity. Therefore, randomizing the pivot selection would execute the algorithm faster than the worst time complexity. However, even if the algorithm chose the first element as pivot randomly and obtains the worst time complexity, executing it another time with the same input will solve the problem since it chooses another pivot this time.\nOn the other hand, for algorithms like merge sort the time complexity does not depend on the input; even if the algorithm is randomized the time complexity will always remain the same. Hence, \n.\nFew popular examples of the Randomized algorithms are −\nThe Subset Sum Problem\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/dsa_travelling_salesman_approximation_algorithm.htm", "title": "Travelling Salesman using Approximation Algorithm", "content": "We have already discussed the travelling salesperson problem using the \n and \n approaches, and it is established that solving the travelling salesperson problems for the perfect optimal solutions is not possible in polynomial time.\nTherefore, the approximation solution is expected to find a near optimal solution for this NP-Hard problem. However, an approximate algorithm is devised only if the cost function (which is defined as the distance between two plotted points) in the problem satisfies \n.\nThe triangle inequality is satisfied only if the cost function c, for all the vertices of a triangle u, v and w, satisfies the following equation\nIt is usually automatically satisfied in many applications.\nThe travelling salesperson approximation algorithm requires some prerequisite algorithms to be performed so we can achieve a near optimal solution. Let us look at those prerequisite algorithms briefly −\n − The minimum spanning tree is a tree data structure that contains all the vertices of main graph with minimum number of edges connecting them. We apply prims algorithm for minimum spanning tree in this case.\n − The pre-order traversal is done on tree data structures where a pointer is walked through all the nodes of the tree in a [root  left child  right child] order.\n − Choose any vertex of the given graph randomly as the starting and ending point.\n − Construct a minimum spanning tree of the graph with the vertex chosen as the root using prims algorithm.\n − Once the spanning tree is constructed, pre-order traversal is performed on the minimum spanning tree obtained in the previous step.\n − The pre-order solution obtained is the Hamiltonian path of the travelling salesperson.\nThe approximation algorithm of the travelling salesperson problem is a 2-approximation algorithm if the triangle inequality is satisfied.\nTo prove this, we need to show that the approximate cost of the problem is double the optimal cost. Few observations that support this claim would be as follows −\nThe cost of minimum spanning tree is never less than the cost of the optimal Hamiltonian path. That is, \n).\nThe cost of full walk is also twice as the cost of minimum spanning tree. A full walk is defined as the path traced while traversing the minimum spanning tree preorderly. Full walk traverses every edge present in the graph exactly twice. Thereore, \nSince the preorder walk path is less than the full walk path, the output of the algorithm is always lower than the cost of the full walk.\nLet us look at an example graph to visualize this approximation algorithm −\nConsider vertex 1 from the above graph as the starting and ending point of the travelling salesperson and begin the algorithm from here.\nStarting the algorithm from vertex 1, construct a minimum spanning tree from the graph. To learn more about constructing a minimum spanning tree, please \nOnce, the minimum spanning tree is constructed, consider the starting vertex as the root node (i.e., vertex 1) and walk through the spanning tree preorderly.\nRotating the spanning tree for easier interpretation, we get −\nThe preorder traversal of the tree is found to be − \nAdding the root node at the end of the traced path, we get, \nThis is the output Hamiltonian path of the travelling salesman approximation problem. The cost of the path would be the sum of all the costs in the minimum spanning tree, i.e., \n.\nFollowing are the implementations of the above approach in various programming langauges −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/dsa_set_cover_problem.htm", "title": "Set Cover Problem", "content": "The set cover algorithm provides solution to many real-world resource allocating problems. For instance, consider an airline assigning crew members to each of their airplanes such that they have enough people to fulfill the requirements for the journey. They take into account the flight timings, the duration, the pit-stops, availability of the crew to assign them to the flights. This is where set cover algorithm comes into picture.\nGiven a universal set U, containing few elements which are all divided into subsets. Considering the collection of these subsets as S = {S\n, S\n, S\n, S\n... S\n}, the set cover algorithm finds the minimum number of subsets such that they cover all the elements present in the universal set.\nAs shown in the above diagram, the dots represent the elements present in the universal set U that are divided into different sets, S = {S\n, S\n, S\n, S\n, S\n, S\n}. The minimum number of sets that need to be selected to cover all the elements will be the optimal output = {S\n, S\n, S\n}.\nThe set cover takes the collection of sets as an input and and returns the minimum number of sets required to include all the universal elements.\nThe set cover algorithm is an NP-Hard problem and a 2-approximation greedy algorithm.\n− Initialize Output = {} where Output represents the output set of elements.\n− While the Output set does not include all the elements in the universal set, do the following −\nFind the cost-effectiveness of every subset present in the universal set using the formula, $\\frac{Cost\\left ( S_{i} \\right )}{S_{i}-Output}$\nFind the subset with minimum cost effectiveness for each iteration performed. Add the subset to the Output set.\n− Repeat Step 2 until there is no elements left in the universe. The output achieved is the final Output set.\nassuming the overall number of elements equals the overall number of sets (|X| = |S|), the code runs in time O(|X|3)\nLet us look at an example that describes the approximation algorithm for the set covering problem in more detail\nThe output set, Output = \nFind the cost effectiveness of each set for no elements in the output set,\nThe minimum cost effectiveness in this iteration is achieved at S\n, therefore, the subset added to the output set, Output = {S\n} with elements {1, 2, 3, 4}\nFind the cost effectiveness of each set for the new elements in the output set,\nThe minimum cost effectiveness in this iteration is achieved at S\n, therefore, the subset added to the output set, Output = {S\n, S\n} with elements {1, 2, 3, 4, 5, 7, 9, 11, 13}.\nFind the cost effectiveness of each set for the new elements in the output set,\nThe minimum cost effectiveness in this iteration is achieved at S\n, therefore, the subset added to the output set, Output = {S\n, S\n, S\n} with elements {1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 13}\nFind the cost effectiveness of each set for the new elements in the output set,\nThe minimum cost effectiveness in this iteration is achieved at S\n, therefore, the subset added to the output set, Output = {S\n, S\n, S\n, S\n} with elements {1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 16, 20}\nFind the cost effectiveness of each set for the new elements in the output set,\nThe minimum cost effectiveness in this iteration is achieved at S\n, therefore, the subset added to the output set, Output = {S\n, S\n, S\n, S\n, S\n} with elements {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 20}\nThe final output that covers all the elements present in the universal finite set is, Output = {S\n, S\n, S\n, S\n, S\n}.\nFollowing are the implementations of the above approach in various programming langauges −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/vertex_cover_algorithm.htm", "title": "Vertex Cover Algorithm", "content": "Have you ever wondered about the placement of traffic cameras? That how they are efficiently placed without wasting too much budget from the government? The answer to that comes in the form of \n. The positions of the cameras are chosen in such a way that one camera covers as many roads as possible, i.e., we choose junctions and make sure the camera covers as much area as possible.\nA \n of an undirected graph \n is the subset of vertices of the graph such that, for all the edges \n in the graph,\n. The junction is treated as the node of a graph and the roads as the edges. The algorithm finds the minimum set of junctions that cover maximum number of roads.\nIt is a minimization problem since we find the minimum size of the vertex cover  the size of the vertex cover is the number of vertices in it. The optimization problem is an NP-Complete problem and hence, cannot be solved in polynomial time; but what can be found in polynomial time is the near optimal solution.\nThe vertex cover approximation algorithm takes an undirected graph as an input and is executed to obtain a set of vertices that is definitely twice as the size of optimal vertex cover.\nThe vertex cover is a 2-approximation algorithm.\n − Select any random edge from the input graph and mark all the edges that are incident on the vertices corresponding to the selected edge.\n − Add the vertices of the arbitrary edge to an output set.\n − Repeat Step 1 on the remaining unmarked edges of the graph and add the vertices chosen to the output until theres no edge left unmarked.\n − The final output set achieved would be the near-optimal vertex cover.\nThe set of edges of the given graph is −\nNow, we start by selecting an arbitrary edge (1,6). We eliminate all the edges, which are either incident to vertex 1 or 6 and we add edge (1,6) to cover.\nIn the next step, we have chosen another edge (2,3) at random.\nNow we select another edge (4,7).\nWe select another edge (8,5).\nHence, the vertex cover of this graph is {1,6,2,3,4,7,5,8}.\nIt is easy to see that the running time of this algorithm is \n, using adjacency list to represent \n.\nFollowing are the implementations of the above approach in various programming langauges −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/dsa_approximation_algorithms.htm", "title": "Approximation Algorithms", "content": "Approximation algorithms are algorithms designed to solve problems that are not solvable in polynomial time for approximate solutions. These problems are known as NP complete problems. These problems are significantly effective to solve real world problems, therefore, it becomes important to solve them using a different approach.\nNP complete problems can still be solved in three cases: the input could be so small that the execution time is reduced, some problems can still be classified into problems that can be solved in polynomial time, or use approximation algorithms to find near-optima solutions for the problems.\nThis leads to the concept of performance ratios of an approximation problem.\nThe main idea behind calculating the \n of an approximation algorithm, which is also called as an \n, is to find how close the approximate solution is to the optimal solution.\nThe approximate ratio is represented using \n where n is the input size of the algorithm, C is the near-optimal solution obtained by the algorithm, C* is the optimal solution for the problem. The algorithm has an approximate ratio of (n) if and only if −\n$$max\\left\\{\\frac{C}{C^{\\ast} },\\frac{C^{\\ast }}{C} \\right\\}\\leq \\rho \\left ( n \\right )$$\nThe algorithm is then called a (n)-approximation algorithm. Approximation Algorithms can be applied on two types of optimization problems: minimization problems and maximization problems. If the optimal solution of the problem is to find the maximum cost, the problem is known as the maximization problem; and if the optimal solution of the problem is to find the minimum cost, then the problem is known as a minimization problem.\nFor maximization problems, the approximation ratio is calculated by C*/C since 0  C  C*. For minimization problems, the approximation ratio is calculated by C/C* since 0  C*  C.\nAssuming that the costs of approximation algorithms are all positive, the performance ratio is well defined and will not be less than 1. If the value is 1, that means the approximate algorithm generates the exact optimal solution.\nFew popular examples of the approximation algorithms are −\nThe Subset Sum Problem\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/bloom_filter_data_structure.htm", "title": "Bloom Filters", "content": "A Bloom filter is defined as a data structure designed to identify of a elements presence in a set in a rapid and memory efficient manner.\nYou can think of it as a \n data structure. This data structure helps us to identify that an element is either present or absent in a set. It is not used to store the actual data, but to check whether the data is present or not.\nIt is a space-efficient probabilistic data structure that is used to test whether an element is a member of a set. False positive matches are possible, but false negatives are not. In other words, a query returns either \"possibly in set\" or \"definitely not in set\".\nLet's understand how a Bloom filter works with the help of an example:\nSuppose we have a set of elements {A, B, C, D, E, F, G, H, I, J} and we want to check whether the element 'X' is present in the set or not.\nHere's how a Bloom filter works:\nHere's an example implementation of a Bloom filter C, C++, Java and Python:\nFollowing is the output of the above C program:\nFollowing is the output of the above C++ program:\nFollowing is the output of the above Java program:\nFollowing is the output of the above Python program:\nSome of the key features of Bloom filters include:\nBloom filters are used in various applications, including:\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/bit_mask_in_data_structures.htm", "title": "Bit Mask in Data Structures", "content": "A bit mask is a powerful technique that is widely used in computer science and programming to manipulate bits. You can think of it as a way to turn on or off specific bits in a binary number.\nBit masks are used for performing bitwise operations, such as \n, \n, \n, and \n, on binary numbers. They are commonly used in computer science for various applications, we will discuss about the bit mask in detail in this chapter.\nBitmask, also known as mask is a sequence of \n that encode the subset of our collection. The element of the mask can be either set or not set (i.e. \n or \n). This denotes the availability of the chosen element in the bitmask.\nFor example, an element i is available in the subset if the ith bit of mask is set. For the N element set, there can be a \n mask each corresponding to a subset.\nSo, for solving the problem, we will start for a mask i.e. a subset assigns it a value and then finds the values for further masks using the values of the previous mask. Using this we will finally be able to calculate the value of the main set.\nNow, let's discuss the bitwise operations that can be performed on a bit mask:\nThere are mainly three operations that can be performed on a bit mask:\nSetting a bit to 1 in a binary number is done by using the OR operation. The OR operation is used to set a bit to 1 if either of the bits is 1, otherwise it will be set to 0.\nFollowing is the algorithm to set the i\n bit to 1 in the binary number:\nHere is the code for performing the Set operation on a bit mask in different programming languages:\n a bit to 0 in a binary number is done by using the \n. The \n is used to set a bit to 1 if both bits are 1, otherwise it will be set to 0.\nSometimes, we need to clear a bit to 0 in a binary number to perform further operations on it. For example, if we want to set a bit to 1, we first clear the bit to 0 and then set it to 1.\nFollowing is the algorithm to clear the ith bit to 0 in the binary number:\nHere is the code for performing the Clear operation on a bit mask in different programming languages:\n a bit from 0 to 1 or 1 to 0 in a binary number is done by using the \n. The \n is used to set a bit to 1 if the bits are different, otherwise it will be set to 0.\nFollowing is the algorithm to toggle the ith bit in the binary number:\nHere is the code for performing the Toggle operation on a bit mask in different programming languages:\nBit masks are used in computer science for various applications, such as:\n are useful for manipulating bits in a binary number. They are commonly used in computer science and programming for performing \n on binary numbers.\nBit masks can be used to \n, \n, and \n bits in a binary number, as well as perform bitwise operations like \n, \n, \n, and \n. They are a powerful technique for optimizing algorithms and data structures for performance.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/treaps.htm", "title": "Treap (A Randomized Binary Search Tree)", "content": "Treaps (pronounced as \"trees\" and \"heaps\") are a data structure that is combination of \n and \n. It is a binary search tree that is also a heap. We can call it a randomized data structure that maintains a dynamic set of elements, it ensures both the binary search order and the heap order.\nImagine a scenario where you have to organize a bookshelf. You want these book to be in a sorted order, but you also want to keep the books that you read most often at the top. Treaps are the data structure that can help you in this scenario. \nFollowing are the properties of Treaps −\nEach node in a Treap has two fields:\nThe key field is used to maintain the binary search tree property, and the priority field is used to maintain the heap property. The priority of a node is always greater than the priority of its children.\nHere is an example of a Treap:\nThere are many operations that can be performed on Treaps. Some of the major operations are:\nNow we will see how to implement Treaps in C, C++, Java and Python.\nNow, let's understand the code for implementing Treaps in C, C++, Java and Python.\nFor implementing treaps, we need to create a structure for the node and define the functions for inserting and rotating the nodes. After that we can insert the nodes and perform the operations on the treap.\nFollowing is the output of the above C program:\nFollowing is the output of the above C++ program:\nFollowing is the output of the above Java program:\nFollowing is the output of the above Python program:\nSearching in a Treap is similar to searching in a Binary Search Tree. We start from the root and compare the key with the root. If the key is less than the root, we move to the left subtree. If the key is greater than the root, we move to the right subtree. We continue this process until we find the key or reach a NULL node.\nNow, let's see the code for searching in Treaps in C, C++, Java and Python.\nFollowing is the output of the above C program:\nFollowing is the output of the above C++ program:\nFollowing is the output of the above Java program:\nFollowing is the output of the above Python program:\nDeletion in a Treap is similar to deletion in a Binary Search Tree. We first search for the key to be deleted. If the key is found, we delete the node and rearrange the tree to maintain the heap property. We can delete the node by replacing it with the node that has the smallest priority in the right subtree or the node that has the largest priority in the left subtree.\nNow, let's see the code for deleting a node in Treaps in C, C++, Java and Python.\nFollowing is the output of the above C program:\nFollowing is the output of the above C++ program:\nFollowing is the output of the above Java program:\nFollowing is the output of the above Python program:\nFollowing is the time complexity of Treaps:\nIn this chapter, we learned about Treaps, which are a combination of Binary Search Trees and Heaps. We saw the structure of Treaps, the operations that can be performed on Treaps, and the implementation of Treaps in C, C++, Java and Python. We also saw the time and space complexity of Treaps along with the applications of Treaps.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/suffix_tries.htm", "title": "Suffix Tries", "content": "A suffix trie is a type of trie mainly used for representing all the suffixes of a string. It is a tree where each node represents the suffix of a string. The root node is the entire string, and each child node is a suffix of the string that's one character shorter than the parent node's suffix.\nFor example, consider the string \"banana\". The suffix trie for this string would look like this:\nEach path from the root to a leaf node represents a suffix of the string. The leaf nodes represent the end of a suffix, and the dollar sign \n is used to indicate the end of the string.\nThe children store  entire word instead of characters.\nFollowing are the Features of Suffix Tries −\nIt is trie, so it stores suffixes of string in trie data structure. for string of length n, there are n suffixes. \nWe can say compact representation of all suffixes of a string. Means, to save space, it becomes \n by collapsing the nodes with only one child.\nSuffix tries are effective for sub-string searches, pattern matching, and other string-related operations.\nSome of the common operations on suffix tries include:\nWe can insert a new suffix into the trie by following these steps:\nFor example, to insert the suffix \"ana\" into the trie, we would follow these steps:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nDeletion and search operations on suffix tries are similar to insertion. We can delete a suffix by removing the corresponding leaf node from the trie. To search for a specific suffix, we can traverse the trie from the root node to the leaf node corresponding to the suffix.\nFollowing are the steps to delete a suffix from the trie:\nFollowing are the steps to search for a specific suffix in the trie:\nHere is the code for deletion and search operations on suffix tries:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nThe time complexity of suffix tries depends on the operations performed:\nSuffix tries are used in various applications, including:\nIn this chapter, we discussed suffix tries, a type of trie used for representing all the suffixes of a string. We explored the structure of a suffix trie, its features, operations, and applications. Suffix tries are a powerful data structure for substring searches, pattern matching, and other string-related operations.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/compressed_tries.htm", "title": "Compressed Tries", "content": "Compressed Trie is also known as \n or \n. It is a type of trie data structure that is used for storing and retrieving keys in a dataset, where the keys are strings.\nYou can think of a compressed trie as a trie where the nodes with only one child are merged with their parent nodes. This helps in reducing the space complexity of the trie and makes it more efficient.\nThe compression of redundant nodes helps in memory management. This type of trie is used with applications that need space management.\nFollowing are some of the properties of compressed tries:\nNow, let's see how a compressed trie is represented:\nIt is represented as a tree where each node has a label and a list of children. The edges of the tree are labeled with characters. The keys are stored in the leaves of the tree.\nHere is an example of a compressed trie:\nIn the above example, the keys \"bear\", \"bell\", \"bid\", \"bull\" are stored in the leaves of the trie.\nWe can perform the following operations on compressed tries:\nFor insertion of a new key into the compressed trie, we need to follow these steps:\nFollowing is the algorithm for inserting a new key into the compressed trie:\nNow, let's implement the insert operation in a compressed trie. For this, first we need to define the structure of the trie node then implement the insert operation.\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFor searching a key in the compressed trie, we need to follow these steps:\nFollowing is the algorithm for searching a key in the compressed trie:\nNow, let's implement the search operation in a compressed trie. For this, we need to define the structure of the trie node and then implement the search operation.\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFor deleting a key from the compressed trie, we need to follow these steps:\nFollowing is the algorithm for deleting a key from the compressed trie:\nNow, let's implement the delete operation in a compressed trie. For this, we need to define the structure of the trie node and then implement the delete operation.\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nCompressed tries are used in various applications, such as:\nIn this chapter, we learned about compressed tries, which are used to store a large number of strings efficiently. We discussed the structure of a compressed trie, the insert operation, search operation, and delete operation on a compressed trie. We also implemented these operations in C, C++, Java, and Python programming languages.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/standard_tries.htm", "title": "Standard Tries", "content": "are tree-like data structures that are mainly used for storing dynamic set of strings or associative arrays where the keys are usually strings. The word \"trie\" comes from the word \"retrieval\".\nWe already discussed the basic concepts of tries in \n chapter. In this chapter, we will discuss the \n in data structures.\nChildren of a node are stored in a certain order such as dictionary order (alphabetical) or ASCII order. The order des on the use cases. For example, if the trie is used for a dictionary, the children are stored in alphabetical order.\nLet's understand with an example.\nThe above example shows the \n structure for the strings {cat, car, dog, doll}. The root node is empty and the children are stored in alphabetical order.\nIn the \n, a string can be traced from the root node to the node where the strings. The node is marked as a leaf node.\nThere are mainly three operations that can be performed on a standard trie:\nFor implementing the standard trie, we need to create a structure for the trie node. The trie node contains an array of pointers to the children nodes and a flag to mark the of the string.\nNow, let's implement the standard trie, which supports the above operations.\nInserting a new string in the Standard Trie is done by inserting the characters of the string one by one. The characters are inserted in the trie in the order of the string. The last character of the string is marked as a leaf node.\nFollowing is the algorithm for the insert operation in the standard trie:\nLet's see the code for the insert operation in the standard trie using C, C++, Java, and Python programming languages.\nFollowing is the output obtained −\nThe output obtained is as follows −\nThe output obtained is as follows −\nFollowing is the output obtained −\nSearching a string in the Standard Trie is done by traversing the trie from the root node to the node where the strings. If the string is found in the trie, then the search operation returns true; otherwise, it returns false.\nFollowing is the algorithm for the search operation in the Standard Trie:\nFollowing is the code for the search operation in the Standard Trie using C, C++, Java, and Python programming languages.\nThe output obtained is as follows −\nFollowing is the output of the above code −\nThe output is as follows −\nThe output obtained is as follows −\nDeleting a string from the Standard Trie is done by deleting the characters of the string one by one. The characters are deleted in the order of the string. The last character of the string is marked as a non-leaf node.\nFollowing is the algorithm for the delete operation in the Standard Trie:\nLet's see the code for the delete operation in the Standard Trie using C, C++, Java, and Python programming languages.\nThe output obtained is as follows −\nFollowing is the output of the above code −\nFollowing is the output of the above code −\nThe output obtained is as follows −\nFollowing is the time complexity of the Standard Trie:\nStandard Trie is used in various applications such as:\nThese are some of the applications where Standard Trie is used.\nIn this chapter, we discussed the Standard Trie in data structures. The structure of the Standard Trie, the operations that can be performed on the Standard Trie, and the implementation of the Standard Trie. We also discussed the applications of the Standard Trie.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/tries.htm", "title": "Tries Data Structure", "content": "A trie is a type of a multi-way search tree, which is fundamentally used to retrieve specific keys from a string or a set of strings. It stores the data in an ordered efficient way since it uses pointers to every letter within the alphabet.\nThe trie data structure works based on the common prefixes of strings. The root node can have any number of nodes considering the amount of strings present in the set. The root of a trie does not contain any value except the pointers to its child nodes.\nThere are three types of trie data structures −\nStandard Tries\nCompressed Tries\nSuffix Tries\nThe real-world applications of trie include − autocorrect, text prediction, sentiment analysis and data sciences.\nThe trie data structures also perform the same operations that tree data structures perform. They are −\nInsertion\nDeletion\nSearch\nThe insertion operation in a trie is a simple approach. The root in a trie does not hold any value and the insertion starts from the immediate child nodes of the root, which act like a key to their child nodes. However, we observe that each node in a trie represents a singlecharacter in the input string. Hence the characters are added into the tries one by one while the links in the trie act as pointers to the next level nodes.\nFollowing are the implementations of this operation in various programming languages −\nThe deletion operation in a trie is performed using the bottom-up approach. The element is searched for in a trie and deleted, if found. However, there are some special scenarios that need to be kept in mind while performing the deletion operation.\n − The key is unique − in this case, the entire key path is deleted from the node. (Unique key suggests that there is no other path that branches out from one path).\n − The key is not unique − the leaf nodes are updated. For example, if the key to be deleted is \n but it is a prefix of another key \n; we delete the see and change the Boolean values of t, h and e as false.\n − The key to be deleted already has a prefix − the values until the prefix are deleted and the prefix remains in the tree. For example, if the key to be deleted is \n but there is another key present \n; so we delete a, r, and t until only he remains.\nFollowing are the implementations of this operation in various programming languages −\nSearching in a trie is a rather straightforward approach. We can only move down the levels of trie based on the key node (the nodes where insertion operation starts at). Searching is done until the end of the path is reached. If the element is found, search is successful; otherwise, search is prompted unsuccessful.\nFollowing are the implementations of this operation in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/fibonacci_heap.htm", "title": "Fibbonacci Heap", "content": "Like \n, \n are collection of trees. They are loosely based on binomial heaps. Unlike trees within binomial heaps, trees within Fibonacci heaps are rooted but unordered.\nEach node x in Fibonacci heaps contains a pointer \n to its parent, and a pointer \n to any one of its children. The children of x are linked together in a circular doubly linked list known as the child list of x.\nEach child y in a child list has pointers \n and \n to point to the left and right siblings of y respectively. If node y is the only child, then \n. The order in which siblings appear in a child list is arbitrary.\nEach node x in Fibonacci heap contains the following fields:\nEach Fibonacci heap H is a set of rooted trees that are min-heap ordered. That is, each tree obeys the min-heap property: the key of a node is greater than or equal to the key of its parent.\nThe root list of H is a circular doubly linked list of elements of H containing exactly one root from each tree in the root list. The \n pointer in H points to the root of the tree containing the minimum key.\nThis Fibonacci Heap H consists of five Fibonacci Heaps and 16 nodes. The line with arrow head indicates the root list. Minimum node in the list is denoted by \n which is holding 4.\nThe asymptotically fast algorithms for problems such as computing minimum spanning trees, finding single source of shortest paths etc. make essential use of Fibonacci heaps.\nFollowing are the operations that can be performed on Fibonacci heap:\n a new node x to the heap H. Its \n, just inserts without much rearranging.\nLet's assume that we have a Fibonacci heap H and a node x. The algorithm to insert node x into Fibonacci heap H is as follows:\nLet's write a simple code to insert a node into Fibonacci heap.\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nRemoves the node containing the minimum key from the heap H. Its a bit complex operation as it involves rearranging the tree structure.\nLet's assume that we have a Fibonacci heap H. The algorithm to extract minimum node from Fibonacci heap H is as follows:\nLet's write a simple code to extract minimum node from Fibonacci heap.\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\n the key of node x to the new key k. Its a bit complex operation as it involves rearranging the tree structure.\nLet's assume that we have a \n, a \n and a \n. The algorithm to decrease key of node x to k in Fibonacci heap H is as follows:\nLet's write a simple code to decrease key of a node in Fibonacci heap.\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\n the node x from the heap H. Its a bit complex operation as it involves rearranging the tree structure.\nLet's assume that we have a Fibonacci heap H and a node x. The algorithm to delete node x from Fibonacci heap H is as follows:\nLet's write a simple code to delete a node from Fibonacci heap.\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nLet's analyze the time complexity of various operations in Fibonacci heap:\nThus, the \n is a data structure that supports \n, \n, \n, and \n operations in amortized \n time complexity.\nFollowing are some of the applications of Fibonacci heap:\nIn this tutorial, we learned about Fibonacci heap, its operations, and its time complexity. We also implemented the Fibonacci heap in C, C++, Java, and Python programming languages. We also discussed the applications of Fibonacci heap in various algorithms.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/binomial_heap.htm", "title": "Binomial Heaps", "content": "A \n is a collection of \n. A binomial tree Bk is an ordered tree defined recursively. A binomial Tree B0 consists of a single node.\nA binomial tree is an ordered tree defined recursively. A binomial tree \n consists of two binomial trees \n that are linked together. The root of one is the leftmost child of the root of the other.\nBinomial trees are defined recursively. A binomial tree B\n is defined as follows \nB\n is a single node.\nB\n is formed by linking two binomial trees B\n such that the root of one is the leftmost child of the root of the other.\nSome properties of binomial trees are \nBinomial tree with B\n has 2\n nodes.\nHeight of the tree is k\nThere are exactly \n nodes at depth i for all i in range 0 to k.\nAs mentioned above, a \n is a collection of \n. These binomial trees are linked together in a specific way. The binomial heap has the following properties \nEach binomial tree in the heap follows the \n.\nNo two \n in the heap can have the same number of nodes.\nThere is at most one \n of any order.\nFollowing is a representation of binomial heap, where B\n is a binomial heap of order k.\nSome binomial heaps are like below \nThis binomial Heap H consists of binomial trees B0, B2 and B3. Which have 1, 4 and 8 nodes respectively. And in total n = 13 nodes. The root of binomial trees are linked by a linked list in order of increasing degree\nFollowing are the operations that can be performed on a binomial heap \n As name suggests, it is used to insert a node into the heap.\n It is used to merge two binomial heaps into a single binomial heap.\n This operation removes the node with the smallest key from the heap.\n This operation decreases the key of a node in the heap.\n Simply put, it deletes a node from the heap.\nWe can implement a \n using a linked list of \n. Each node in the linked list is a \n. The root of each \n is the head of the linked list. The linked list is ordered by the order of the \n. The \n is the head of the linked list.\n of \n using following steps \nFollowing is the implementation of binomial heap in C, C++, Java and Python.\nIn the following code, first we create a structure Node to represent a node in the binomial heap. The structure contains the data, degree, child, sibling, and parent of the node. We then define functions to create a new node, merge two binomial trees, union two binomial heaps, adjust the heap, and insert a new node into the heap. Finally, we print the binomial heap.\nFollowing is the output of the above C code \nFollowing is the output of the above C++ code \nFollowing is the output of the above Java code \nFollowing is the output of the above Python code \nThe extract-min operation removes the node with the smallest key from the binomial heap. The extract-min operation is performed as follows \nFollowing is the implementation of extract-min operation in C, C++, Java and Python.\nFollowing is the output of the above C code \nFollowing is the output of the above C++ code \nFollowing is the output of the above Java code \nFollowing is the output of the above Python code \nDecrease-key is a important operation in the binomial heap. It is used to decrease the value of a key in the binomial heap. The decrease-key operation is performed as follows \nFollow the steps below to decrease-key.\nFollowing is the implementation of decrease-key operation in C, C++, Java and Python.\nFollowing is the output of the above C code \nFollowing is the output of the above C++ code \nFollowing is the output of the above Java code \nFollowing is the output of the above Python code \nDelete operation is used for deleting a key from a binomial heap.\n\nFollowing is the implementation of delete operation in C, C++, Java and Python.\nFollowing is the output of the above C code \nFollowing is the output of the above C++ code \nFollowing is the output of the above Java code \nFollowing is the output of the above Python code \nFollowing are the time complexities of the Binomial Heap \nBinomial heaps are used in plenty of applications. Some of them are listed below \nIn this tutorial, we have learned about Binomial Heap, its operations, and its implementation in C, C++, Java, and Python. We also discussed the time complexity of the Binomial Heap and its applications.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/binary_heaps.htm", "title": "Binary Heaps", "content": "or \n is a special case of balanced binary tree data structure. It is a complete binary tree structure. Means, all levels of the tree are fully filled except possibly for the last level which has all keys as left as possible.\nIn this structure, the \n is compared to its children, meaning that the root node will be either the smallest or the largest element in the tree. The root node will be the topmost element of all the nodes in the tree.\nBinary heap is represented as an array. The root element will be at \n. For any given node at position \n, the left child will be at position \n and the right child at position \n.\nFor a Min-Heap, the root element will be the minimum element in the tree. The left child will be greater than the parent and the right child will be greater than the parent. The representation of a Min-Heap is as follows:\nFor a Max-Heap, the root element will be the maximum element in the tree. The left child will be less than the parent and the right child will be less than the parent. The representation of a Max-Heap is as follows:\nThere are mainly three operations that can be performed on a binary heap:\n operations are used for maintaining the heap property of the binary heap. There are two types of heapify operations:\n a new element in the heap is done by inserting the element at the end of the heap and then \n the heap.\nThe \n process is done by comparing the new element with its parent and swapping the elements if the new element is smaller than the parent. The process is repeated until the new element is greater than its parent.\nFollowing is the algorithm for Insert Operation on Binary Heap:\n1.Insert the new element at the end of the heap.\n2.Initialize i as the index of the new element.\n3.WHILE i is not 0 AND heap[parent(i)] > heap[i]\n4.SWAP heap[i] and heap[parent(i)]\n5.Update i to parent(i)\nFollowing is the code for Insert Operation on Binary Heap:\nFollowing is the output of the above C program:\nFollowing is the output of the above C++ program:\nFollowing is the output of the above Java program:\nFollowing is the output of the above Python program:\n the root element of the heap is done by replacing the root element with the last element of the heap and then \n the heap.\nThe \n process is done by comparing the root element with its children and swapping the elements if the root element is greater than its children. The process is repeated until the root element is less than its children.\nFollowing is the algorithm for Delete Operation on Binary Heap:\nFollowing is the code for Delete Operation on Binary Heap:\nFollowing is the output of the above C program:\nFollowing is the output of the above C++ program:\nFollowing is the output of the above Java program:\nFollowing is the output of the above Python program:\nBinary Heap is used for many applications. Some of the common applications of Binary Heap are as follows:\n are useful data structures which can be used to manage the priority of the element. It is the backbone of many algorithms and data structures. It is used in many applications like \n, \n, \n, etc.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/heap_data_structure.htm", "title": "Heap Data Structure", "content": "Heap is a special case of balanced binary tree data structure where the root-node key is compared with its children and arranged accordingly. If \n has child node \n then −\nkey(α) ≥ key(β)\nAs the value of parent is greater than that of child, this property generates \n. Based on this criteria, a heap can be of two types −\n − Where the value of the root node is less than or equal to either of its children.\n − Where the value of the root node is greater than or equal to either of its children.\nBoth trees are constructed using the same input and order of arrival.\nWe shall use the same example to demonstrate how a Max Heap is created. The procedure to create Min Heap is similar but we go for min values instead of max values.\nWe are going to derive an algorithm for max heap by inserting one element at a time. At any point of time, heap must maintain its property. While insertion, we also assume that we are inserting a node in an already heapified tree.\n − In Min Heap construction algorithm, we expect the value of the parent node to be less than that of the child node.\nLet's understand Max Heap construction by an animated illustration. We consider the same input sample that we used earlier.\nFollowing are the implementations of this operation in various programming languages −\nLet us derive an algorithm to delete from max heap. Deletion in Max (or Min) Heap always happens at the root to remove the Maximum (or minimum) value.\nFollowing are the implementations of this operation in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/path_compression_union_by_rank.htm", "title": "Path Compression and Union by Rank: Optimizing Disjoint Sets", "content": "Path Compression and Union by Rank are two optimization techniques used in Disjoint Set Data Structure. These techniques help to improve the efficiency of the Disjoint Set operations like Find, Union, and MakeSet.\nAs the name suggests, Path Compression is a technique that compresses the path from a node to the root of the tree. When we perform a Find operation on a node, we traverse the path from the node to the root of the tree. During this traversal, we update the parent of each node to the root of the tree. \nIt helps to reduce the height of the tree and improves the efficiency of the Find operation.\nHere is the algorithm for Path Compression −\nLet's see the code for Path Compression in C, C++, Java, and Python −\nFollowing is the output of the above C code −\nFollowing is the output of the above C++ code −\nFollowing is the output of the above Java code −\nFollowing is the output of the above Python code −\nUnion by Rank is another optimization technique used in Disjoint Set Data Structure. In this technique, we maintain the rank of each node in the tree. The rank of a node is the height of the tree rooted at that node. When we perform a Union operation, we merge the tree with a smaller rank into the tree with a larger rank.\nIt helps to keep the height of the tree as small as possible and improves the efficiency of the Union operation.\nHere is the algorithm for Union by Rank −\nLet's see the code for Union by Rank in C, C++, Java, and Python −\nFollowing is the output of the above C code −\nFollowing is the output of the above C++ code −\nFollowing is the output of the above Java code −\nFollowing is the output of the above Python code −\nLet's compare the efficiency of the Disjoint Set operations with and without Path Compression and Union by Rank −\nPath Compression and Union by Rank are two optimization techniques used in Disjoint Set Data Structure. These techniques help to improve the efficiency of the Disjoint Set operations like Find, Union, and MakeSet.\nUsing both Path Compression and Union by Rank, we can keep the height of the tree as small as possible and improve the efficiency of the Disjoint Set operations.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/disjoint_set.htm", "title": "Disjoint Set Data Structure", "content": "also known as \n. It is a type of data structure that keeps track of a collection of elements that are partitioned into multiple non-overlapping (one element can be in only one set) disjoint sets.\nIt provides operations for \n two sets, finding the \n of the set to which an element belongs, and finding if two elements are in the same set.\nImagine you have a group of \n. Initially, each student is in his/her own group. Over time, some students form their own groups due to common interests, and these groups can merge with other groups.\nFor example,\nIn the above example, we can see that the students are partitioned into multiple disjoint sets. They also merge with other sets to form a bigger set. Here, we can use disjoint set data structure to keep track of these sets.\nFollowing are the key features of disjoint set data structure:\nFollowing are the operations that can be performed on disjoint set:\nDisjoint set can be implemented using the following data structures:\nFollowing is the example code for disjoint set using array:\nIn order to implement disjoint set using array, we need to create a structure that contains an array to store the parent of each element and the number of elements in the set. We will also cover the following operations −\nThese operations will help us to create a new set with element x, find the representative of the set to which element x belongs and merge the sets to which elements x and y belong.\nWe need to follow the following steps to implement disjoint set using array:\nFollowing is code for implementing disjoint set using array:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing are the time complexities of disjoint set operations:\nFollowing are the applications of disjoint set data structure:\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/collision_in_hashing.htm", "title": "Collision in Hashing", "content": "is a data structure that uses a \n to map data to a location in the data structure. The hash function takes the data as input and returns an index in the data structure where the data should be stored. However, there can be cases where two different data elements map to the same index in the data structure. This is known as a \n.\nFor example, suppose we have a \n with 10 buckets and a hash function that maps data elements to the buckets based on their value. If two data elements have the same hash value, they will be stored in the same bucket, causing a collision.\nIf there is a collision, we need to resolve it for the data structure to work correctly. There are several techniques to handle collisions in hashing:\n is also known as \n. In open addressing all the keys are stored directly into the hash table. When situation arises where two keys are mapped to the same position, the algorithm searches for the next empty slot in the hash table for storing the key. \nThere are several techniques for open addressing:\nThe algorithm of open addressing is as follows:\nFollowing code demonstrates the open addressing technique using linear probing in C, C++, Python, Java programming languages.\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output obtained is as shown below −\nFollowing is the output of the above code −\n is also known as open hashing, in this techniques each slot in the hash table is a linked list. When a collision occurs, the data elements are stored in the linked list at that slot. This allows multiple data elements to be stored at the same index in the hash table.\n is a simple and effective technique for handling collisions in hashing. It allows for efficient storage and retrieval of data elements, even when collisions occur.\nThere are several types of separate chaining techniques:\nThe algorithm of separate chaining is as follows:\nFollowing code demonstrates the separate chaining technique using linked list in C, C++, Python, Java programming languages.\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output obtained is as shown below −\nFollowing is the output of the above code −\n in \n occurs when two different data elements map to the same index in the data structure. This can be resolved using \n like \n and \n. These techniques allow for efficient storage and retrieval of data elements, even when collisions occur.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/hashing_data_structure.htm", "title": "Hashing in Data Structure", "content": "is a data structure, where we can store the data and look up that data very quickly. Hashing uses a special formula called a \n to map data to a location in the data structure.\nThe hash function takes the data as input and returns an index in the data structure where the data should be stored. This allows us to quickly retrieve the data by using the hash function to calculate the index and then looking up the data at that index.\nImagine you have a list of names and you want to look up a specific name in the list. You could use a \n to map the name to an index in a data structure, such as an array or a hash table. This allows you to quickly find the name in the data structure without having to search through the entire list.\nA \n is a mathematical function, which takes an input and returns a fixed size string of bytes. The output of the hash function is called a \n or \n. The hash function is designed to be fast and efficient, so that it can quickly calculate the hash value for a given input.\nHash functions have several important properties:\nA \n is a data structure that make use of hash function to map keys to values. It consists of an array of \n, where each bucket stores a key-value pair.\nThe hash function is used to calculate the index of the bucket where the key-value pair should be stored. This allows us to quickly retrieve the value associated with a key by using the hash function to calculate the index and then looking up the value at that index.\nHash tables have several important properties:\n occurs when we get similar output values, or rather hash values, for different input values. This can happen due to the limited range of hash values or the nature of the hash function.\nThere are several hashing algorithms that are commonly used in computer science, such as:\nThese algorithms are used for various applications, such as \n, \n, \n, and \n.\nHashing is used in various applications in computer science, such as:\nWe have discussed the concept of \n, \n, \n, and \n. We also looked at some common \n and \n in computer science.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/travelling_salesman_problem_dynamic_programming.htm", "title": "Travelling Salesman Problem (Dynamic Approach)", "content": "Travelling salesman problem is the most notorious computational problem. We can use brute-force approach to evaluate every possible tour and select the best one. For n number of vertices in a graph, there are \n number of possibilities. Thus, maintaining a higher complexity.\nHowever, instead of using brute-force, using the dynamic programming approach will obtain the solution in lesser time, though there is no polynomial time algorithm.\nLet us consider a graph \n, where \n is a set of cities and E is a set of weighted edges. An edge \n represents that vertices \n and \n are connected. Distance between vertex \n and \n is \n, which should be non-negative.\nSuppose we have started at city \n and after visiting some cities now we are in city \n. Hence, this is a partial tour. We certainly need to know \n, since this will determine which cities are most convenient to visit next. We also need to know all the cities visited so far, so that we don't repeat any of them. Hence, this is an appropriate sub-problem.\nFor a subset of cities \n that includes \n, and \n \n be the length of the shortest path visiting each node in S exactly once, starting at 1 and ending at \n.\nWhen \n , we define \n since the path cannot start and end at 1.\nNow, let express \n in terms of smaller sub-problems. We need to start at 1 and end at \n. We should select the next city in such a way that\n$$C\\left ( S,j \\right )\\, =\\, min\\, C\\left ( S\\, -\\, \\left\\{j \\right\\},i \\right )\\, +\\, d\\left ( i,j \\right )\\: where\\:  i\\:  \\epsilon \\: S\\: and\\: i\\neq j$$\nThere are at the most \n sub-problems and each one takes linear time to solve. Therefore, the total running time is \n.\nIn the following example, we will illustrate the steps to solve the travelling salesman problem.\nFrom the above graph, the following table is prepared.\n$$Cost\\left ( 2,\\Phi ,1 \\right )\\, =\\, d\\left ( 2,1 \\right )\\,=\\,5$$\n$$Cost\\left ( 3,\\Phi ,1 \\right )\\, =\\, d\\left ( 3,1 \\right )\\, =\\, 6$$\n$$Cost\\left ( 4,\\Phi ,1 \\right )\\, =\\, d\\left ( 4,1 \\right )\\, =\\, 8$$\n$$Cost(i,s)=min\\left\\{Cos\\left ( j,s-(j) \\right )\\, +\\,d\\left [ i,j \\right ]  \\right\\}$$\n$$Cost(2,\\left\\{3 \\right\\},1)=d[2,3]\\, +\\, Cost\\left ( 3,\\Phi ,1 \\right )\\, =\\, 9\\, +\\, 6\\, =\\, 15$$\n$$Cost(2,\\left\\{4 \\right\\},1)=d[2,4]\\, +\\, Cost\\left ( 4,\\Phi ,1 \\right )\\, =\\, 10\\, +\\, 8\\, =\\, 18$$\n$$Cost(3,\\left\\{2 \\right\\},1)=d[3,2]\\, +\\, Cost\\left ( 2,\\Phi ,1 \\right )\\, =\\, 13\\, +\\, 5\\, =\\, 18$$\n$$Cost(3,\\left\\{4 \\right\\},1)=d[3,4]\\, +\\, Cost\\left ( 4,\\Phi ,1 \\right )\\, =\\, 12\\, +\\, 8\\, =\\, 20$$\n$$Cost(4,\\left\\{3 \\right\\},1)=d[4,3]\\, +\\, Cost\\left ( 3,\\Phi ,1 \\right )\\, =\\, 9\\, +\\, 6\\, =\\, 15$$\n$$Cost(4,\\left\\{2 \\right\\},1)=d[4,2]\\, +\\, Cost\\left ( 2,\\Phi ,1 \\right )\\, =\\, 8\\, +\\, 5\\, =\\, 13$$\n$$Cost(2,\\left\\{3,4 \\right\\},1)=min\\left\\{\\begin{matrix}\nd\\left [ 2,3 \\right ]\\,+ \\,Cost\\left ( 3,\\left\\{ 4\\right\\},1 \\right )\\, =\\, 9\\, +\\, 20\\, =\\, 29  \\\\\nd\\left [ 2,4 \\right ]\\,+ \\,Cost\\left ( 4,\\left\\{ 3\\right\\},1 \\right )\\, =\\, 10\\, +\\, 15\\, =\\, 25  \\\\\n\\end{matrix}\\right.\\, =\\,25$$\n$$Cost(3,\\left\\{2,4 \\right\\},1)=min\\left\\{\\begin{matrix}\nd\\left [ 3,2 \\right ]\\,+ \\,Cost\\left ( 2,\\left\\{ 4\\right\\},1 \\right )\\, =\\, 13\\, +\\, 18\\, =\\, 31  \\\\\nd\\left [ 3,4 \\right ]\\,+ \\,Cost\\left ( 4,\\left\\{ 2\\right\\},1 \\right )\\, =\\, 12\\, +\\, 13\\, =\\, 25  \\\\\n\\end{matrix}\\right.\\, =\\,25$$\n$$Cost(4,\\left\\{2,3 \\right\\},1)=min\\left\\{\\begin{matrix}\nd\\left [ 4,2 \\right ]\\,+ \\,Cost\\left ( 2,\\left\\{ 3\\right\\},1 \\right )\\, =\\, 8\\, +\\, 15\\, =\\, 23  \\\\\nd\\left [ 4,3 \\right ]\\,+ \\,Cost\\left ( 3,\\left\\{ 2\\right\\},1 \\right )\\, =\\, 9\\, +\\, 18\\, =\\, 27  \\\\\n\\end{matrix}\\right.\\, =\\,23$$\n$$Cost(1,\\left\\{2,3,4 \\right\\},1)=min\\left\\{\\begin{matrix}\nd\\left [ 1,2 \\right ]\\,+ \\,Cost\\left ( 2,\\left\\{ 3,4\\right\\},1 \\right )\\, =\\, 10\\, +\\, 25\\, =\\, 35  \\\\\nd\\left [ 1,3 \\right ]\\,+ \\,Cost\\left ( 3,\\left\\{ 2,4\\right\\},1 \\right )\\, =\\, 15\\, +\\, 25\\, =\\, 40  \\\\\nd\\left [ 1,4 \\right ]\\,+ \\,Cost\\left ( 4,\\left\\{ 2,3\\right\\},1 \\right )\\, =\\, 20\\, +\\, 23\\, =\\, 43 \\\\\n\\end{matrix}\\right.\\, =\\, 35$$\nThe minimum cost path is 35.\nStart from cost {1, {2, 3, 4}, 1}, we get the minimum value for d [1, 2]. When s = 3, select the path from 1 to 2 (cost is 10) then go backwards. When s = 2, we get the minimum value for d [4, 2]. Select the path from 2 to 4 (cost is 10) then go backwards.\nWhen s = 1, we get the minimum value for d [4, 2] but 2 and 4 is already selected. Therefore, we select d [4, 3] (two possible values are 15 for d [2, 3] and d [4, 3], but our last node of the path is 4). Select path 4 to 3 (cost is 9), then go to s =  step. We get the minimum value for d [3, 1] (cost is 6).\nFollowing are the implementations of the above approach in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/longest_common_subsequence.htm", "title": "Longest Common Subsequence Algorithm", "content": "The longest common subsequence problem is finding the longest sequence which exists in both the given strings.\nBut before we understand the problem, let us understand what the term subsequence is −\nLet us consider a sequence S = <s\n, s\n, s\n, s\n, ,s\n>. And another sequence Z = <z\n, z\n, z\n, ,z\n> over S is called a subsequence of S, if and only if it can be derived from S deletion of some elements. In simple words, a subsequence consists of consecutive elements that make up a small part in a sequence.\nSuppose, \n and \n are two sequences over a finite set of elements. We can say that \n is a common subsequence of \n and \n, if \n is a subsequence of both \n and \n.\nIf a set of sequences are given, the longest common subsequence problem is to find a common subsequence of all the sequences that is of maximal length.\nLet \n be a sequence of length m and \n a sequence of length n. Check for every subsequence of \n whether it is a subsequence of \n, and return the longest common subsequence found.\nThere are \n subsequences of \n. Testing sequences whether or not it is a subsequence of \n takes O(n) time. Thus, the naive algorithm would take O(n2\n) time.\nLet X=<x\n,x\n,x\n....,x\n> and Y=<y\n,y\n,y\n....,y\n> be the sequences. To compute the length of an element the following algorithm is used.\n − Construct an empty adjacency table with the size, n  m, where n = size of sequence \n and m = size of sequence \n. The rows in the table represent the elements in sequence X and columns represent the elements in sequence Y.\n − The zeroeth rows and columns must be filled with zeroes. And the remaining values are filled in based on different cases, by maintaining a counter value.\n − If the counter encounters common element in both X and Y sequences, increment the counter by 1.\n − If the counter does not encounter common elements in X and Y sequences at T[i, j], find the maximum value between T[i-1, j] and T[i, j-1] to fill it in T[i, j].\n − Once the table is filled, backtrack from the last value in the table. Backtracking here is done by tracing the path where the counter incremented first.\n − The longest common subseqence obtained by noting the elements in the traced path.\nIn this procedure, table \n is computed in row major order and another table \n is computed to construct optimal solution.\nThis algorithm will print the longest common subsequence of \n and \n.\nTo populate the table, the outer for loop iterates m times and the inner \n loop iterates \n times. Hence, the complexity of the algorithm is \n, where \n and \n are the length of two strings.\nIn this example, we have two strings \n and \n to find the longest common subsequence.\nFollowing the algorithm, we need to calculate two tables 1 and 2.\nGiven n = length of X, m = length of Y\nX = BDCB, Y = BACDB\nIn the table below, the zeroeth rows and columns are filled with zeroes. Remianing values are filled by incrementing and choosing the maximum values according to the algorithm.\nOnce the values are filled, the path is traced back from the last value in the table at T[4, 5].\nFrom the traced path, the longest common subsequence is found by choosing the values where the counter is first incremented.\nIn this example, the final count is 3 so the counter is incremented at 3 places, i.e., B, C, B. Therefore, the longest common subsequence of sequences X and Y is BCB.\nFollowing is the final implementation to find the Longest Common Subsequence using Dynamic Programming Approach −\nThe longest common subsequence problem is a classic computer science problem, the basis of data comparison programs such as the diff-utility, and has applications in bioinformatics. It is also widely used by revision control systems, such as SVN and Git, for reconciling multiple changes made to a revision-controlled collection of files.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/01_knapsack_problem.htm", "title": "0-1 Knapsack Problem", "content": "We discussed the fractional knapsack problem using the greedy approach, earlier in this tutorial. It is shown that Greedy approach gives an optimal solution for Fractional Knapsack. However, this chapter will cover 0-1 Knapsack problem using dynamic programming approach and its analysis.\nUnlike in fractional knapsack, the items are always stored fully without using the fractional part of them. Its either the item is added to the knapsack or not. That is why, this method is known as the\n.\nHence, in case of 0-1 Knapsack, the value of \n can be either \n or \n, where other constraints remain the same.\n0-1 Knapsack cannot be solved by Greedy approach. Greedy approach does not ensure an optimal solution in this method. In many instances, Greedy approach may give an optimal solution.\n − A thief is robbing a store and can carry a maximal weight of \n into his knapsack. There are \n items and weight of i\n item is wi and the profit of selecting this item is \n. What items should the thief take?\nLet i be the highest-numbered item in an optimal solution \n for \n dollars. Then \n is an optimal solution for \n dollars and the value to the solution S is V\n plus the value of the sub-problem.\nWe can express this fact in the following formula: define \n to be the solution for items \n and the maximum weight \n.\nThe algorithm takes the following inputs\nThe maximum weight \nThe number of items \nThe two sequences \nThe set of items to take can be deduced from the table, starting at \n and tracing backwards where the optimal values came from.\nIf \n, then item i is not part of the solution, and we continue tracing with \n. Otherwise, item i is part of the solution, and we continue tracing with \n.\nThe following examples will establish our statement.\nLet us consider that the capacity of the knapsack is W = 8 and the items are as shown in the following table.\nUsing the greedy approach of 0-1 knapsack, the weight thats stored in the knapsack would be A+B = 4 with the maximum profit 2 + 4 = 6. But, that solution would not be the optimal solution.\nTherefore, dynamic programming must be adopted to solve 0-1 knapsack problems.\nConstruct an adjacency table with maximum weight of knapsack as rows and items with respective weights and profits as columns.\nValues to be stored in the table are cumulative profits of the items whose weights do not exceed the maximum weight of the knapsack (designated values of each row)\nSo we add zeroes to the 0\n row and 0\n column because if the weight of item is 0, then it weighs nothing; if the maximum weight of knapsack is 0, then no item can be added into the knapsack.\nThe remaining values are filled with the maximum profit achievable with respect to the items and weight per column that can be stored in the knapsack.\nThe formula to store the profit values is −\n$$c\\left [ i,w \\right ]=max\\left\\{c\\left [ i-1,w-w\\left [ i \\right ] \\right ]+P\\left [ i \\right ] \\right\\}$$\nBy computing all the values using the formula, the table obtained would be −\nTo find the items to be added in the knapsack, recognize the maximum profit from the table and identify the items that make up the profit, in this example, its {1, 7}.\nThe optimal solution is {1, 7} with the maximum profit is 12.\nThis algorithm takes (n.w) times as table c has (n+1).(w+1) entries, where each entry requires (1) time to compute.\nFollowing is the final implementation of 0-1 Knapsack Algorithm using Dynamic Programming Approach.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/divide_and_conquer.htm", "title": "Divide & Conquer Algorithm", "content": "Using divide and conquer approach, the problem in hand, is divided into smaller sub-problems and then each problem is solved independently. When we keep dividing the sub-problems into even smaller sub-problems, we may eventually reach a stage where no more division is possible. Those smallest possible sub-problems are solved using original solution because it takes lesser time to compute. The solution of all sub-problems is finally merged in order to obtain the solution of the original problem.\nBroadly, we can understand \n approach in a three-step process.\nThis step involves breaking the problem into smaller sub-problems. Sub-problems should represent a part of the original problem. This step generally takes a recursive approach to divide the problem until no sub-problem is further divisible. At this stage, sub-problems become atomic in size but still represent some part of the actual problem.\nThis step receives a lot of smaller sub-problems to be solved. Generally, at this level, the problems are considered 'solved' on their own.\nWhen the smaller sub-problems are solved, this stage recursively combines them until they formulate a solution of the original problem. This algorithmic approach works recursively and conquer & merge steps works so close that they appear as one.\nThere are various ways in which various algorithms can take input such that they can be solved using the divide and conquer technique. Arrays are one of them. In algorithms that require input to be in the form of a list, like various sorting algorithms, array data structures are most commonly used.\nIn the input for a sorting algorithm below, the array input is divided into subproblems until they cannot be divided further.\nThen, the subproblems are sorted (the conquer step) and are merged to form the solution of the original array back (the combine step).\nSince arrays are indexed and linear data structures, sorting algorithms most popularly use array data structures to receive input.\nAnother data structure that can be used to take input for divide and conquer algorithms is a linked list (for example, merge sort using linked lists). Like arrays, linked lists are also linear data structures that store data sequentially.\nConsider the merge sort algorithm on linked list; following the very popular tortoise and hare algorithm, the list is divided until it cannot be divided further.\nThen, the nodes in the list are sorted (conquered). These nodes are then combined (or merged) in recursively until the final solution is achieved.\nVarious searching algorithms can also be performed on the linked list data structures with a slightly different technique as linked lists are not indexed linear data structures. They must be handled using the pointers available in the nodes of the list.\nDivide and conquer approach supports parallelism as sub-problems are independent. Hence, an algorithm, which is designed using this technique, can run on the multiprocessor system or in different machines simultaneously.\nIn this approach, most of the algorithms are designed using recursion, hence memory management is very high. For recursive function stack is used, where function state needs to be stored.\nThe following computer algorithms are based on divide-and-conquer programming approach −\nClosest pair (points)\nThere are various ways available to solve any computer problem, but the mentioned are a good example of divide and conquer approach.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/fibonacci_series.htm", "title": "Fibonacci Series Using Recursion", "content": "Fibonacci series satisfies the following conditions −\nHence, a Fibonacci series can look like this −\nF\n = 0 1 1 2 3 5 8 13\nor, this −\nF\n = 1 1 2 3 5 8 13 21\nFor illustration purpose, Fibonacci of F\n is displayed as −\nFirst we try to draft the iterative algorithm for Fibonacci series.\nLet us learn how to create a recursive algorithm Fibonacci series. The base criteria of recursion.\nFollowing are the implementations of the above approach in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/tower_of_hanoi.htm", "title": "Tower of Hanoi Using Recursion", "content": "Tower of Hanoi, is a mathematical puzzle which consists of three towers (pegs) and more than one rings is as depicted −\nThese rings are of different sizes and stacked upon in an ascending order, i.e. the smaller one sits over the larger one. There are other variations of the puzzle where the number of disks increase, but the tower count remains the same.\nThe mission is to move all the disks to some another tower without violating the sequence of arrangement. A few rules to be followed for Tower of Hanoi are −\nFollowing is an animated representation of solving a Tower of Hanoi puzzle with three disks.\nTower of Hanoi puzzle with n disks can be solved in minimum \n steps. This presentation shows that a puzzle with 3 disks has taken \n steps.\nTo write an algorithm for Tower of Hanoi, first we need to learn how to solve this problem with lesser amount of disks, say → 1 or 2. We mark three towers with name, \n, \n and \n (only to help moving the disks). If we have only one disk, then it can easily be moved from source to destination peg.\nIf we have 2 disks −\nSo now, we are in a position to design an algorithm for Tower of Hanoi with more than two disks. We divide the stack of disks in two parts. The largest disk (n\n disk) is in one part and all other (n-1) disks are in the second part.\nOur ultimate aim is to move disk \n from source to destination and then put all other (n1) disks onto it. We can imagine to apply the same in a recursive way for all given set of disks.\nThe steps to follow are −\nA recursive algorithm for Tower of Hanoi can be driven as follows −\nFollowing are the implementations of this approach in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/recursion_basics.htm", "title": "Recursion Algorithms", "content": "Some computer programming languages allow a module or function to call itself. This technique is known as recursion. In recursion, a function \n either calls itself directly or calls a function \n that in turn calls the original function \n. The function \n is called recursive function.\n − a function calling itself.\n − a function that calls another function which in turn calls it again.\nA recursive function can go infinite like a loop. To avoid infinite running of recursive function, there are two properties that a recursive function must have −\n − There must be at least one base criteria or condition, such that, when this condition is met the function stops calling itself recursively.\n − The recursive calls should progress in such a way that each time a recursive call is made it comes closer to the base criteria.\nMany programming languages implement recursion by means of \n. Generally, whenever a function (\n) calls another function (\n) or itself as callee, the caller function transfers execution control to the callee. This transfer process may also involve some data to be passed from the caller to the callee.\nThis implies, the caller function has to suspend its execution temporarily and resume later when the execution control returns from the callee function. Here, the caller function needs to start exactly from the point of execution where it puts itself on hold. It also needs the exact same data values it was working on. For this purpose, an activation record (or stack frame) is created for the caller function.\nThis activation record keeps the information about local variables, formal parameters, return address and all information passed to the caller function.\nOne may argue why to use recursion, as the same task can be done with iteration. The first reason is, recursion makes a program more readable and because of latest enhanced CPU systems, recursion is more efficient than iterations.\nIn case of iterations, we take number of iterations to count the time complexity. Likewise, in case of recursion, assuming everything is constant, we try to figure out the number of times a recursive call is being made. A call made to a function is Ο(1), hence the (n) number of times a recursive call is made makes the recursive function Ο(n).\nSpace complexity is counted as what amount of extra space is required for a module to execute. In case of iterations, the compiler hardly requires any extra space. The compiler keeps updating the values of variables used in the iterations. But in case of recursion, the system needs to store activation record each time a recursive call is made. Hence, it is considered that space complexity of recursive function may go higher than that of a function with iteration.\nFollowing are the implementations of the recursion in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/priority_search_tree.htm", "title": "Priority Search Tree Data Structure", "content": "Priority search tree is a hybrid data structures, means it is a combination of binary search tree and priority queue. It is used for storing a set of points in a two-dimensional space, which are ordered by their priority and also key value.\nIt is a data structure that stores the points in sorted order based on their x-coordinate. Each node in the tree contains a priority queue that stores the points in sorted order based on their y-coordinate.\nThis data structure mainly used for solving the \n problem in computational geometry.\nLet's suppose you have a set of points in a two-dimensional space, and you want to find all the points that are within a given range. Let us understand how we can achieve this using a priority search tree.\nWhen we want to find all the points that are within a given range, we start at the root of the tree and recursively search the tree based on the x-coordinate of the range. At each node, we check if the range intersects the x-coordinate of the node. If it does, we search the priority queue at the node based on the y-coordinate of the range and retrieve the points that lie within the range.\nBy using a priority search tree, we can easily find all the points that lie within a given range in a two-dimensional space.\nPriority search tree has two main components:\n The binary search tree stores the points in sorted order based on their x-coordinate.\nThe left subtree of a node contains points with x-coordinates less than the x-coordinate of the node, and the right subtree contains points with x-coordinates greater than the x-coordinate of the node.\n The priority queue stores the points in sorted order based on their y-coordinate.\nEach node in the tree contains a priority queue that stores the points in sorted order based on their y-coordinate.\nPriority search tree supports the following operations −\n Insert a point into the priority search tree.\n Search for a point in the priority search tree based on its x-coordinate.\n Find all the points that lie within a given range in the two-dimensional space.\nPriority search tree can be implemented using a binary search tree and a priority queue.\nHere is an example of a priority search tree implemented using a binary search tree and a priority queue:\nOutput of the above code will be:\nOutput of the above code will be:\nOutput of the above code will be:\nOutput of the above code will be:\nSearching for a point in a priority search tree is similar to searching for a key in a binary search tree. We start at the root of the tree and recursively search the tree based on the x-coordinate of the point.\nLet us understand how the search operation works in a priority search tree:\nFollowing is the algorithm, follow the steps:\nHere is an example code that demonstrates the search operation in a priority search tree:\nOutput of the above code will be:\nOutput of the above code will be:\nOutput of the above code will be:\nOutput of the above code will be:\nThis operation is the main application of the priority search tree. It is used to find all the points that lie within a given range in the two-dimensional space.\nFollowing is the algorithm, follow the steps:\nHere is an example code that demonstrates the range query operation in a priority search tree:\nOutput of the above code will be:\nOutput of the above code will be:\nOutput of the above code will be:\nOutput of the above code will be:\nFollowing are the applications of a Priority Search Tree −\n : Priority search tree is mostly used in computational geometry to solve two-dimensional range query problems.\n : Used for search records in a database based on their x and y coordinates.\n : It can be used for scheduling tasks based on their priority and key value.\nIn this chapter, we have learned about the priority search tree data structure and its applications. We have also seen how to implement a priority search tree using a binary search tree and a priority queue. We have discussed the operations supported by the priority search tree, such as insert, search, and range query.\nWe have also seen how to search for a point in a priority search tree and find all the points that lie within a given range in a two-dimensional space.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/k_d_trees.htm", "title": "K-Dimensional (K-D) Trees in Datastructures", "content": "The \n is a multi-dimensional binary search tree. It is defined as a data structure for storing \n. This structure has been implemented to solve a number of \n problems in statistics and data analysis.\nA \n (short for \n) is defined as a space-partitioning data structure for organizing points in a \n. Data structure \n are implemented for several applications, for example, searches involving a \n (e.g. \n and \n). \n are treated as a special case of \n.\nFollowing are the properties of k-d trees:\nFollowing are the operations that can be performed on k-d trees:\nThe construction of k-d trees is done by using recursive partitioning. The steps to construct a k-d tree are as follows:\nNow, let's code the construction of k-d trees:\nWe performed insert operation in order to construct a k-d tree.\nFollowing is the example code to construct a k-d tree in C, C++, Java, and Python:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nThe delete operation in k-d trees is performed by following the below steps:\nFollowing is the example code to delete a node from k-d trees in C, C++, Java, and Python:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nThe search operation in k-d trees is performed by following the below steps:\nFollowing is the example code to search a node in k-d trees in C, C++, Java, and Python:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nThe time complexity of k-d trees for various operations is as follows:\nK-D trees are used in the following applications:\nIn this chapter, we discussed k-d trees, which are a data structure used for storing k-dimensional points. We discussed the construction of k-d trees, operations on k-d trees, and the time complexity of k-d trees. We also provided example code to construct, insert, delete, and search nodes in k-d trees in C, C++, Java, and Python.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/k_ary_tree.htm", "title": "k-ary Tree in Data Structure", "content": "K-ary tree also known as \n or \n is a tree data structure in which each node has at most \n children. The value of \n is fixed for a given tree. The value of \n can be 2, 3, 4, 5, etc.\nFor example, a binary tree is a 2-ary tree, a ternary tree is a 3-ary tree, and so on.\nFollowing are the characteristics of K-ary tree:\nThere are two types of K-ary tree:\nK-ary tree can be represented using an array. The array representation of a complete K-ary tree is as follows:\nFor example, consider a 3-ary tree:\nThe array representation of the above 3-ary tree is as follows:\nThe formula to find the parent node of the ith node is \n.\nThe formula to find the child nodes of the ith node is \n.\nSimilarly, we can find the child nodes of each node using the above formula.\nFollowing are the important operations that can be performed on a K-ary tree:\nWe will discuss the Insertion operation on the K-ary tree. The following is the algorithm to insert a new node into the K-ary tree\nFollow the steps below to insert a new node into the K-ary tree:\nFollowing is the code to insert a new node into the K-ary tree:\nFollowing is the output of the above C program:\nFollowing is the output of the above C++ program:\nFollowing is the output of the above Java program:\nFollowing is the output of the above Python program:\nThere are different ways to traverse a K-ary tree:\nFollow the steps below to perform an inorder traversal of a K-ary tree:\nFollow the steps below to perform a preorder traversal of a K-ary tree:\nFollow the steps below to perform a postorder traversal of a K-ary tree:\nFollowing is the code to perform inorder, preorder, and postorder traversal of a K-ary tree:\nFollowing is the output of the above C program:\nFollowing is the output of the above C++ program:\nFollowing is the output of the above Java program:\nFollowing is the output of the above Python program:\nFollow the steps below to perform a level order traversal of a K-ary tree:\nFollowing is the code to perform a level order traversal of a K-ary tree:\nFollowing is the output of the above C program:\nFollowing is the output of the above C++ program:\nFollowing is the output of the above Java program:\nFollowing is the output of the above Python program:\nFollow the steps below to search for a specific node in a K-ary tree:\nFollowing is the code to search for a specific node in a K-ary tree:\nFollowing is the output of the above C program:\nFollowing is the output of the above C++ program:\nFollowing is the output of the above Java program:\nFollowing is the output of the above Python program:\nK-ary tree is used in the following applications:\nIn this tutorial, we learned about K-ary trees, their representation, and different operations like insertion, traversal, search, and level order traversal. We also discussed the applications of K-ary trees. You can now implement K-ary trees in your programs and solve problems related to them.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/hashed_array_tree.htm", "title": "Hashed Array Tree", "content": "is a type of data structure that is used for storing and managing data in the form of an array. It is a combination of an array and a hash table. The hashed array tree provides the benefits of both arrays and hash tables, such as \n to elements and \n.\nIt has a \n, so it can grow and shrink as needed. The hashed array tree is implemented using an array that stores the elements and a hash table that maps the keys to the indices of the elements in the array.\nLet's understand how a hashed array tree works with the help of an example:\nSuppose we have a set of elements {A, B, C, D, E, F, G, H, I, J} and we want to store these elements in a hashed array tree.\nHere's how a hashed array tree works:\nA hashed array tree supports the following operations:\nNow, let's understand how we can implement a hashed array tree. We need to keep few things in mind while implementing hashed array trees we need to handle collisions, resizing the array, and rehashing the keys.\nFollowing is the algorithm for Insert Operation −\nFollowing is an example of how we can implement the insert operation in a hashed array tree:\nFollowing is the output of the above C program:\nFollowing is the output of the above C++ program:\nFollowing is the output of the above Java program:\nFollowing is the output of the above Python program:\nFollowing is the algorithm for the Get Operation on Hashed Array Tree −\nFollowing is an example of how we can implement the get operation in a hashed array tree:\nFollowing is the output of the above C program:\nFollowing is the output of the above C++ program:\nFollowing is the output of the above Java program:\nFollowing is the output of the above Python program:\nFollowing is the algorithm for the Delete Operation on Hashed Array Tree −\nFollowing is an example of how we can implement the delete operation in a hashed array tree:\nFollowing is the output of the above C program:\nFollowing is the output of the above C++ program:\nFollowing is the output of the above Java program:\nFollowing is the output of the above Python program:\nThe time complexity of operations on a hashed array tree is as follows:\nHashed array tree is used in the following applications:\nIn this chapter, we learned about \n in data structures. We discussed how \n works, its operations, implementation, and time complexity. We also saw examples of \n, \n, and \n operations on a \n in C, C++, Java, and Python. \n is a powerful data structure that provides \n to elements and \n.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/fusion_tree.htm", "title": "Fusion Tree", "content": "is a part of advanced data structures which is used for maintaining the ordered set of elements.\nOn a finite universe, if each input integers has size less than 2\n and if those numbers are non-negative, it implements an associative array on \n integers.\nIt uses the combination of a \n and a \n which help in reducing the time complexity of the operations like \n, \n and \n in the tree.\nFollowing factors are considered while implementing a fusion tree:\nFusion tree is represented as a \n of height \n where \n is the number of elements in the tree.\nHere, \n is the number of bits in the integer.\nEach node can hold child point upto \n just like a \n where \n is the maximum number of children a node can have.\nFollowing are the operations that can be performed on a fusion tree:\nNow let's see how we can implement a fusion tree in C, C++, Java and Python:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\n and \n operations in a fusion tree are similar to the insertion operation. You can implement these operations by modifying the above code.\n operation is used to find the element in the tree and \n operation is used to delete the element from the tree.\nFollowing are the steps to implement search and delete operations in a fusion tree:\nNow let's see how we can implement search and delete operations in a fusion tree in C, C++, Java and Python:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nTime complexity of the operations on a fusion tree is as follows:\nHere, n is the number of elements in the tree and W is the word size.\nFusion trees are used in various applications where we need to store a large number of elements and perform operations like insertion, deletion, and searching efficiently. Some of the applications of fusion trees are:\nThese are some of the applications where fusion trees are used to store and manage large datasets efficiently.\nIn this chapter, we learned about fusion trees, a data structure that is used to store a large number of elements efficiently. We discussed the structure of a fusion tree, its operations like insertion, deletion, and searching, and the time complexity of these operations. We also saw how to implement fusion trees in C, C++, Java, and Python and perform search and delete operations on them. We also discussed the applications of fusion trees in various fields.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/fenwick_tree.htm", "title": "Fenwick Tree or, Binary Indexed Tree", "content": "also known as \n is a data structure that is designed for querying and updating the prefix sums of an array efficiently.\nFenwick trees permit both operations to be accomplished in \n time. This is obtained by representing the numbers as a tree, where the value of each node is treated as the sum of the numbers in that subtree. The tree structure permits operations to be accomplished using only \n node accesses.\nFor example, you are tracking customer reward points for each day on basis of the number of orders placed. You need to calculate the total reward points for a range of days.\nYou want to quickly check their total points up to a specific day and update the points for a specific day. Here, you can use \n to calculate the cumulative sum of the reward points in a range of days.\nFollowing are the characteristics of Fenwick tree:\n is a binary tree that is represented as an array. It is implemented as a one-dimensional array where each element stores information about a segment of the original array.\nIt uses \n for showing cumulative data.\nThere are basically two main operations that can be performed on Fenwick tree:\nWe will find the \n of the elements from the start of the array to a specific index. This example will help you understand the \n on Fenwick tree.\nIn order to perform the \n on Fenwick tree, all you need to do is follow the steps below:\nFollowing is the code for the query operation on Fenwick tree in C, C++, Java, and Python:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nWe will update the value of an element in the array. This example will help you understand the \n on Fenwick tree.\nWe need to use the \n when we need to change the dynamic values of the array. For example, you are tracking customer reward points for each day on the basis of the number of orders placed. You need to update the points for a specific day. Here, you can use the \n on Fenwick tree to update the points for a specific day.\nFollow the steps below to perform the \n on Fenwick tree:\nFollowing is the code for the update operation on Fenwick tree in C, C++, Java, and Python:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing are the applications of Fenwick tree:\nFollowing is the complexity of Fenwick tree:\nIn this chapter, we learned about what is \n, its \n, \n, \n, and \n. We also saw how to perform \n and \n on \n using code examples in \n, \n, \n, and \n. \n is a powerful data structure that allows us to efficiently calculate the \n of an array and perform \n and \n.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/segment_trees.htm", "title": "Segment Trees", "content": "is a binary tree where each node represents an interval. The root node represents the whole array and the leaf nodes represent the single element of the array.\n is  a data structure which is used for solving range queries in logarithmic time. It is used for storage of \n or \n of elements.\nInterval is a range of elements between the start and end index.Intervals are dynamic as they change during operations. For example, if we have an array of 10 elements [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. Then the interval between the 3rd and 7th index is [3, 4, 5, 6, 7].\nThe Segment is little different from the interval. Means, it is refered to a specific range that represented by the node of the segment tree. The segment is static as it does not change during operations it is fixed.\nLet's understand the segment with an example.\nHere, the entire array is divided into two segments. The first segment is [1, 2, 3, 4, 5] and the second segment is [6, 7, 8, 9, 10].\n is a data structure that is build for solving the range query problems.\nFor example, you have an array of 10 elements [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. Now you have to find the minimum value between the 3rd and 7th index.\nSo, the minimum will be 3 between the 3rd and 7th index.\nIf we solve this problem using the general brute force approch time complexity will be \n where \n is the total elements of the array. This is not better way to do this problem because if we have a large dataset then it will take more time to find the minimum value.\nSo, to solve this problem efficiently we can use the segment tree data structure. In this tutorial let's understand how we can solve this problem using the segment tree.\n You should be familiar with the recursion and tree data structure before learning the segment tree.\nHere, we will represent the segment tree using the array. The segment tree is a binary tree where each node represents an interval. The root node represents the whole array and the leaf nodes represent the single element of the array.\nIn the above image, the segment tree is formed for finding the maximum value in the array [3,2,1,0,4,5] where the segment of the root node is [0,5]. The left child of the root node represents the segment [0,2] and the right child of the root node represents the segment [3,5]. Similary it goes on.\nEach node of the segment tree contains the following information:\nSegment tree works in the following way:\nThe sengment tree uses the recursive approach to build the tree. The steps to build the segment tree are:\nLet's understand the steps to build the segment tree with an example.\nNow, we will build the segment tree for the sum query of the array [4, 3, 2, 1, 6, 7].\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output is as follows −\nFollowing is the output of the above code −\nSegment tree is used in various applications, such as:\nIn this tutorial, we have learned about the segment tree data structure. We have seen how the segment tree is built and how it is used to solve the range query problems. We have also seen the applications of the segment tree in various fields.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/range_queries.htm", "title": "Range Queries", "content": "is a question that asks for the sum, minimum, maximum, or average of the values between the two indices of a dataset. It is a common operation in data structures and is used in various applications.\nFor example, suppose you have to know the sum of all the values between the 3rd and 7th index of an array. This is a sum range query.\nLet's assume you have a list of numbers : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nNow, someone asks you to find the sum between the 3rd and 7th index. You will have to add the values at the 3rd, 4th, 5th, 6th, and 7th index. Let's do it.\nWe got the sum of the values between the 3rd and 7th index. This is a range query, where we have to find the sum of the values between the start and end index.\nNow, let's understand what and how range queries are used in data structures.\n are mostly used for finding the sum of the values between the start and end index. But, it can be used for other operations like finding the minimum, maximum, or average of the values between the start and end index.\nFor example, you might have a large dataset (like an array, a list, or a tree), and you want to quickly access the range of values. In this case, instead of iterating through the entire dataset, you can use a data structure that supports range queries.\nThere are many data structures that support range queries, and those are listed below:\nThere are different types of range queries that can be performed on a dataset. Some of the common types are:\nIt is used for calculating the sum of the values between the start and end index.\nIt calculates the minimum or maximum value between the start and end index.\nIt updates the values between the start and end index.\nIt counts the number of values between the start and end index.\nRange Queries are used in various applications, such as:\nThese are some of the applications where range queries are used to efficiently access and process the data.\nRange queries are important concept in data structures. You can use it for various operations and also it is useful in competitive programming. You can use range queries to solve many problems efficiently.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/splay_trees.htm", "title": "Splay Trees", "content": "Splay trees are the altered versions of the Binary Search Trees, since it contains all the operations of BSTs, like insertion, deletion and searching, followed by another extended operation called \n.\nFor instance, a value \"A\" is supposed to be inserted into the tree. If the tree is empty, add \"A\" to the root of the tree and exit; but if the tree is not empty, use binary search insertion operation to insert the element and then perform splaying on the new node.\nSimilarly, after searching an element in the splay tree, the node consisting of the element must be splayed as well.\n Splaying, in simpler terms, is just a process to bring an operational node to the root. There are six types of rotations for it.\nZig rotation\nZag rotation\nZig-Zig rotation\nZag-Zag rotation\nZig-Zag rotation\nZag-Zig rotation\nThe zig rotations are performed when the operational node is either the root node or the left child node of the root node. The node is rotated towards its right.\nAfter the shift, the tree will look like −\nThe zag rotations are also performed when the operational node is either the root node or the right child nod of the root node. The node is rotated towards its left.\nThe operational node becomes the root node after the shift −\nThe zig-zig rotations are performed when the operational node has both parent and a grandparent. The node is rotated two places towards its right.\nThe first rotation will shift the tree to one position right −\nThe second right rotation will once again shift the node for one position. The final tree after the shift will look like this −\nThe zag-zag rotations are also performed when the operational node has both parent and a grandparent. The node is rotated two places towards its left.\nAfter the first rotation, the tree will look like −\nThen the final tree after the second rotation is given as follows. However, the operational node is still not the root so the splaying is considered incomplete. Hence, other suitable rotations are again applied in this case until the node becomes the root.\nThe zig-zag rotations are performed when the operational node has both a parent and a grandparent. But the difference is the grandparent, parent and child are in LRL format. The node is rotated first towards its right followed by left.\nAfter the first rotation, the tree is  −\nThe final tree after the second rotation −\nThe zag-zig rotations are also performed when the operational node has both parent and grandparent. But the difference is the grandparent, parent and child are in RLR format. The node is rotated first towards its left followed by right.\nFirst rotation is performed, the tree is obtained as −\nAfter second rotation, the final tree is given as below. However, the operational node is not the root node yet so one more rotation needs to be performed to make the said node as the root.\nA splay contains the same basic operations that a Binary Search Tree provides with: Insertion, Deletion, and Search. However, after every operation there is an additional operation that differs them from Binary Search tree operations: Splaying. We have learned about Splaying already so let us understand the procedures of the other operations.\nThe insertion operation in a Splay tree is performed in the exact same way insertion in a binary search tree is performed. The procedure to perform the insertion in a splay tree is given as follows −\nCheck whether the tree is empty; if yes, add the new node and exit\nIf the tree is not empty, add the new node to the existing tree using the binary search insertion.\nThen, suitable splaying is chosen and applied on the newly added node.\nFollowing are the implementations of this operation in various programming languages −\nThe deletion operation in a splay tree is performed as following −\nApply splaying operation on the node to be deleted.\nOnce, the node is made the root, delete the node.\nNow, the tree is split into two trees, the left subtree and the right subtree; with their respective first nodes as the root nodes: say root_left and root_right.\nIf root_left is a NULL value, then the root_right will become the root of the tree. And vice versa.\nBut if both root_left and root_right are not NULL values, then select the maximum value from the left subtree and make it the new root by connecting the subtrees.\nFollowing are the implementations of Splay Tree deletion operation in various programming languages −\nThe search operation in a Splay tree follows the same procedure of the Binary Search Tree operation. However, after the searching is done and the element is found, splaying is applied on the node searched. If the element is not found, then unsuccessful search is prompted.\nFollowing are the implementations of this operation in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/b_plus_trees.htm", "title": "B+ Trees", "content": "The B+ trees are extensions of B trees designed to make the insertion, deletion and searching operations more efficient.\nThe properties of B+ trees are similar to the properties of B trees, except that the B trees can store keys and records in all internal nodes and leaf nodes while B+ trees store records in leaf nodes and keys in internal nodes. One profound property of the B+ tree is that all the leaf nodes are connected to each other in a single linked list format and a data pointer is available to point to the data present in disk file. This helps fetch the records in equal numbers of disk access.\nSince the size of main memory is limited, B+ trees act as the data storage for the records that couldnt be stored in the main memory. For this, the internal nodes are stored in the main memory and the leaf nodes are stored in the secondary memory storage.\nEvery node in a B+ Tree, except root, will hold a maximum of \n children and (m-1) keys, and a minimum of $\\mathrm{\\left \\lceil \\frac{m}{2}\\right \\rceil}$ children and $\\mathrm{\\left \\lceil \\frac{m-1}{2}\\right \\rceil}$ keys, since the order of the tree is \n.\nThe root node must have no less than two children and at least one search key.\nAll the paths in a B tree must end at the same level, i.e. the leaf nodes must be at the same level.\nA B+ tree always maintains sorted data.\nThe operations supported in B+ trees are Insertion, deletion and searching with the time complexity of \n for every operation.\nThey are almost similar to the B tree operations as the base idea to store data in both data structures is same. However, the difference occurs as the data is stored only in the leaf nodes of a B+ trees, unlike B trees.\nThe insertion to a B+ tree starts at a leaf node.\n − Calculate the maximum and minimum number of keys to be added onto the B+ tree node.\n − Insert the elements one by one accordingly into a leaf node until it exceeds the maximum key number.\n − The node is split into half where the left child consists of minimum number of keys and the remaining keys are stored in the right child.\n − But if the internal node also exceeds the maximum key property, the node is split in half where the left child consists of the minimum keys and remaining keys are stored in the right child. However, the smallest number in the right child is made the parent.\n − If both the leaf node and internal node have the maximum keys, both of them are split in the similar manner and the smallest key in the right child is added to the parent node.\nFollowing are the implementations of this operation in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/b_trees.htm", "title": "B Trees", "content": "B trees are extended binary search trees that are specialized in m-way searching, since the order of B trees is 'm'. Order of a tree is defined as the maximum number of children a node can accommodate. Therefore, the height of a b tree is relatively smaller than the height of AVL tree and RB tree.\nThey are general form of a Binary Search Tree as it holds more than one key and two children.\nThe various properties of B trees include −\nEvery node in a B Tree will hold a maximum of m children and (m-1) keys, since the order of the tree is m.\nEvery node in a B tree, except root and leaf, can hold at least m/2 children\nThe root node must have no less than two children.\nAll the paths in a B tree must end at the same level, i.e. the leaf nodes must be at the same level.\nA B tree always maintains sorted data.\nB trees are also widely used in disk access, minimizing the disk access time since the height of a b tree is low.\n − A disk access is the memory access to the computer disk where the information is stored and disk access time is the time taken by the system to access the disk memory.\nThe operations supported in B trees are Insertion, deletion and searching with the time complexity of \n for every operation.\nThe insertion operation for a B Tree is done similar to the Binary Search Tree but the elements are inserted into the same node until the maximum keys are reached. The insertion is done using the following procedure −\n − Calculate the maximum $\\mathrm{\\left ( m-1 \\right )}$ and, minimum $\\mathrm{\\left ( \\left \\lceil \\frac{m}{2}\\right \\rceil-1 \\right )}$ number of keys a node can hold, where m is denoted by the order of the B Tree.\n − The data is inserted into the tree using the binary search insertion and once the keys reach the maximum number, the node is split into half and the median key becomes the internal node while the left and right keys become its children.\n − All the leaf nodes must be on the same level.\nThe keys, 5, 3, 21, 9, 13 are all added into the node according to the binary search property but if we add the key 22, it will violate the maximum key property. Hence, the node is split in half, the median key is shifted to the parent node and the insertion is then continued.\nAnother hiccup occurs during the insertion of 11, so the node is split and median is shifted to the parent.\nWhile inserting 16, even if the node is split in two parts, the parent node also overflows as it reached the maximum keys. Hence, the parent node is split first and the median key becomes the root. Then, the leaf node is split in half the median of leaf node is shifted to its parent.\nThe final B tree after inserting all the elements is achieved.\nFollowing are the implementations of this operation in various programming languages −\nThe deletion operation in a B tree is slightly different from the deletion operation of a Binary Search Tree. The procedure to delete a node from a B tree is as follows −\n − If the key to be deleted is in a leaf node and the deletion does not violate the minimum key property, just delete the node.\n − If the key to be deleted is in a leaf node but the deletion violates the minimum key property, borrow a key from either its left sibling or right sibling. In case if both siblings have exact minimum number of keys, merge the node in either of them.\n − If the key to be deleted is in an internal node, it is replaced by a key in either left child or right child based on which child has more keys. But if both child nodes have minimum number of keys, theyre merged together.\n − If the key to be deleted is in an internal node violating the minimum keys property, and both its children and sibling have minimum number of keys, merge the children. Then merge its sibling with its parent.\nFollowing are the implementations of this operation in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/red_black_trees.htm", "title": "Red Black Trees", "content": "Red-Black Trees are another type of the Balanced Binary Search Trees with two coloured nodes: Red and Black. It is a self-balancing binary search tree that makes use of these colours to maintain the balance factor during the insertion and deletion operations. Hence, during the Red-Black Tree operations, the memory uses 1 bit of storage to accommodate the colour information of each node\nIn Red-Black trees, also known as RB trees, there are different conditions to follow while assigning the colours to the nodes.\nThe root node is always black in colour.\nNo two adjacent nodes must be red in colour.\nEvery path in the tree (from the root node to the leaf node) must have the same amount of black coloured nodes.\nEven though AVL trees are more balanced than RB trees, with the balancing algorithm in AVL trees being stricter than that of RB trees, multiple and faster insertion and deletion operations are made more efficient through RB trees.\nThe operations on Red-Black Trees include all the basic operations usually performed on a Binary Search Tree. Some of the basic operations of an RB Tree include −\nInsertion\nDeletion\nSearch\nInsertion operation of a Red-Black tree follows the same insertion algorithm of a binary search tree. The elements are inserted following the binary search property and as an addition, the nodes are color coded as red and black to balance the tree according to the red-black tree properties.\nFollow the procedure given below to insert an element into a red-black tree by maintaining both binary search tree and red black tree properties.\n − Check whether the tree is empty; make the current node as the root and color the node black if it is empty.\n − But if the tree is not empty, we create a new node and color it red. Here we face two different cases −\nIf the parent of the new node is a black colored node, we exit the operation and tree is left as it is.\nIf the parent of this new node is red and the color of the parent's sibling is either black or if it does not exist, we apply a suitable rotation and recolor accordingly.\nIf the parent of this new node is red and color of the parent's sibling is red, recolor the parent, the sibling and grandparent nodes to black. The grandparent is recolored only if it is \n the root node; if it is the root node recolor only the parent and the sibling.\nLet us construct an RB Tree for the first 7 integer numbers to understand the insertion operation in detail −\nThe tree is checked to be empty so the first node added is a root and is colored black.\nNow, the tree is not empty so we create a new node and add the next integer with color red,\nThe nodes do not violate the binary search tree and RB tree properties, hence we move ahead to add another node.\nThe tree is not empty; we create a new red node with the next integer to it. But the parent of the new node is not a black colored node,\nThe tree right now violates both the binary search tree and RB tree properties; since parent's sibling is NULL, we apply a suitable rotation and recolor the nodes.\nNow that the RB Tree property is restored, we add another node to the tree −\nThe tree once again violates the RB Tree balance property, so we check for the parent's sibling node color, red in this case, so we just recolor the parent and the sibling.\nWe next insert the element 5, which makes the tree violate the RB Tree balance property once again.\nAnd since the sibling is NULL, we apply suitable rotation and recolor.\nNow, we insert element 6, but the RB Tree property is violated and one of the insertion cases need to be applied −\nThe parent's sibling is red, so we recolor the parent, parent's sibling and the grandparent nodes since the grandparent is not the root node.\nNow, we add the last element, 7, but the parent node of this new node is red.\nSince the parent's sibling is NULL, we apply suitable rotations (RR rotation)\nThe final RB Tree is achieved.\nThe deletion operation on red black tree must be performed in such a way that it must restore all the properties of a binary search tree and a red black tree. Follow the steps below to perform the deletion operation on the red black tree −\nFirstly, we perform deletion based on the binary search tree properties.\n − If either the node to be deleted or the node's parent is red, just delete it.\n − If the node is a double black, just remove the double black (double black occurs when the node to be deleted is a black colored leaf node, as it adds up the NULL nodes which are considered black colored nodes too)\n − If the double black's sibling node is also a black node and its child nodes are also black in color, follow the steps below −\nRemove double black\nRecolor its parent to black (if the parent is a red node, it becomes black; if the parent is already a black node, it becomes double black)\nRecolor the parent's sibling with red\nIf double black node still exists, we apply other cases.\n − If the double black node's sibling is red, we perform the following steps −\nSwap the colors of the parent node and the parent's sibling node.\nRotate parent node in the double black's direction\nReapply other cases that are suitable.\n − If the double black's sibling is a black node but the sibling's child node that is closest to the double black is red, follows the steps below −\nSwap the colors of double black's sibling and the sibling's child in question\nRotate the sibling node is the opposite direction of double black (i.e. if the double black is a right child apply left rotations and vice versa)\nApply case 6.\n − If the double black's sibling is a black node but the sibling's child node that is farther to the double black is red, follows the steps below −\nSwap the colors of double black's parent and sibling nodes\nRotate the parent in double black's direction (i.e. if the double black is a right child apply right rotations and vice versa)\nRemove double black\nChange the color of red child node to black.\nConsidering the same constructed Red-Black Tree above, let us delete few elements from the tree.\nDelete elements 4, 5, 3 from the tree.\nTo delete the element 4, let us perform the binary search deletion first.\nAfter performing the binary search deletion, the RB Tree property is not disturbed, therefore the tree is left as it is.\nThen, we delete the element 5 using the binary search deletion\nBut the RB property is violated after performing the binary search deletion, i.e., all the paths in the tree do not hold same number of black nodes; so we swap the colors to balance the tree.\nThen, we delete the node 3 from the tree obtained −\nApplying binary search deletion, we delete node 3 normally as it is a leaf node. And we get a double node as 3 is a black colored node.\nWe apply case 3 deletion as double black's sibling node is black and its child nodes are also black. Here, we remove the double black, recolor the double black's parent and sibling.\nAll the desired nodes are deleted and the RB Tree property is maintained.\nThe search operation in red-black tree follows the same algorithm as that of a binary search tree. The tree is traversed and each node is compared with the key element to be searched; if found it returns a successful search. Otherwise, it returns an unsuccessful search.\nFollowing are the complete implementations of Red Black Tree in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/avl_tree_algorithm.htm", "title": "AVL Trees", "content": "The first type of self-balancing binary search tree to be invented is the AVL tree. The name AVL tree is coined after its inventor's names − Adelson-Velsky and Landis.\nIn AVL trees, the difference between the heights of left and right subtrees, known as the \n, must be at most one. Once the difference exceeds one, the tree automatically executes the balancing algorithm until the difference becomes one again.\nThere are usually four cases of rotation in the balancing algorithm of AVL trees: LL, RR, LR, RL.\nLL rotation is performed when the node is inserted into the right subtree leading to an unbalanced tree. This is a single left rotation to make the tree balanced again −\nThe node where the unbalance occurs becomes the left child and the newly added node becomes the right child with the middle node as the parent node.\nRR rotation is performed when the node is inserted into the left subtree leading to an unbalanced tree. This is a single right rotation to make the tree balanced again −\nThe node where the unbalance occurs becomes the right child and the newly added node becomes the left child with the middle node as the parent node.\nLR rotation is the extended version of the previous single rotations, also called a double rotation. It is performed when a node is inserted into the right subtree of the left subtree. The LR rotation is a combination of the left rotation followed by the right rotation. There are multiple steps to be followed to carry this out.\nConsider an example with A as the root node, B as the left child of A and C as the right child of B.\nSince the unbalance occurs at A, a left rotation is applied on the child nodes of A, i.e. B and C.\nAfter the rotation, the C node becomes the left child of A and B becomes the left child of C.\nThe unbalance still persists, therefore a right rotation is applied at the root node A and the left child C.\nAfter the final right rotation, C becomes the root node, A becomes the right child and B is the left child.\nRL rotation is also the extended version of the previous single rotations, hence it is called a double rotation and it is performed if a node is inserted into the left subtree of the right subtree. The RL rotation is a combination of the right rotation followed by the left rotation. There are multiple steps to be followed to carry this out.\nConsider an example with A as the root node, B as the right child of A and C as the left child of B.\nSince the unbalance occurs at A, a right rotation is applied on the child nodes of A, i.e. B and C.\nAfter the rotation, the C node becomes the right child of A and B becomes the right child of C.\nThe unbalance still persists, therefore a left rotation is applied at the root node A and the right child C.\nAfter the final left rotation, C becomes the root node, A becomes the left child and B is the right child.\nThe basic operations performed on the AVL Tree structures include all the operations performed on a binary search tree, since the AVL Tree at its core is actually just a binary search tree holding all its properties. Therefore, basic operations performed on an AVL Tree are − \n and \n.\nThe data is inserted into the AVL Tree by following the Binary Search Tree property of insertion, i.e. the left subtree must contain elements less than the root value and right subtree must contain all the greater elements. \nHowever, in AVL Trees, after the insertion of each element, the balance factor of the tree is checked; if it does not exceed 1, the tree is left as it is. But if the balance factor exceeds 1, a balancing algorithm is applied to readjust the tree such that balance factor becomes less than or equal to 1 again.\nThe following steps are involved in performing the insertion operation of an AVL Tree −\nLet us understand the insertion operation by constructing an example AVL tree with 1 to 7 integers.\nStarting with the first element 1, we create a node and measure the balance, i.e., 0.\nSince both the binary search property and the balance factor are satisfied, we insert another element into the tree.\nThe balance factor for the two nodes are calculated and is found to be -1 (Height of left subtree is 0 and height of the right subtree is 1). Since it does not exceed 1, we add another element to the tree.\nNow, after adding the third element, the balance factor exceeds 1 and becomes 2. Therefore, rotations are applied. In this case, the RR rotation is applied since the imbalance occurs at two right nodes.\nThe tree is rearranged as −\nSimilarly, the next elements are inserted and rearranged using these rotations. After rearrangement, we achieve the tree as −\nFollowing are the implementations of this operation in various programming languages −\nDeletion in the AVL Trees take place in three different scenarios −\n − If the node to be deleted is a leaf node, then it is deleted without any replacement as it does not disturb the binary search tree property. However, the balance factor may get disturbed, so rotations are applied to restore it.\n − If the node to be deleted has one child, replace the value in that node with the value in its child node. Then delete the child node. If the balance factor is disturbed, rotations are applied.\n − If the node to be deleted has two child nodes, find the inorder successor of that node and replace its value with the inorder successor value. Then try to delete the inorder successor node. If the balance factor exceeds 1 after deletion, apply balance algorithms.\nUsing the same tree given above, let us perform deletion in three scenarios −\nDeleting element 7 from the tree above −\nSince the element 7 is a leaf, we normally remove the element without disturbing any other node in the tree\nDeleting element 6 from the output tree achieved −\nHowever, element 6 is not a leaf node and has one child node attached to it. In this case, we replace node 6 with its child node: node 5.\nThe balance of the tree becomes 1, and since it does not exceed 1 the tree is left as it is. If we delete the element 5 further, we would have to apply the left rotations; either LL or LR since the imbalance occurs at both 1-2-4 and 3-2-4.\nThe balance factor is disturbed after deleting the element 5, therefore we apply LL rotation (we can also apply the LR rotation here).\nOnce the LL rotation is applied on path 1-2-4, the node 3 remains as it was supposed to be the right child of node 2 (which is now occupied by node 4). Hence, the node is added to the right subtree of the node 2 and as the left child of the node 4.\nDeleting element 2 from the remaining tree −\nAs mentioned in scenario 3, this node has two children. Therefore, we find its inorder successor that is a leaf node (say, 3) and replace its value with the inorder successor.\nThe balance of the tree still remains 1, therefore we leave the tree as it is without performing any rotations.\nFollowing are the implementations of this operation in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/binary_search_tree.htm", "title": "Binary Search Tree", "content": "A Binary Search Tree (BST) is a tree in which all the nodes follow the below-mentioned properties −\nThe left sub-tree of a node has a key less than or equal to its parent node's key.\nThe right sub-tree of a node has a key greater than or equal to its parent node's key.\nThus, BST divides all its sub-trees into two segments; the left sub-tree and the right sub-tree and can be defined as −\nBST is a collection of nodes arranged in a way where they maintain BST properties. Each node has a key and an associated value. While searching, the desired key is compared to the keys in BST and if found, the associated value is retrieved.\nFollowing is a pictorial representation of BST −\nWe observe that the root node key (27) has all less-valued keys on the left sub-tree and the higher valued keys on the right sub-tree.\nFollowing are the basic operations of a Binary Search Tree −\n − Searches an element in a tree.\n − Inserts an element in a tree.\n − Traverses a tree in a pre-order manner.\n − Traverses a tree in an in-order manner.\n − Traverses a tree in a post-order manner.\nDefine a node that stores some data, and references to its left and right child nodes.\nWhenever an element is to be searched, start searching from the root node. Then if the data is less than the key value, search for the element in the left subtree. Otherwise, search for the element in the right subtree. Follow the same algorithm for each node.\nFollowing are the implementations of this operation in various programming languages −\nWhenever an element is to be inserted, first locate its proper location. Start searching from the root node, then if the data is less than the key value, search for the empty location in the left subtree and insert the data. Otherwise, search for the empty location in the right subtree and insert the data.\nFollowing are the implementations of this operation in various programming languages −\nThe inorder traversal operation in a Binary Search Tree visits all its nodes in the following order −\nFirstly, we traverse the left child of the root node/current node, if any.\nNext, traverse the current node.\nLastly, traverse the right child of the current node, if any.\nFollowing are the implementations of this operation in various programming languages −\nThe preorder traversal operation in a Binary Search Tree visits all its nodes. However, the root node in it is first printed, followed by its left subtree and then its right subtree.\nFollowing are the implementations of this operation in various programming languages −\nLike the other traversals, postorder traversal also visits all the nodes in a Binary Search Tree and displays them. However, the left subtree is printed first, followed by the right subtree and lastly, the root node.\nFollowing are the implementations of this operation in various programming languages −\nFollowing are the complete implementations of Binary Search Tree in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/tree_traversal.htm", "title": "Tree Traversal", "content": "Traversal is a process to visit all the nodes of a tree and may print their values too. Because, all nodes are connected via edges (links) we always start from the root (head) node. That is, we cannot randomly access a node in a tree. There are three ways which we use to traverse a tree −\nIn-order Traversal\nPre-order Traversal\nPost-order Traversal\nGenerally, we traverse a tree to search or locate a given item or key in the tree or to print all the values it contains.\nIn this traversal method, the left subtree is visited first, then the root and later the right sub-tree. We should always remember that every node may represent a subtree itself.\nIf a binary tree is traversed \n, the output will produce sorted key values in an ascending order.\nWe start from \n, and following in-order traversal, we move to its left subtree \n.\n is also traversed in-order. The process goes on until all the nodes are visited. The output of in-order traversal of this tree will be −\nUntil all nodes are traversed −\nFollowing are the implementations of this operation in various programming languages −\nIn this traversal method, the root node is visited first, then the left subtree and finally the right subtree.\nWe start from \n, and following pre-order traversal, we first visit \n itself and then move to its left subtree \n. \n is also traversed pre-order. The process goes on until all the nodes are visited. The output of pre-order traversal of this tree will be −\nUntil all nodes are traversed −\nFollowing are the implementations of this operation in various programming languages −\nIn this traversal method, the root node is visited last, hence the name. First we traverse the left subtree, then the right subtree and finally the root node.\nWe start from \n, and following pre-order traversal, we first visit the left subtree \n. \n is also traversed post-order. The process goes on until all the nodes are visited. The output of post-order traversal of this tree will be \nUntil all nodes are traversed −\nFollowing are the implementations of this operation in various programming languages −\nNow let's see the complete implementation of tree traversal in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/tree_data_structure.htm", "title": "Tree Data Structure", "content": "A tree is a non-linear abstract data type with a hierarchy-based structure. It consists of nodes (where the data is stored) that are connected via links. The tree data structure stems from a single node called a root node and has subtrees connected to the root.\nFollowing are the important terms with respect to tree.\n − Path refers to the sequence of nodes along the edges of a tree.\n − The node at the top of the tree is called root. There is only one root per tree and one path from the root node to any node.\n − Any node except the root node has one edge upward to a node called parent.\n − The node below a given node connected by its edge downward is called its child node.\n − The node which does not have any child node is called the leaf node.\n − Subtree represents the descendants of a node.\n − Visiting refers to checking the value of a node when control is on the node.\n − Traversing means passing through nodes in a specific order.\n − Level of a node represents the generation of a node. If the root node is at level 0, then its next child node is at level 1, its grandchild is at level 2, and so on.\n − Key represents a value of a node based on which a search operation is to be carried out for a node.\nThere are three types of trees −\nGeneral Trees\nBinary Trees\nBinary Search Trees\nGeneral trees are unordered tree data structures where the root node has minimum 0 or maximum n subtrees.\nThe General trees have no constraint placed on their hierarchy. The root node thus acts like the superset of all the other subtrees.\nBinary Trees are general trees in which the root node can only hold up to maximum 2 subtrees: left subtree and right subtree. Based on the number of children, binary trees are divided into three types.\nA full binary tree is a binary tree type where every node has either 0 or 2 child nodes.\nA complete binary tree is a binary tree type where all the leaf nodes must be on the same level. However, root and internal nodes in a complete binary tree can either have 0, 1 or 2 child nodes.\nA perfect binary tree is a binary tree type where all the leaf nodes are on the same level and every node except leaf nodes have 2 children.\nBinary Search Trees possess all the properties of Binary Trees including some extra properties of their own, based on some constraints, making them more efficient than binary trees.\nThe data in the Binary Search Trees (BST) is always stored in such a way that the values in the left subtree are always less than the values in the root node and the values in the right subtree are always greater than the values in the root node, i.e. left subtree < root node  right subtree.\nBinary Search Trees are more efficient than Binary Trees since time complexity for performing various operations reduces.\nSince the order of keys is based on just the parent node, searching operation becomes simpler.\nThe alignment of BST also favors Range Queries, which are executed to find values existing between two keys. This helps in the Database Management System.\nThe main disadvantage of Binary Search Trees is that if all elements in nodes are either greater than or lesser than the root node,\n. Simply put, the tree becomes slanted to one side completely.\nThis \n will make the tree a linked list rather than a BST, since the worst case time complexity for searching operation becomes O(n).\nTo overcome this issue of skewness in the Binary Search Trees, the concept of \n was introduced.\nConsider a Binary Search Tree with m as the height of the left subtree and n as the height of the right subtree. If the value of (m-n) is equal to 0,1 or -1, the tree is said to be a \n.\nThe trees are designed in a way that they self-balance once the height difference exceeds 1. Binary Search Trees use rotations as self-balancing algorithms. There are four different types of rotations: Left Left, Right Right, Left Right, Right Left.\nThere are various types of self-balancing binary search trees −\nPriority Search Trees\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/maxflow_mincut_theorem.htm", "title": "Maxflow Mincut Theorem", "content": "Let's talk about \n and \n. These are like two sides of the same coin when dealing with network flow problems. If you ever wondered how stuff moves through a network, whether its water in pipes, cars on roads, or data in a network, then youre already thinking in terms of max flow and min cut.\n (Maximum Flow) is all about figuring out the most stuff (data, water, traffic) that can move from one point (source) to another (sink) in a network without breaking any capacity limits. Imagine a bunch of pipes carrying water, and you want to know the maximum amount of water that can flow through the system without overflowing.\n (Minimum Cut) is kind of the opposite. Its about finding the weakest link in the network that, if cut, would completely stop the flow from the source to the sink. Think of it like identifying the narrowest point in a road system where a traffic jam would completely block all movement.\nHeres something cool: The \n says that the maximum flow in a network is equal to the total capacity of the smallest set of edges that, if removed, would stop the flow completely. This means if you\n\nThese concepts arent just theoretical; they are used in real life all the time:\nImagine a city with roads connecting different areas. If you want to get the max number of cars from the start point to the endpoint, thats maxflow. Now, let's write code.\nSteps to find the maxflow in a graph using the Ford-Fulkerson Algorithm:\nHere's the code to find the maxflow in a graph using the Ford-Fulkerson Algorithm:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nWe already know what mincut is, let's code to find the mincut in a graph using the Ford-Fulkerson Algorithm:\nSteps to find the mincut in a graph using the Ford-Fulkerson Algorithm:\nNow, let's see how to find the mincut in a graph using the Ford-Fulkerson Algorithm:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nMaxflow helps us figure out how much we can push through a system, while Mincut tells us where the weakest spots are. Both are super useful in real-life applications, from networks to transportation and beyond. Learning these concepts can help solve big optimization problems in different fields.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/edmonds_blossom_algorithm.htm", "title": "Edmond's Blossom Algorithm", "content": "The \n is used to find the \n in general graphs. It extends the \n, which is designed for bipartite graphs. The Edmonds Blossom Algorithm works with any graph, making it a versatile tool for solving matching problems.\nThe \n is a smart way to find the largest possible matchings in general graphs. It's an extension of the \n, which was originally created to work with bipartite graphs.\nBy building on the \n, the \n can also work with more complicated graphs, making it more versatile and efficient at finding maximum matchings with fewer steps.\nConsider a simple graph with vertices and edges. Our goal is to find a maximum matching using the Edmonds Blossom Algorithm. The graph is shown below:\nA \n is a set of edges where no two edges share a common vertex. A \n contains the maximum number of edges possible.\nThe Edmonds Blossom Algorithm finds augmenting paths in the graph to increase the matching size. Here are the steps:\nFollowing these steps, the Edmonds Blossom Algorithm efficiently finds a maximum matching in a general graph.\nHere's an example code snippet for the Edmonds Blossom Algorithm in C, C++, Java, and Python:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nThe Edmonds Blossom Algorithm is used in various applications, including:\nIn this chapter, we discussed the \n, why it's useful and steps to implement it. We have also provided example code snippets in \n, \n, \n, and \n to demonstrate how the \n works. And we explored application of the \n.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/flow_networks.htm", "title": "Flow Networks", "content": "are a special kind of graph that are used for representing the flow of data or resources from one point to another. They are used in a variety of applications, including transportation networks, communication networks, and computer networks. In this chapter, we will discuss the basic concepts of \n and how they are used in data structures.\nA \n is a directed graph in which each edge has a capacity and a flow. The capacity of an edge represents the maximum amount of flow that can pass through it, while the flow of an edge represents the actual amount of flow passing through it. The flow of an edge must be less than or equal to its capacity.\n are used to model the flow of data or resources from a source to a sink. The source is the starting point of the flow, while the sink is the ending point. The goal of a \n is to maximize the flow from the source to the sink while respecting the capacities of the edges.\nLet's consider a simple flow network with a source, a sink, and some intermediate nodes. The edges of the flow network have capacities and flows as shown below:\nThe flow network represents the flow of data from the source A to the sink F. The goal is to maximize the flow from A to F while respecting the capacities of the edges. In this example, the maximum flow from A to F is 10, which is achieved by sending 5 units of flow along edge AB, 3 units along edge AC, 6 units along edge BD, 4 units along edge CE, and 4 units along edge EF.\nThere are several algorithms that can be used to find the maximum flow in a flow network. Some of the most common algorithms include:\nThese algorithms use different techniques to find the maximum flow in a flow network. They are based on the concept of augmenting paths, which are paths from the source to the sink that can carry additional flow. By finding augmenting paths and increasing the flow along them.\nLet's take a look at the Ford-Fulkerson algorithm, which is one of the most popular algorithms for finding the maximum flow in a flow network. The algorithm works by finding augmenting paths from the source to the sink and increasing the flow along these paths.\nThe steps of the Ford-Fulkerson algorithm are as follows:\nBy following these steps, the Ford-Fulkerson algorithm can find the maximum flow in a flow network. The algorithm terminates when there are no more augmenting paths from the source to the sink.\nHere's an example code snippet for the Ford-Fulkerson algorithm in C, C++, Java, and Python:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\n are used in a variety of applications, including:\nBy using \n, engineers and researchers can analyze and optimize the flow of data or resources in a wide range of applications. \n are a powerful tool for modeling complex systems and finding efficient solutions to flow-related problems.\nIn this chapter, we discussed the concept of \n and how they are used in data structures. We also explored the Ford-Fulkerson algorithm, which is one of the most popular algorithms for finding the maximum flow in a flow network. By understanding \n and the algorithms used to analyze them, you can solve a wide range of flow-related problems in various domains.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/network_flow_problems.htm", "title": "Network Flow Problems", "content": "are all about figuring out how stuff (data, resources, whatever) can move from a start point (source) to an endpoint (sink) in a network. Its like optimizing a traffic system, a data network, or even supply chains. In this article, well talk about what network flow problems are and how we handle them using data structures.\n deal with this idea of moving flow from one point to another. We have a network of nodes connected by edges, and each edge has a capacity. The capacity shows how much can actually flow through that edge, and the flow is the amount thats actually moving. The rule is that the flow on an edge can't be more than its capacity.\nThe whole point of these problems is to move as much \"stuff\" (data, goods, etc.) as we can from the source to the sink. But we have to respect the limits (capacities) of each edge along the way. The goal is to maximize the flow without breaking any edges.\nImagine we have a simple network with a source node and a sink node. The nodes are connected by edges with certain capacities. We apply the right algorithm to find out how much flow can go from source to sink while respecting the edge capacities. We want to push as much flow through the network as possible.\nTo solve these problems, we use a few different algorithms. They help us figure out the max flow in the network. Heres a quick rundown:\nThey all work by finding these paths called augmenting pathspaths where we can add more flow. Its like a race to find the best path to push more stuff through until theres no room left to move more.\nYou are given a directed graph representing a network with capacities on edges. The graph has a source node (s) and a sink node (t). The goal is to determine the maximum flow that can be sent from source to sink while respecting the capacity constraints.\nIn the above graph, the numbers on the edges represent the capacities. The maximum flow from source (0) to sink (5) is 20.\nHere's code for above problem in C, C++, Java, and Python:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nThese problems arent just theoretical. They show up everywhere! Here are some areas where we use them:\nBy solving network flow problems, engineers and researchers can tweak and improve systems where resources flow from one place to another. Whether its traffic, data, or anything in between, these problems help optimize flow in the best way possible.\nIn this tutorial, we talked about network flow problems and how they help us figure out the best way to move stuff from one place to another. We use algorithms to find the maximum flow in a network, respecting the capacities of each edge. These problems show up in all sorts of systems, from transportation to communication to supply chains. By solving network flow problems, we can optimize how resources move around in these systems, making them more efficient and effective.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/augmenting_path_algorithm.htm", "title": "What Is an Augmenting Path?", "content": "An \n in a graph is just a way to get from the \n to the \n where we can push more flow. In a flow network, the goal is to find these paths and increase the flow along them, ultimately making the total flow as big as possible.\nThese paths are essential in finding the \n from the source to the sink in flow networks.\nLets break it down with an example. Imagine we have a simple flow network with a source, a sink, and a few intermediate nodes. Each edge between nodes has a \n (how much it can carry) and \n (how much is currently flowing). Here's how it looks:\nIn this network, node \n is the \n and node \n is the \n. The capacities and flows of the edges are as follows:\nThis \n shows how \n from \n to \n. The key here is to push as much \n as possible while respecting the \n of the edges. In this case, the \n from \n to \n is \n, which is achieved by sending \n of flow through edge \n, \n through edge \n, and \n through edge \n. But theres still room for improvement by finding \n to push more flow through!\nNow, lets talk about some algorithms that help us find these augmenting paths. There are a few options, and they work in different ways to maximize the flow:\nNow that we know what \n are and how they help us increase the \n in a \n, lets see how we can implement them in code. Heres a simple example of the \n -\nHeres a simple code snippet that uses the Ford-Fulkerson algorithm to find the maximum flow in a flow network:\nFollowing is the output of the above code:\nEach of these algorithms helps us find the best paths to push more \n through the \n and, eventually, determine the \n possible.\nAugmenting paths are super useful in many real-world situations, especially when we need to manage the \n of things like \n, \n, or \n. Here are some areas where these concepts come in handy:\nBy using augmenting paths, engineers can better understand how things flow through these systems and find ways to improve them. Whether its \n, \n, \n, or \n, these techniques help us optimize how \n move and find the most efficient solutions!\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/biconnected_components.htm", "title": "Bi-connected Components", "content": "Imagine you have a big network of cities \n connected by roads \n. Now, if you remove just one city, and the whole network breaks into separate parts, then that city is called an \n.\nSo, \n(BCC) are the largest possible parts of a graph(\n) where no single node removal can disconnect them.\nFor example, consider the following graph:\nThe articulation points in the graph are:\nThe Bi-connected Components (BCCs) in the graph are:\nHere are some key properties of bi-connected components:\nWe use \n to find biconnected components.\nHere's a simple algorithm to find biconnected components in a graph:\nLet's say we have this graph\nThe edges are:\nIf we remove node \n, the graph look like this:\nA, B, C, and D form one part.\nF and G are now completely disconnected from the rest of the graph.\nSo E is an articulation point.\nBiconnected components are −\nLet's see the code for finding biconnected components in a above graph:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nBiconnected components are used in various applications, such as:\nWe have learned about biconnected components in a graph. We have seen how to find biconnected components using a simple algorithm. We have also seen the code in C, C++, Java, and Python. Biconnected components are essential in network design, fault tolerance, and distributed systems.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/strongly_connected_components.htm", "title": "Strongly Connected Components", "content": "are specific sub-graphs in a directed graph where every node can reach every other node within that sub-graph. In simple terms, SCCs are clusters of nodes that are tightly connected but are separate from other parts of the graph. Understanding SCCs is important, as they help in detecting cycles and studying network structures.\nA directed graph is said to be strongly connected if every node can reach every other node directly or indirectly. However, in larger graphs, we may have multiple strongly connected parts that are not connected to each other. These independent sections are called strongly connected components.\nFor example, consider a directed graph with nodes A, B, C, D, and E. If A can reach B and B can reach A, but C, D, and E form a separate cluster where they can all reach each other but not A or B, then there are two SCCs in the graph: {A, B} and {C, D, E}.\nThere are two popular algorithms to find strongly connected components in a graph:\nKosaraju's Algorithm is a two-pass algorithm that finds strongly connected components in a directed graph. Here's how it works:\nLet's consider a directed graph with nodes A, B, C, D, and E, and the following edges:\nUsing Kosaraju's Algorithm, we can find the strongly connected components in the graph:\nHere's an example code snippet for Kosaraju's Algorithm in C, C++, Java, and Python:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nTarjan's Algorithm is another popular algorithm to find strongly connected components in a directed graph. It uses Depth First Search (DFS) to traverse the graph and identify SCCs. Here's how it works:\nConsider the same directed graph with nodes A, B, C, D, and E, and the following edges:\nUsing Tarjan's Algorithm, we can find the strongly connected components in the graph:\nHere's an example code snippet for Tarjan's Algorithm in C, C++, Java, and Python:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing is the output of the above code:\nFollowing are some common applications of Strongly Connected Components:\nUnderstanding Strongly Connected Components is essential for analyzing the connectivity and structure of directed graphs. Algorithms like Kosaraju's Algorithm and Tarjan's Algorithm provide efficient ways to identify SCCs in graphs, enabling various applications in network analysis, graph algorithms, and software engineering.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/topological_sorting.htm", "title": "Topological Sorting in Graph Data Structure", "content": "is a way of arranging the nodes of a \n in a line, making sure that for every directed edge from \n to \n, node \n comes before \n. If the graph has cycles, \n.\nIn \n, if theres an edge from \n to \n, \n should come before \n in the order. Its different from \n, where we visit a node and then explore its neighbors. In \n, a node should be printed before any of its connected nodes.\nNow, lets understand \n. A \n has edges that go from one node to another in a specific direction. It is called \n if there are \nmeaning theres no way to start from a node and return to it by following edges.\n only works for \n. If theres a \n in the graph, we cant arrange the nodes in a proper order because some nodes will keep pointing back, making a valid linear order \n.\n can be done using \n algorithm. The algorithm follows the following steps:\nFollowing are the steps to perform \n using \n:\nAfter performing the above steps for all the vertices, the \n list will have the vertices in \n.\nLet's see the code for topological sorting using Depth First Search (DFS) algorithm:\nFollowing is the output of the above code:\nWhen using \n for \n, the time complexity is \n, where \n is the number of vertices and \n is the number of edges in the graph. This is because we visit each node once and explore all its edges.\n has many applications in various fields. Some of the applications of \n are:\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/spanning_tree.htm", "title": "Spanning Tree", "content": "A spanning tree is a subset of Graph G, which has all the vertices covered with minimum possible number of edges. Hence, a spanning tree does not have cycles and it cannot be disconnected..\nBy this definition, we can draw a conclusion that every connected and undirected Graph G has at least one spanning tree. A disconnected graph does not have any spanning tree, as it cannot be spanned to all its vertices.\nWe found three spanning trees off one complete graph. A complete undirected graph can have maximum \n number of spanning trees, where \n is the number of nodes. In the above addressed example, \n hence \n spanning trees are possible.\nWe now understand that one graph can have more than one spanning tree. Following are a few properties of the spanning tree connected to graph G −\nA connected graph G can have more than one spanning tree.\nAll possible spanning trees of graph G, have the same number of edges and vertices.\nThe spanning tree does not have any cycle (loops).\nRemoving one edge from the spanning tree will make the graph disconnected, i.e. the spanning tree is \n.\nAdding one edge to the spanning tree will create a circuit or loop, i.e. the spanning tree is \n.\nSpanning tree has \n edges, where \n is the number of nodes (vertices).\nFrom a complete graph, by removing maximum \n edges, we can construct a spanning tree.\nA complete graph can have maximum \n number of spanning trees.\nThus, we can conclude that spanning trees are a subset of connected Graph G and disconnected graphs do not have spanning tree.\nSpanning tree is basically used to find a minimum path to connect all nodes in a graph. Common application of spanning trees are −\nLet us understand this through a small example. Consider, city network as a huge graph and now plans to deploy telephone lines in such a way that in minimum lines we can connect to all city nodes. This is where the spanning tree comes into picture.\nIn a weighted graph, a minimum spanning tree is a spanning tree that has minimum weight than all other spanning trees of the same graph. In real-world situations, this weight can be measured as distance, congestion, traffic load or any arbitrary value denoted to the edges.\nWe shall learn about two most important spanning tree algorithms here −\nThese two algorithms are Greedy algorithms.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/breadth_first_traversal.htm", "title": "Breadth First Search (BFS) Algorithm", "content": "Breadth First Search (BFS) algorithm traverses a graph in a breadthward motion to search a graph data structure for a node that meets a set of criteria. It uses a queue to remember the next vertex to start a search, when a dead end occurs in any iteration.\nBreadth First Search (BFS) algorithm starts at the tree root and explores all nodes at the present depth prior to moving on to the nodes at the next depth level.\nAs in the example given above, BFS algorithm traverses from A to B to E to F first then to C and G lastly to D. It employs the following rules.\n − Visit the adjacent unvisited vertex. Mark it as visited. Display it. Insert it in a queue.\n − If no adjacent vertex is found, remove the first vertex from the queue.\n − Repeat Rule 1 and Rule 2 until the queue is empty.\nAt this stage, we are left with no unmarked (unvisited) nodes. But as per the algorithm we keep on dequeuing in order to get all unvisited nodes. When the queue gets emptied, the program is over.\nFollowing are the implementations of Breadth First Search (BFS) Algorithm in various programming languages −\nClick to check C implementation of \nThe time complexity of the BFS algorithm is represented in the form of O(V + E), where V is the number of nodes and E is the number of edges.\nThe space complexity of the BFS algorithm is O(V).\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/depth_first_traversal.htm", "title": "Depth First Search (DFS) Algorithm", "content": "Depth First Search (DFS) algorithm is a recursive algorithm for searching all the vertices of a graph or tree data structure. This algorithm traverses a graph in a depthward motion and uses a stack to remember to get the next vertex to start a search, when a dead end occurs in any iteration.\nAs in the example given above, DFS algorithm traverses from S to A to D to G to E to B first, then to F and lastly to C. It employs the following rules.\n − Visit the adjacent unvisited vertex. Mark it as visited. Display it. Push it in a stack.\n − If no adjacent vertex is found, pop up a vertex from the stack. (It will pop up all the vertices from the stack, which do not have adjacent vertices.)\n − Repeat Rule 1 and Rule 2 until the stack is empty.\nAs \n does not have any unvisited adjacent node so we keep popping the stack until we find a node that has an unvisited adjacent node. In this case, there's none and we keep popping until the stack is empty.\nFollowing are the implementations of Depth First Search (DFS) Algorithm in various programming languages −\nClick to check C implementation of \nThe time complexity of the DFS algorithm is represented in the form of O(V + E), where V is the number of nodes and E is the number of edges.\nThe space complexity of the DFS algorithm is O(V).\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/graph_data_structure.htm", "title": "Graph Data Structure", "content": "A graph is an abstract data type (ADT)  which consists of a set of objects that are connected to each other via links. The interconnected objects are represented by points termed as \n, and the links that connect the vertices are called \n.\nFormally, a graph is a pair of sets \n, where \n is the set of vertices and \n is the set of edges, connecting the pairs of vertices. Take a look at the following graph −\nIn the above graph,\nV = {a, b, c, d, e}\nE = {ab, ac, bd, cd, de}\nMathematical graphs can be represented in data structure. We can represent a graph using an array of vertices and a two-dimensional array of edges. Before we proceed further, let's familiarize ourselves with some important terms −\n − Each node of the graph is represented as a vertex. In the following example, the labeled circle represents vertices. Thus, A to G are vertices. We can represent them using an array as shown in the following image. Here A can be identified by index 0. B can be identified using index 1 and so on.\n − Edge represents a path between two vertices or a line between two vertices. In the following example, the lines from A to B, B to C, and so on represents edges. We can use a two-dimensional array to represent an array as shown in the following image. Here AB can be represented as 1 at row 0, column 1, BC as 1 at row 1, column 2 and so on, keeping other combinations as 0.\n − Two node or vertices are adjacent if they are connected to each other through an edge. In the following example, B is adjacent to A, C is adjacent to B, and so on.\n − Path represents a sequence of edges between the two vertices. In the following example, ABCD represents a path from A to D.\nThe primary operations of a graph include creating a graph with vertices and edges, and displaying the said graph. However, one of the most common and popular operation performed using graphs are Traversal, i.e. visiting every vertex of the graph in a specific order.\nThere are two types of traversals in Graphs −\nDepth First Search is a traversal algorithm that visits all the vertices of a graph in the decreasing order of its depth. In this algorithm, an arbitrary node is chosen as the starting point and the graph is traversed back and forth by marking unvisited adjacent nodes until all the vertices are marked.\nThe DFS traversal uses the stack data structure to keep track of the unvisited nodes.\nClick and check \nBreadth First Search is a traversal algorithm that visits all the vertices of a graph present at one level of the depth before moving to the next level of depth. In this algorithm, an arbitrary node is chosen as the starting point and the graph is traversed by visiting the adjacent vertices on the same depth level and marking them until there is no vertex left.\nThe DFS traversal uses the queue data structure to keep track of the unvisited nodes.\nClick and check \nWhile representing graphs, we must carefully depict the elements (vertices and edges) present in the graph and the relationship between them. Pictorially, a graph is represented with a finite set of nodes and connecting links between them. However, we can also represent the graph in other most commonly used ways, like −\nAdjacency Matrix\nAdjacency List\nThe Adjacency Matrix is a V x V matrix where the values are filled with either 0 or 1. If the link exists between Vi and Vj, it is recorded 1; otherwise, 0.\nFor the given graph below, let us construct an adjacency matrix −\nThe adjacency matrix is −\nThe adjacency list is a list of the vertices directly connected to the other vertices in the graph.\nThe adjacency list is −\nThere are two basic types of graph −\nDirected Graph\nUndirected Graph\nDirected graph, as the name suggests, consists of edges that possess a direction that goes either away from a vertex or towards the vertex. Undirected graphs have edges that are not directed at all.\nA \n is a subset of an undirected graph that contains all the vertices of the graph connected with the minimum number of edges in the graph. Precisely, the edges of the spanning tree is a subset of the edges in the original graph.\nIf all the vertices are connected in a graph, then there exists at least one spanning tree. In a graph, there may exist more than one spanning tree.\nA spanning tree does not have any cycle.\nAny vertex can be reached from any other vertex.\nIn the following graph, the highlighted edges form a spanning tree.\nA \n is a subset of edges of a connected weighted undirected graph that connects all the vertices together with the minimum possible total edge weight. To derive an MST, Prim's algorithm or Kruskal's algorithm can be used. Hence, we will discuss Prim's algorithm in this chapter.\nAs we have discussed, one graph may have more than one spanning tree. If there are n number of vertices, the spanning tree should have n − l1 number of edges. In this context, if each edge of the graph is associated with a weight and there exists more than one spanning tree, we need to find the minimum spanning tree of the graph.\nMoreover, if there exist any duplicate weighted edges, the graph may have multiple minimum spanning tree.\nIn the above graph, we have shown a spanning tree though it's not the minimum spanning tree. The cost of this spanning tree is \n.\nThe shortest path in a graph is defined as the minimum cost route from one vertex to another. This is most commonly seen in weighted directed graphs but are also applicable to undirected graphs.\nA popular real−world application of finding the shortest path in a graph is a map. Navigation is made easier and simpler with the various shortest path algorithms where destinations are considered vertices of the graph and routes are the edges. The two common shortest path algorithms are −\nDijkstra's Shortest Path Algorithm\nBellman Ford's Shortest Path Algorithm\nFollowing are the implementations of this operation in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/lu_decomposition_in_matrices.htm", "title": "LU Decomposition in Matrices", "content": "is a method of decomposing a square matrix into two matrices, one of which is a lower triangular matrix and the other is an upper triangular matrix.\n is used in numerical analysis to solve systems of linear equations.\n is also used in the solution of linear systems of equations, the calculation of the determinant of a matrix, and the calculation of the inverse of a matrix.\nIn data structures, LU decomposition means splitting a square matrix A into two smaller matrices:\n - Lower triangular matrix\n - Upper triangular matrix\nThis decomposition helps in efficient computation of the determinant and inverse of a matrix.\n the process of solving linear equations and calculating the determinant and inverse of a matrix by breaking down the original matrix into two simpler matrices.\nImagine you have a project where you have to build a feature that require solving linear equations and you don't want it to be complex. In this case, you can use \n to simplify the process. It means you can solve the same equations with less effort and complexity by breaking down the matrix into two simpler matrices.\nThe LU decomposition algorithm is as follows:\nLet's consider a 3x3 matrix A. We will perform LU decomposition on this matrix. Following is the example, using Java, CPP, C and Python programming languages.\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output obtained is as shown below −\nFollowing is the output of the above code −\nLU decomposition is used in various applications, such as:\nLU decomposition is also used in numerical analysis, scientific computing, and engineering to solve complex problems.\nIn this chapter, we learned about LU decomposition in matrices. We discussed the importance of LU decomposition in data structures and its applications in solving linear equations, calculating the determinant and inverse of a matrix. We also saw the LU decomposition algorithm and an example of LU decomposition in a 3x3 matrix using different programming languages.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/lup_decomposition_in_matrices.htm", "title": "LUP Decomposition in Matrices", "content": "is a similar method as LU Decomposition, but the difference is that LUP decomposition includes a permutation matrix.\nThe LUP decomposition is used in numerical analysis to solve systems of linear equations, and also in the solution of linear systems of equations, the calculation of the determinant of a matrix, and the calculation of the inverse of a matrix.\nIn data structures, LUP decomposition means splitting a square matrix A into three smaller matrices:\n A square matrix in which all the elements above the main diagonal are zero.\n A square matrix in which all the elements below the main diagonal are zero.\n A square matrix that represents the permutation of the rows of the identity matrix.\nLUP decomposition is preferred over LU decomposition because it includes a permutation matrix P. The permutation matrix helps in reducing the error in the calculation of the determinant and inverse of a matrix.\nThe LUP decomposition algorithm is as follows:\nLet's consider a 3x3 matrix A. We will perform LUP decomposition on this matrix. Following is the example, using Java, CPP, C and Python programming languages.\nThe output produced is as follows −\nThe output produced is as follows −\nThe output obtained is as shown below −\nFollowing is the output of the above code −\nLUP decomposition is used in various applications in computer science, such as:\nIn this chapter, we learned about LUP decomposition in matrices. LUP decomposition is a method used to solve systems of linear equations, calculate the determinant of a matrix, and calculate the inverse of a matrix.\nLUP decomposition includes three matrices: L (lower triangular matrix), U (upper triangular matrix), and P (permutation matrix). LUP decomposition is preferred over LU decomposition because it includes a permutation matrix that helps in reducing errors in the calculation of the determinant and inverse of a matrix.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/matrices_in_data_structures.htm", "title": "Matrices in Data Structures", "content": "A \n is a two-dimensional data structure, where we can store data in \n and \n format. In data structures, a \n is a two-dimensional array of elements, with the same data type. All values in a \n must have the same data type. The \n can have a fixed number of rows and columns, or it can be dynamically allocated based on the requirement.\nMatrices can be used in various applications, such as image processing, data analysis, and machine learning. In this chapter, we will learn about matrices in data structures, how to create and access elements in a \n, and various operations that can be performed on matrices.\nIf the \n has \n rows and \n columns, there are \n elements. The \n is represented by uppercase letters (in this case, \"\n\"), and the elements in the \n are represented by lowercase letters and two subscripts that represent the positions of the elements in the same order. In the case of, '\n', where \n is the number of rows and \n is the number of columns.\nThere are many types of matrices, which are basically categorized based on element values, order, number of rows and columns, and so on. There are different types of matrices in linear algebra. All types of matrices are distinguished based on their elements, order, and specific conditions.\nSpecial types of matrices are square, diagonal, identity, translocations, and symmetric matrices. This is a square \n with the same number of rows and columns. Currently, using different terms, different \n types are categorized below, along with their definitions and examples.\nTherefore, if a\n = 0 for all i and j, then the A = [aij] m  n is a zero \n.\nThere are various opereations that can be performed on matrices. Some of the common operations are:\n - Traversing a \n means visiting each element in the \n exactly once. This can be done using nested loops to iterate over each row and column in the \n.\n - Searching for an element in a \n involves finding the position of a specific element in the \n. This can be done by traversing the \n and comparing each element with the target element.\n - Traversing a \n row-wise means visiting each element in each row of the \n. This can be done by iterating over each row and then iterating over each column in the row.\n - Traversing a \n column-wise means visiting each element in each column of the \n. This can be done by iterating over each column and then iterating over each row in the column.\n - Accessing an element in a \n means retrieving the value stored at a specific position in the \n. This can be done by specifying the row and column index of the element to be accessed.\nFollowing code demonstrates how to traverse a matrix using nested loops:\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output obtained is as shown below −\nFollowing is the output of the above code −\nFollowing code demonstrates how to search an element in a matrix:\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output obtained is as shown below −\nFollowing is the output of the above code −\nFollowing code demonstrates how to traverse a matrix row-wise:\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output obtained is as shown below −\nFollowing is the output of the above code −\nFollowing code demonstrates how to traverse a matrix column-wise:\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output obtained is as shown below −\nFollowing is the output of the above code −\nFollowing code demonstrates how to access elements of a matrix:\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output obtained is as shown below −\nFollowing is the output of the above code −\nFollowing are some of the applications of matrices:\nMatrices are used for images. let's say we have an image of 100x100 pixels, then the image can be represented as a 100x100 matrix where each element represents the color of a pixel.\nMatrices can be used to perform operations such as scaling, rotation, and filtering on images.\nIt is also used for data analysis. Imagine we have a data set with multiple features, then the data set can be represented as a matrix where each row represents a data point and each column represents a feature.\nMatrices can be used to perform operations such as normalization, standardization, and dimensionality reduction on data sets.\nIn machine learning, matrices are used for representing data sets and models. input data, output data, and parameters of a model.\nMatrices can be used for performing operations such as forward propagation, backward propagation, and optimization in machine learning algorithms.\nMatrices are used in computer graphics for representing objects in 3D space, to store the position, rotation, and scale of objects, and perform operations such as translation, rotation, and scaling.\nThey are also used in scientific computing to represent physical systems and perform simulations.\nMatrices can be used to store the state of a physical system, and perform operations such as integration, differentiation, and solving differential equations.\nIn this chapter, we learned about matrices in data structures, different types of matrices, components of a matrix, operations that can be performed on matrices, and applications of matrices in various fields. Matrices are a fundamental data structure used in various applications such as image processing, data analysis, machine learning, computer graphics, and scientific computing.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/quick_sort_algorithm.htm", "title": "Quick Sort Algorithm", "content": "Quick sort is a highly efficient sorting algorithm and is based on partitioning of array of data into smaller arrays. A large array is partitioned into two arrays one of which holds values smaller than the specified value, say pivot, based on which the partition is made and another array holds values greater than the pivot value.\nQuicksort partitions an array and then calls itself recursively twice to sort the two resulting subarrays. This algorithm is quite efficient for large-sized data sets as its average and worst-case complexity are O(n2), respectively.\nFollowing animated representation explains how to find the pivot value in an array.\nThe pivot value divides the list into two parts. And recursively, we find the pivot for each sub-lists until all lists contains only one element.\nBased on our understanding of partitioning in quick sort, we will now try to write an algorithm for it, which is as follows.\nThe pseudocode for the above algorithm can be derived as −\nUsing pivot algorithm recursively, we end up with smaller possible partitions. Each partition is then processed for quick sort. We define recursive algorithm for quicksort as follows −\nTo get more into it, let see the pseudocode for quick sort algorithm −\nThe worst case complexity of Quick-Sort algorithm is \n. However, using this technique, in average cases generally we get the output in \n time.\nFollowing are the implementations of Quick Sort algorithm in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/radix_sort_algorithm.htm", "title": "Radix Sort Algorithm", "content": "Radix sort is a step-wise sorting algorithm that starts the sorting from the least significant digit of the input elements. Like Counting Sort and Bucket Sort, Radix sort also assumes something about the input elements, that they are all k-digit numbers.\nThe sorting starts with the least significant digit of each element. These least significant digits are all considered individual elements and sorted first; followed by the second least significant digits. This process is continued until all the digits of the input elements are sorted.\n − If the elements do not have same number of digits, find the maximum number of digits in an input element and add leading zeroes to the elements having less digits. It does not change the values of the elements but still makes them k-digit numbers.\nThe radix sort algorithm makes use of the counting sort algorithm while sorting in every phase. The detailed steps are as follows −\n− Check whether all the input elements have same number of digits. If not, check for numbers that have maximum number of digits in the list and add leading zeroes to the ones that do not.\n− Take the least significant digit of each element.\n− Sort these digits using counting sort logic and change the order of elements based on the output achieved. For example, if the input elements are decimal numbers, the possible values each digit can take would be 0-9, so index the digits based on these values.\n− Repeat the Step 2 for the next least significant digits until all the digits in the elements are sorted.\n− The final list of elements achieved after kth loop is the sorted output.\nThe \n algorithm called would be −\nGiven that there are k-digits in the input elements, the running time taken by the radix sort algorithm would be \n. Here, n is the number of elements in the input list while b is the number of possible values each digit of a number can take.\nFor the given unsorted list of elements, 236, 143, 26, 42, 1, 99, 765, 482, 3, 56, we need to perform the radix sort and obtain the sorted output list −\nCheck for elements with maximum number of digits, which is 3. So we add leading zeroes to the numbers that do not have 3 digits. The list we achieved would be −\nConstruct a table to store the values based on their indexing. Since the inputs given are decimal numbers, the indexing is done based on the possible values of these digits, i.e., 0-9.\nBased on the least significant digit of all the numbers, place the numbers on their respective indices.\nThe elements sorted after this step would be 001, 042, 482, 143, 003, 765, 236, 026, 056, 099.\nThe order of input for this step would be the order of the output in the previous step. Now, we perform sorting using the second least significant digit.\nThe order of the output achieved is 001, 003, 026, 236, 042, 143, 056, 765, 482, 099.\nThe input list after the previous step is rearranged as −\nNow, we need to sort the last digits of the input elements.\nSince there are no further digits in the input elements, the output achieved in this step is considered as the final output.\nThe final sorted output is −\nThe counting sort algorithm assists the radix sort to perform sorting on multiple d-digit numbers iteratively for d loops. Radix sort is implemented in four programming languages in this tutorial: C, C++, Java, Python.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/counting_sort_algorithm.htm", "title": "Counting Sort Algorithm", "content": "Counting sort is an external sorting algorithm that assumes all the input values are integers that lie between the range 0 and k. Then mathematical computations on these input values to place them at the correct position in the output array.\nThis algorithm makes use of a counter to count the frequency of occurrence of the numbers and arrange them accordingly. Suppose, if a number m occurs 5 times in the input sequence, the counter value of the number will become 5 and it is repeated 5 times in the output array.\nThe counting sort algorithm assumes that the input is relatively smaller so the algorithm is as follows −\n− Maintain two arrays, one with the size of input elements without repetition to store the count values and other with the size of the input array to store the output.\n− Initialize the count array with all zeroes and keep the output array empty.\n− Every time an element occurs in the input list, increment the corresponding counter value by 1, until it reaches the end of the input list.\n− Now, in the output array, every time a counter is greater than 0, add the element at its respective index, i.e. if the counter of 0 is 2, 0 added at the 2nd position (i.e. 1st index) of the output array. Then decrement the counter value by 1.\n− Repeat Step 4 until all the counter values become 0. The list obtained is the output list.\nThe average case time complexity for the counting sort algorithm is same as bucket sort. It runs in \n time.\nConsider an input list to be sorted, 0, 2, 1, 4, 6, 2, 1, 1, 0, 3, 7, 7, 9.\nFor easier computations, let us start with single digit numbers.\nCreate two arrays: to store counters and the output. Initialize the counter array with zeroes.\nAfter incrementing all the counter values until it reaches the end of the input list, we achieve −\nNow, push the elements at the corresponding index in the output list.\nDecrement the counter by 1 after adding the elements in the output array. Now, 1 is added at the 4\n index.\nAdd the remaining values preceding the index in previous step.\nAfter adding the last values, we get −\nThe final sorted output is achieved as 0, 0, 1, 1, 1, 2, 2, 3, 4, 6, 7, 7, 9\nThe counting sort implementation works closely with the algorithm where we construct an array to store the frequency of each element of the input array. Based on these frequencies, the elements are placed in the output array. Repetitive elements are also sorted in the counting sort algorithm.\nIn this chapter, we look into the counting sort program implemented in four different programming languages.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/bucket_sort_algorithm.htm", "title": "Bucket Sort Algorithm", "content": "The Bucket Sort algorithm is similar to the \n algorithm, as it is just the generalized form of the counting sort. Bucket sort assumes that the input elements are drawn from a uniform distribution over the interval [0, 1).\nHence, the bucket sort algorithm divides the interval [0, 1) into n equal parts, and the input elements are added to indexed \n where the indices based on the lower bound of the (n  element) value. Since the algorithm assumes the values as the independent numbers evenly distributed over a small range, not many elements fall into one bucket only.\nFor example, let us look at an input list of elements, 0.08, 0.01, 0.19, 0.89, 0.34, 0.07, 0.30, 0.82, 0.39, 0.45, 0.36. The bucket sort would look like −\nLet us look at how this algorithm would proceed further below −\n− Divide the interval in n equal parts, each part being referred to as a bucket. Say if n is 10, then there are 10 buckets; otherwise more.\n− Take the input elements from the input array A and add them to these output buckets B based on the computation formula, B[i]= $\\lfloor$n.A[i]$\\rfloor$\n− If there are any elements being added to the already occupied buckets, created a linked list through the corresponding bucket.\n− Then we apply insertion sort to sort all the elements in each bucket.\n− These buckets are concatenated together which in turn is obtained as the output.\nThe bucket sort algorithm assumes the identity of the input, therefore, the average case time complexity of the algorithm is (n)\nConsider, an input list of elements, 0.78, 0.17, 0.93, 0.39, 0.26, 0.72, 0.21, 0.12, 0.33, 0.28, to sort these elements using bucket sort −\nLinearly insert all the elements from the index 0 of the input array. That is, we insert 0.78 first followed by other elements sequentially. The position to insert the element is obtained using the formula − B[i]= $\\lfloor$n.A[i]$\\rfloor$, i.e, $\\lfloor$10 0.78$\\rfloor$=7\nNow, we insert 0.17 at index $\\lfloor$10 0.17$\\rfloor$=1\nInserting the next element, 0.93 into the output buckets at $\\lfloor$10 0.93$\\rfloor$=9\nInsert 0.39 at index 3 using the formula $\\lfloor$10 0.39$\\rfloor$=3\nInserting the next element in the input array, 0.26, at position $\\lfloor$10 0.26$\\rfloor$=2\nHere is where it gets tricky. Now, the next element in the input list is 0.72 which needs to be inserted at index 7 using the formula $\\lfloor$10 0.72$\\rfloor$=7. But theres already a number in the 7\n bucket. So, a link is created from the 7\n index to store the new number like a linked list, as shown below −\nAdd the remaining numbers to the buckets in the similar manner by creating linked lists from the desired buckets. But while inserting these elements as lists, we apply insertion sort, i.e., compare the two elements and add the minimum value at the front as shown below −\nNow, to achieve the output, concatenate all the buckets together.\n0.12, 0.17, 0.21, 0.26, 0.28, 0.33, 0.39, 0.72, 0.78, 0.93\nThe implementation of the bucket sort algorithm first retrieves the maximum element of the array and decides the bucket size of the output. The elements are inserted into these buckets based on few computations.\nIn this tutorial, we execute bucket sort in four programming languages.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/heap_sort_algorithm.htm", "title": "Heap Sort Algorithm", "content": "Heap Sort is an efficient sorting technique based on the \n.\nThe heap is a nearly-complete binary tree where the parent node could either be minimum or maximum. The heap with minimum root node is called \n and the root node with maximum root node is called \n. The elements in the input data of the heap sort algorithm are processed using these two methods.\nThe heap sort algorithm follows two main operations in this procedure −\nBuilds a heap H from the input data using the \n (explained further into the chapter) method, based on the way of sorting  ascending order or descending order.\nDeletes the root element of the root element and repeats until all the input elements are processed.\nThe heap sort algorithm heavily depends upon the heapify method of the binary tree. So what is this heapify method?\nThe \n method of a binary tree is to convert the tree into a heap data structure. This method uses recursion approach to heapify all the nodes of the binary tree.\n − The binary tree must always be a complete binary tree as it must have two children nodes always.\nThe complete binary tree will be converted into either a max-heap or a min-heap by applying the \n method.\nTo know more about the heapify algorithm, please \nAs described in the algorithm below, the sorting algorithm first constructs the heap ADT by calling the Build-Max-Heap algorithm and removes the root element to swap it with the minimum valued node at the leaf. Then the heapify method is applied to rearrange the elements accordingly.\nThe heap sort algorithm is the combination of two other sorting algorithms: insertion sort and merge sort.\nThe similarities with insertion sort include that only a constant number of array elements are stored outside the input array at any time.\nThe time complexity of the heap sort algorithm is \n, similar to merge sort.\nLet us look at an example array to understand the sort algorithm better −\nBuilding a heap using the BUILD-MAX-HEAP algorithm from the input array −\nRearrange the obtained binary tree by exchanging the nodes such that a heap data structure is formed.\nApplying the heapify method, remove the root node from the heap and replace it with the next immediate maximum valued child of the root.\nThe root node is 23, so 23 is popped and 18 is made the next root because it is the next maximum node in the heap.\nNow, 18 is popped after 23 which is replaced by 14.\nThe current root 14 is popped from the heap and is replaced by 12.\n12 is popped and replaced with 10.\nSimilarly all the other elements are popped using the same process.\nHere the current root element 9 is popped and the elements 8 and 3 are remained in the tree.\nThen, 8 will be popped leaving 3 in the tree.\nAfter completing the heap sort operation on the given heap, the sorted elements are displayed as shown below − \nEvery time an element is popped, it is added at the beginning of the output array since the heap data structure formed is a max-heap. But if the heapify method converts the binary tree to the min-heap, add the popped elements are on the end of the output array.\nThe final sorted list is,\nThe logic applied on the implementation of the heap sort is: firstly, the heap data structure is built based on the max-heap property where the parent nodes must have greater values than the child nodes. Then the root node is popped from the heap and the next maximum node on the heap is shifted to the root. The process is continued iteratively until the heap is empty.\nIn this tutorial, we show the heap sort implementation in four different programming languages.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/shell_sort_algorithm.htm", "title": "Shell Sort Algorithm", "content": "Shell sort is a highly efficient sorting algorithm and is based on insertion sort algorithm. This algorithm avoids large shifts as in case of insertion sort, if the smaller value is to the far right and has to be moved to the far left.\nThis algorithm uses insertion sort on a widely spread elements, first to sort them and then sorts the less widely spaced elements. This spacing is termed as \n. This interval is calculated based on Knuth's formula as −\nThis algorithm is quite efficient for medium-sized data sets as its average and worst case complexity are of O(n), where \n is the number of items.\nFollowing is the algorithm for shell sort.\nFollowing is the pseudocode for shell sort.\nLet us consider the following example to have an idea of how shell sort works. We take the same array we have used in our previous examples. For our example and ease of understanding, we take the interval of 4. Make a virtual sub-list of all values located at the interval of 4 positions. Here these values are {35, 14}, {33, 19}, {42, 27} and {10, 14}\nWe compare values in each sub-list and swap them (if necessary) in the original array. After this step, the new array should look like this  −\nThen, we take interval of 2 and this gap generates two sub-lists - {14, 27, 35, 42}, {19, 10, 33, 44}\nWe compare and swap the values, if required, in the original array. After this step, the array should look like this −\nFinally, we sort the rest of the array using interval of value 1. Shell sort uses insertion sort to sort the array.\nFollowing is the step-by-step depiction −\nWe see that it required only four swaps to sort the rest of the array.\nShell sort is a highly efficient sorting algorithm and is based on insertion sort algorithm. This algorithm avoids large shifts as in case of insertion sort, if the smaller value is to the far right and has to be moved to the far left.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/merge_sort_algorithm.htm", "title": "Merge Sort Algorithm", "content": "Merge sort is a sorting technique based on divide and conquer technique. With worst-case time complexity being (n log n), it is one of the most used and approached algorithms.\nMerge sort first divides the array into equal halves and then combines them in a sorted manner.\nTo understand merge sort, we take an unsorted array as the following −\nWe know that merge sort first divides the whole array iteratively into equal halves unless the atomic values are achieved. We see here that an array of 8 items is divided into two arrays of size 4.\nThis does not change the sequence of appearance of items in the original. Now we divide these two arrays into halves.\nWe further divide these arrays and we achieve atomic value which can no more be divided.\nNow, we combine them in exactly the same manner as they were broken down. Please note the color codes given to these lists.\nWe first compare the element for each list and then combine them into another list in a sorted manner. We see that 14 and 33 are in sorted positions. We compare 27 and 10 and in the target list of 2 values we put 10 first, followed by 27. We change the order of 19 and 35 whereas 42 and 44 are placed sequentially.\nIn the next iteration of the combining phase, we compare lists of two data values, and merge them into a list of found data values placing all in a sorted order.\nAfter the final merging, the list becomes sorted and is considered the final solution.\nMerge sort keeps on dividing the list into equal halves until it can no more be divided. By definition, if it is only one element in the list, it is considered sorted. Then, merge sort combines the smaller sorted lists keeping the new list sorted too.\nWe shall now see the pseudocodes for merge sort functions. As our algorithms point out two main functions  divide & merge.\nMerge sort works with recursion and we shall see our implementation in the same way.\nIn the following example, we have shown Merge-Sort algorithm step by step. First, every iteration array is divided into two sub-arrays, until the sub-array contains only one element. When these sub-arrays cannot be divided further, then merge operations are performed.\nLet us consider, the running time of Merge-Sort as \n. Hence,\n$$\\mathrm{T\\left ( n \\right )=\\left\\{\\begin{matrix} c & if\\, n\\leq 1 \\\\ 2\\, xT\\left ( \\frac{n}{2} \\right )+dxn &otherwise  \\\\\n\\end{matrix}\\right.}\\:where\\: c\\: and\\: d\\: are\\: constants$$ \nTherefore, using this recurrence relation,\n$$T\\left ( n \\right )=2^{i}\\, T\\left ( n/2^{i} \\right )+i\\cdot d\\cdot n$$\n$$As,\\:\\: i=log\\: n,\\: T\\left ( n \\right )=2^{log\\, n}T\\left ( n/2^{log\\, n} \\right )+log\\, n\\cdot d\\cdot n$$\n$$=c\\cdot n+d\\cdot n\\cdot log\\: n$$\n$$Therefore,\\: \\: T\\left ( n \\right ) = O(n\\: log\\: n ).$$\nFollowing are the implementations of the above approach in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/selection_sort_algorithm.htm", "title": "Selection Sort Algorithm", "content": "Selection sort is a simple sorting algorithm. This sorting algorithm, like insertion sort, is an in-place comparison-based algorithm in which the list is divided into two parts, the sorted part at the left end and the unsorted part at the right end. Initially, the sorted part is empty and the unsorted part is the entire list.\nThe smallest element is selected from the unsorted array and swapped with the leftmost element, and that element becomes a part of the sorted array. This process continues moving unsorted array boundaries by one element to the right.\nThis algorithm is not suitable for large data sets as its average and worst case complexities are of \n, where \n is the number of items.\nThis type of sorting is called Selection Sort as it works by repeatedly sorting elements. That is: we first find the smallest value in the array and exchange it with the element in the first position, then find the second smallest element and exchange it with the element in the second position, and we continue the process in this way until the entire array is sorted.\nSelection sort is among the simplest of sorting techniques and it works very well for small files. It has a quite important application as each item is actually moved at the most once.\nSection sort is a method of choice for sorting files with very large objects (records) and small keys. The worst case occurs if the array is already sorted in a descending order and we want to sort them in an ascending order.\nNonetheless, the time required by selection sort algorithm is not very sensitive to the original order of the array to be sorted: the test if [] < \n is executed exactly the same number of times in every case.\nSelection sort spends most of its time trying to find the minimum element in the unsorted part of the array. It clearly shows the similarity between Selection sort and Bubble sort.\nBubble sort selects the maximum remaining elements at each stage, but wastes some effort imparting some order to an unsorted part of the array.\nSelection sort is quadratic in both the worst and the average case, and requires no extra memory.\nFor each \n from \n to \n, there is one exchange and \n comparisons, so there is a total of \n exchanges and\n comparisons.\nThese observations hold, no matter what the input data is.\nIn the worst case, this could be quadratic, but in the average case, this quantity is \n. It implies that the \n.\nConsider the following depicted array as an example.\nFor the first position in the sorted list, the whole list is scanned sequentially. The first position where 14 is stored presently, we search the whole list and find that 10 is the lowest value.\nSo we replace 14 with 10. After one iteration 10, which happens to be the minimum value in the list, appears in the first position of the sorted list.\nFor the second position, where 33 is residing, we start scanning the rest of the list in a linear manner.\nWe find that 14 is the second lowest value in the list and it should appear at the second place. We swap these values.\nAfter two iterations, two least values are positioned at the beginning in a sorted manner.\nThe same process is applied to the rest of the items in the array −\nThe selection sort algorithm is implemented in four different programming languages below. The given program selects the minimum number of the array and swaps it with the element in the first index. The second minimum number is swapped with the element present in the second index. The process goes on until the end of the array is reached.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/insertion_sort_algorithm.htm", "title": "Insertion Sort Algorithm", "content": "Insertion sort is a very simple method to sort numbers in an ascending or descending order. This method follows the incremental method. It can be compared with the technique how cards are sorted at the time of playing a game.\nThis is an in-place comparison-based sorting algorithm. Here, a sub-list is maintained which is always sorted. For example, the lower part of an array is maintained to be sorted. An element which is to be 'inserted' in this sorted sub-list, has to find its appropriate place and then it has to be inserted there. Hence the name, \n.\nThe array is searched sequentially and unsorted items are moved and inserted into the sorted sub-list (in the same array). This algorithm is not suitable for large data sets as its average and worst case complexity are of (n\n), where \n is the number of items.\nNow we have a bigger picture of how this sorting technique works, so we can derive simple steps by which we can achieve insertion sort.\n− If it is the first element, it is already sorted. return 1;\n− Pick next element\n− Compare with all elements in the sorted sub-list\n− Shift all the elements in the sorted sub-list that is greater than the value to be sorted\n− Insert the value\n− Repeat until list is sorted\nRun time of this algorithm is very much dependent on the given input.\nIf the given numbers are sorted, this algorithm runs in \n time. If the given numbers are in reverse order, the algorithm runs in \n time.\nWe take an unsorted array for our example.\nInsertion sort compares the first two elements.\nIt finds that both 14 and 33 are already in ascending order. For now, 14 is in sorted sub-list.\nInsertion sort moves ahead and compares 33 with 27.\nAnd finds that 33 is not in the correct position. It swaps 33 with 27. It also checks with all the elements of sorted sub-list. Here we see that the sorted sub-list has only one element 14, and 27 is greater than 14. Hence, the sorted sub-list remains sorted after swapping.\nBy now we have 14 and 27 in the sorted sub-list. Next, it compares 33 with 10. These values are not in a sorted order.\nSo they are swapped.\nHowever, swapping makes 27 and 10 unsorted.\nHence, we swap them too.\nAgain we find 14 and 10 in an unsorted order.\nWe swap them again.\nBy the end of third iteration, we have a sorted sub-list of 4 items.\nThis process goes on until all the unsorted values are covered in a sorted sub-list. Now we shall see some programming aspects of insertion sort.\nSince insertion sort is an in-place sorting algorithm, the algorithm is implemented in a way where the key element  which is iteratively chosen as every element in the array  is compared with it consequent elements to check its position. If the key element is less than its successive element, the swapping is not done. Otherwise, the two elements compared will be swapped and the next element is chosen as the key element.\nInsertion sort is implemented in four programming languages, C, C++, Java, and Python −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/bubble_sort_algorithm.htm", "title": "Bubble Sort Algorithm", "content": "Bubble sort is a simple sorting algorithm. This sorting algorithm is comparison-based algorithm in which each pair of adjacent elements is compared and the elements are swapped if they are not in order. This algorithm is not suitable for large data sets as its average and worst case complexity are of O(n\n) where \n is the number of items.\nBubble Sort is an elementary sorting algorithm, which works by repeatedly exchanging adjacent elements, if necessary. When no exchanges are required, the file is sorted.\nWe assume \n is an array of \n elements. We further assume that \n function swaps the values of the given array elements.\n− Check if the first element in the input array is greater than the next element in the array.\n\n− If it is greater, swap the two elements; otherwise move the pointer forward in the array.\n\n− Repeat Step 2 until we reach the end of the array.\n\n− Check if the elements are sorted; if not, repeat the same process (Step 1 to Step 3) from the last element of the array to the first.\n\n− The final output achieved is the sorted array.\n\nWe observe in algorithm that Bubble Sort compares each pair of array element unless the whole array is completely sorted in an ascending order. This may cause a few complexity issues like what if the array needs no more swapping as all the elements are already ascending.\nTo ease-out the issue, we use one flag variable \n which will help us see if any swap has happened or not. If no swap has occurred, i.e. the array requires no more processing to be sorted, it will come out of the loop.\nPseudocode of bubble sort algorithm can be written as follows −\nHere, the number of comparisons are\nClearly, the graph shows the \n nature of the bubble sort.\nIn this algorithm, the number of comparison is irrespective of the data set, i.e. whether the provided input elements are in sorted order or in reverse order or at random.\nFrom the algorithm stated above, it is clear that bubble sort does not require extra memory.\nWe take an unsorted array for our example. Bubble sort takes (n2) time so we're keeping it short and precise.\nBubble sort starts with very first two elements, comparing them to check which one is greater.\nIn this case, value 33 is greater than 14, so it is already in sorted locations. Next, we compare 33 with 27.\nWe find that 27 is smaller than 33 and these two values must be swapped.\nNext we compare 33 and 35. We find that both are in already sorted positions.\nThen we move to the next two values, 35 and 10.\nWe know then that 10 is smaller 35. Hence they are not sorted. We swap these values. We find that we have reached the end of the array. After one iteration, the array should look like this −\nTo be precise, we are now showing how an array should look like after each iteration. After the second iteration, it should look like this −\nNotice that after each iteration, at least one value moves at the end.\nAnd when there's no swap required, bubble sort learns that an array is completely sorted.\nNow we should look into some practical aspects of bubble sort.\nOne more issue we did not address in our original algorithm and its improvised pseudocode, is that, after every iteration the highest values settles down at the end of the array. Hence, the next iteration need not include already sorted elements. For this purpose, in our implementation, we restrict the inner loop to avoid already sorted values.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/sorting_algorithms.htm", "title": "Data Structures - Sorting Techniques", "content": "Sorting refers to arranging data in a particular format. Sorting algorithm specifies the way to arrange data in a particular order. Most common orders are in numerical or lexicographical order.\nThe importance of sorting lies in the fact that data searching can be optimized to a very high level, if data is stored in a sorted manner. Sorting is also used to represent data in more readable formats. Following are some of the examples of sorting in real-life scenarios −\n − The telephone directory stores the telephone numbers of people sorted by their names, so that the names can be searched easily.\n − The dictionary stores words in an alphabetical order so that searching of any word becomes easy.\nSorting algorithms may require some extra space for comparison and temporary storage of few data elements. These algorithms do not require any extra space and sorting is said to happen in-place, or for example, within the array itself. This is called \n. Bubble sort is an example of in-place sorting.\nHowever, in some sorting algorithms, the program requires space which is more than or equal to the elements being sorted. Sorting which uses equal or more space is called \n. Merge-sort is an example of not-in-place sorting.\nIf a sorting algorithm, after sorting the contents, does not change the sequence of similar content in which they appear, it is called \n.\nIf a sorting algorithm, after sorting the contents, changes the sequence of similar content in which they appear, it is called \n.\nStability of an algorithm matters when we wish to maintain the sequence of original elements, like in a tuple for example.\nA sorting algorithm is said to be adaptive, if it takes advantage of already 'sorted' elements in the list that is to be sorted. That is, while sorting if the source list has some element already sorted, adaptive algorithms will take this into account and will try not to re-order them.\nA non-adaptive algorithm is one which does not take into account the elements which are already sorted. They try to force every single element to be re-ordered to confirm their sorted ness.\nSome terms are generally coined while discussing sorting techniques, here is a brief introduction to them −\nA sequence of values is said to be in \n, if the successive element is greater than the previous one. For example, 1, 3, 4, 6, 8, 9 are in increasing order, as every next element is greater than the previous element.\nA sequence of values is said to be in \n, if the successive element is less than the current one. For example, 9, 8, 6, 4, 3, 1 are in decreasing order, as every next element is less than the previous element.\nA sequence of values is said to be in \n, if the successive element is less than or equal to its previous element in the sequence. This order occurs when the sequence contains duplicate values. For example, 9, 8, 6, 3, 3, 1 are in non-increasing order, as every next element is less than or equal to (in case of 3) but not greater than any previous element.\nA sequence of values is said to be in \n, if the successive element is greater than or equal to its previous element in the sequence. This order occurs when the sequence contains duplicate values. For example, 1, 3, 3, 6, 8, 9 are in non-decreasing order, as every next element is greater than or equal to (in case of 3) but not less than the previous one.\nThere are several sorting techniques available to sort the  contents of various data structures. Following are some of those −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/hash_data_structure.htm", "title": "Hash Table Data structure", "content": "Hash Table is a data structure which stores data in an associative manner. In a hash table, data is stored in an array format, where each data value has its own unique index value. Access of data becomes very fast if we know the index of the desired data.\nThus, it becomes a data structure in which insertion and search operations are very fast irrespective of the size of the data. Hash Table uses an array as a storage medium and uses hash technique to generate an index where an element is to be inserted or is to be located from.\nHashing is a technique to convert a range of key values into a range of indexes of an array. We're going to use modulo operator to get a range of key values. Consider an example of hash table of size 20, and the following items are to be stored. Item are in the (key,value) format.\nAs we can see, it may happen that the hashing technique is used to create an already used index of the array. In such a case, we can search the next empty location in the array by looking into the next cell until we find an empty cell. This technique is called linear probing.\nFollowing are the basic primary operations of a hash table.\n − Searches an element in a hash table.\n − Inserts an element in a hash table.\n − Deletes an element from a hash table.\nDefine a data item having some data and key, based on which the search is to be conducted in a hash table.\nDefine a hashing method to compute the hash code of the key of the data item.\nWhenever an element is to be searched, compute the hash code of the key passed and locate the element using that hash code as index in the array. Use linear probing to get the element ahead if the element is not found at the computed hash code.\nFollowing are the implementations of this operation in various programming language −\nWhenever an element is to be inserted, compute the hash code of the key passed and locate the index using that hash code as an index in the array. Use linear probing for empty location, if an element is found at the computed hash code.\nFollowing are the implementations of this operation in various programming languages −\nWhenever an element is to be deleted, compute the hash code of the key passed and locate the index using that hash code as an index in the array. Use linear probing to get the element ahead if an element is not found at the computed hash code. When found, store a dummy item there to keep the performance of the hash table intact.\nFollowing are the implementations of the deletion operation for Hash Table in various programming languages −\nFollowing are the complete implementations of the above operations in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/sublist_search.htm", "title": "Sublist Search Algorithm", "content": "Until now, in this tutorial, we have only seen how to search for one element in a sequential order of elements. But the sublist search algorithm provides a procedure to search for a linked list in another linked list. It works like any simple pattern matching algorithm where the aim is to determine whether one list is present in the other list or not.\nThe algorithm walks through the linked list where the first element of one list is compared with the first element of the second list; if a match is not found, the second element of the first list is compared with the first element of the second list. This process continues until a match is found or it reaches the end of a list.\nFor example, consider two linked lists with values {4, 6, 7, 3, 8, 2, 6} and {3, 8, 2}. Sublist search checks whether the values of second list are present in the first linked list. The output is obtained in Boolean values {True, False}. It cannot return the position of the sub-list as the linked list is not an ordered data structure.\n − The output is returned true only if the second linked list is present in the exact same order in the first list.\nThe main aim of this algorithm is to prove that one linked list is a sub-list of another list. Searching in this process is done linearly, checking each element of the linked list one by one; if the output returns true, then it is proven that the second list is a sub-list of the first linked list.\nProcedure for the sublist search algorithm is as follows −\n− Maintain two pointers, each pointing to one list. These pointers are used to traverse through the linked lists.\n\n− Check for the base cases of the linked lists −\n\nIf both linked lists are empty, the output returns true.\nIf the second list is not empty but the first list is empty, we return false.\nIf the first list is not empty but the second list is empty, we return false.\n− Once it is established that both the lists are not empty, use the pointers to traverse through the lists element by element.\n− Compare the first element of the first linked list and the first element of the second linked list; if it is a match both the pointers are pointed to the next values in both lists respectively.\n− If it is not a match, keep the pointer in second list at the first element but move the pointer in first list forward. Compare the elements again.\n− Repeat Steps 4 and 5 until we reach the end of the lists.\n− If the output is found, TRUE is returned and if not, FALSE.\nThe time complexity of the sublist search depends on the number of elements present in both linked lists involved. The worst case time taken by the algorithm to be executed is \n where m is the number of elements present in the first linked list and n is the number of elements present in the second linked list.\nSuppose we have two linked lists with elements given as −\nUsing sublist search, we need to find out if List 2 is present in List 1.\nCompare the first element of the List 2 with the first element of List 1. It is not a match, so the pointer in List 1 moves to the next memory address in it.\nIn this step, the second element of the List 1 is compared with the first element of the List 2. It is not a match so the pointer in List 1 moves to next memory address.\nNow the third element in List 1 is compared with the first element in the List 2. Since it is not a match, the pointer in List 1 moves forward.\nNow the fourth element in List 1 is compared with the first element in the List 2. Since it is not a match, the pointer in List 1 moves forward.\nNow the fifth element in List 1 is compared with the first element in the List 2. Since it is a match, the pointers in both List 1 and List 2 move forward.\nThe sixth element in List 1 is compared with the second element in the List 2. Since it is also a match, the pointers in both List 1 and List 2 move forward.\nThe seventh element in List 1 is compared with the third element in the List 2. Since it is also a match, it is proven that List 2 is a sub-list of List 1.\nThe output is returned TRUE.\nIn the sublist search implementation, linked lists are created first using struct keyword in the C language and as an object in C++, JAVA and Python languages. These linked lists are checked whether they are not empty; and then the elements are compared one by one linearly to find a match. If the second linked list is present in the first linked list in the same order, then the output is returned \n; otherwise the output is printed \n.\nThe sublist search is executed in four different programming languages in this tutorial  C, C++, JAVA and Python.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/fibonacci_search.htm", "title": "Fibonacci Search Algorithm", "content": "As the name suggests, the Fibonacci Search Algorithm uses Fibonacci numbers to search for an element in a sorted input array.\nBut first, let us revise our knowledge on Fibonacci numbers −\nFibonacci Series is a series of numbers that have two primitive numbers 0 and 1. The successive numbers are the sum of preceding two numbers in the series. This is an infinite constant series, therefore, the numbers in it are fixed. The first few numbers in this Fibonacci series include −\nThe main idea behind the Fibonacci series is also to eliminate the least possible places where the element could be found. In a way, it acts like a divide & conquer algorithm (logic being the closest to binary search algorithm). This algorithm, like jump search and exponential search, also skips through the indices of the input array in order to perform searching.\nThe Fibonacci Search Algorithm makes use of the Fibonacci Series to diminish the range of an array on which the searching is set to be performed. With every iteration, the search range decreases making it easier to locate the element in the array. The detailed procedure of the searching is seen below −\n− As the first step, find the immediate Fibonacci number that is greater than or equal to the size of the input array. Then, also hold the two preceding numbers of the selected Fibonacci number, that is, we hold Fm, Fm-1, Fm-2 numbers from the Fibonacci Series.\n− Initialize the offset value as -1, as we are considering the entire array as the searching range in the beginning.\n− Until Fm-2 is greater than 0, we perform the following steps −\nCompare the key element to be found with the element at index \n. If a match is found, return the index.\nIf the key element is found to be lesser value than this element, we reduce the range of the input from 0 to the index of this element. The Fibonacci numbers are also updated with F\n = F\n.\nBut if the key element is greater than the element at this index, we remove the elements before this element from the search range. The Fibonacci numbers are updated as F\n = F\n. The \n value is set to the index of this element.\n − As there are two 1s in the Fibonacci series, there arises a case where your two preceding numbers will become 1. So if F\n becomes 1, there is only one element left in the array to be searched. We compare the key element with that element and return the 1st index. Otherwise, the algorithm returns an unsuccessful search.\nThe Fibonacci Search algorithm takes logarithmic time complexity to search for an element. Since it is based on a divide on a conquer approach and is similar to idea of binary search, the time taken by this algorithm to be executed under the worst case consequences is \n.\nSuppose we have a sorted array of elements {12, 14, 16, 17, 20, 24, 31, 43, 50, 62} and need to identify the location of element 24 in it using Fibonacci Search.\nThe size of the input array is 10. The smallest Fibonacci number greater than 10 is 13.\nTherefore, F\n = 13, F\n = 8, F\n = 5.\nWe initialize offset = -1\nIn the first iteration, compare it with the element at index = minimum (offset + F\n, n  1) = minimum (-1 + 5, 9) = minimum (4, 9) = 4.\nThe fourth element in the array is 20, which is not a match and is less than the key element.\nIn the second iteration, update the offset value and the Fibonacci numbers.\nSince the key is greater, the offset value will become the index of the element, i.e. 4. Fibonacci numbers are updated as F\n = F\n = 8.\nFm-1 = 5, Fm-2 = 3.\nNow, compare it with the element at index = minimum (offset + F\n, n  1) = minimum (4 + 3, 9) = minimum (7, 9) = 7.\nElement at the 7\n index of the array is 43, which is not a match and is also lesser than the key.\nWe discard the elements after the 7th index, so n = 7 and offset value remains 4.\nFibonacci numbers are pushed two steps backward, i.e. F\n = F\n = 3.\nF\n = 2, F\n = 1.\nNow, compare it with the element at index = minimum (offset + Fm-2, n  1) = minimum (4 + 1, 6) = minimum (5, 7) = 5.\nThe element at index 5 in the array is 24, which is our key element. 5\n index is returned as the output for this example array.\nThe output is returned as 5.\nThe Fibonacci search algorithm uses the divide and conquer strategy to eliminate the search spaces that are not likely to contain the required element. This elimination is done with the help of the Fibonacci numbers to narrow down the search range within an input array. The implementation for the Fibonacci search method in four different programming languages is shown below −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/exponential_search.htm", "title": "Exponential Search Algorithm", "content": "Exponential search algorithm targets a range of an input array in which it assumes that the required element must be present in and performs a binary search on that particular small range. This algorithm is also known as doubling search or finger search.\nIt is similar to jump search in dividing the sorted input into multiple blocks and conducting a smaller scale search. However, the difference occurs while performing computations to divide the blocks and the type of smaller scale search applied (jump search applies linear search and exponential search applies binary search).\nHence, this algorithm jumps exponentially in the powers of 2. In simpler words, the search is performed on the blocks divided using pow(2, k) where k is an integer greater than or equal to 0. Once the element at position pow(2, n) is greater than the key element, binary search is performed on the current block.\nIn the exponential search algorithm, the jump starts from the 1st index of the array. So we manually compare the first element as the first step in the algorithm.\n− Compare the first element in the array with the key, if a match is found return the 0th index.\n− Initialize i = 1 and compare the ith element of the array with the key to be search. If it matches return the index.\n− If the element does not match, jump through the array exponentially in the powers of 2. Therefore, now the algorithm compares the element present in the incremental position.\n− If the match is found, the index is returned. Otherwise Step 2 is repeated iteratively until the element at the incremental position becomes greater than the key to be searched.\n− Since the next increment has the higher element than the key and the input is sorted, the algorithm applies binary search algorithm on the current block.\n− The index at which the key is present is returned if the match is found; otherwise it is determined as an unsuccessful search.\nEven though it is called Exponential search it does not perform searching in exponential time complexity. But as we know, in this search algorithm, the basic search being performed is binary search. Therefore, the time complexity of the exponential search algorithm will be the same as the binary search algorithms, \n.\nTo understand the exponential search algorithm better and in a simpler way, let us search for an element in an example input array using the exponential search algorithm −\nThe sorted input array given to the search algorithm is −\nLet us search for the position of element 81 in the given array.\nCompare the first element of the array with the key element 81.\nThe first element of the array is 6, but the key element to be searched is 81; hence, the jump starts from the 1st index as there is no match found.\nAfter initializing i = 1, the key element is compared with the element in the first index. Here, the element in the 1st index does not match with the key element. So it is again incremented exponentially in the powers of 2.\nThe index is incremented to 2\n = 2\n = the element in 2\n index is compared with the key element.\nIt is still not a match so it is once again incremented.\nThe index is incremented in the powers of 2 again.\n2\n = 4 = the element in 4\n index is compared with the key element and a match is not found yet.\nThe index is incremented exponentially once again. This time the element in the 8th index is compared with the key element and a match is not found.\nHowever, the element in the 8th index is greater than the key element. Hence, the binary search algorithm is applied on the current block of elements.\nThe current block of elements includes the elements in the indices [4, 5, 6, 7].\nSmall scale binary search is applied on this block of elements, where the mid is calculated to be the 5\n element.\nThe match is not found at the mid element and figures that the desired element is greater than the mid element. Hence, the search takes place is the right half of the block.\nThe mid now is set as 6\n element −\nThe element is still not found at the 6\n element so it now searches in the right half of the mid element.\nThe next mid is set as 7\n element.\nHere, the element is found at the 7\n index.\nIn the implementation of the exponential search algorithm, the program checks for the matches at every exponential jump in the powers of 2. If the match is found the location of the element is returned otherwise the program returns an unsuccessful search.\nOnce the element at an exponential jump becomes greater than the key element, a binary search is performed on the current block of elements.\nIn this chapter, we will look into the implementation of exponential search in four different languages.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/jump_search_algorithm.htm", "title": "Jump Search Algorithm", "content": "Jump Search algorithm is a slightly modified version of the linear search algorithm. The main idea behind this algorithm is to reduce the time complexity by comparing lesser elements than the linear search algorithm. The input array is hence sorted and divided into blocks to perform searching while jumping through these blocks.\nFor example, let us look at the given example below; the sorted input array is searched in the blocks of 3 elements each. The desired key is found only after 2 comparisons rather than the 6 comparisons of the linear search.\nHere, there arises a question about how to divide these blocks. To answer that, if the input array is of size n, the blocks are divided in the intervals of n. First element of every block is compared with the key element until the key elements value is less than the block element. Linear search is performed only on that previous block since the input is sorted. If the element is found, it is a successful search; otherwise, an unsuccessful search is returned.\nJump search algorithm is discussed in detail further into this chapter.\nThe jump search algorithm takes a sorted array as an input which is divided into smaller blocks to make the search simpler. The algorithm is as follows −\n− If the size of the input array is n, then the size of the block is n. Set i = 0.\n− The key to be searched is compared with the ith element of the array. If it is a match, the position of the element is returned; otherwise i is incremented with the block size.\n− The Step 2 is repeated until the ith element is greater than the key element.\n− Now, the element is figured to be in the previous block, since the input array is sorted. Therefore, linear search is applied on that block to find the element.\n− If the element is found, the position is returned. If the element is not found, unsuccessful search is prompted.\nThe time complexity of the jump search technique is \n and space complexity is \n.\nLet us understand the jump search algorithm by searching for element 66 from the given sorted array, A, below −\nInitialize i = 0, and size of the input array n = 12\nSuppose, block size is represented as m. Then, m = n = 12 = 3\nCompare A[0] with the key element and check whether it matches,\nTherefore, i is incremented by the block size = 3. Now the element compared with the key element is A[3].\nSince it is not a match, i is again incremented by 3.\ni is incremented by 3 again. A[9] is compared with the key element.\nHowever, 88 is greater than 66, therefore linear search is applied on the current block.\nAfter applying linear search, the pointer increments from 6th index to 7th. Therefore, A[7] is compared with the key element.\nWe find that A[7] is the required element, hence the program returns 7th index as the output.\nThe jump search algorithm is an extended variant of linear search. The algorithm divides the input array into multiple small blocks and performs the linear search on a single block that is assumed to contain the element. If the element is not found in the assumed blocked, it returns an unsuccessful search.\nThe output prints the position of the element in the array instead of its index. Indexing refers to the index numbers of the array that start from 0 while position is the place where the element is stored.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/interpolation_search_algorithm.htm", "title": "Interpolation Search Algorithm", "content": "Interpolation search is an improved variant of binary search. This search algorithm works on the probing position of the required value. For this algorithm to work properly, the data collection should be in a sorted form and equally distributed.\nBinary search has a huge advantage of time complexity over linear search. Linear search has worst-case complexity of (n) whereas binary search has (log n).\nThere are cases where the location of target data may be known in advance. For example, in case of a telephone directory, if we want to search the telephone number of Morpheus. Here, linear search and even binary search will seem slow as we can directly jump to memory space where the names start from 'M' are stored.\nIn binary search, if the desired data is not found then the rest of the list is divided in two parts, lower and higher. The search is carried out in either of them.\nEven when the data is sorted, binary search does not take advantage to probe the position of the desired data.\nInterpolation search finds a particular item by computing the probe position. Initially, the probe position is the position of the middle most item of the collection.\nIf a match occurs, then the index of the item is returned. To split the list into two parts, we use the following method −\n$$mid\\, =\\, Lo\\, +\\, \\frac{\\left ( Hi\\, -\\, Lo \\right )\\ast \\left ( X\\, -\\, A\\left [ Lo \\right ] \\right )}{A\\left [ Hi \\right ]\\, -\\, A\\left [ Lo \\right ]}$$\nwhere −\nIf the middle item is greater than the item, then the probe position is again calculated in the sub-array to the right of the middle item. Otherwise, the item is searched in the sub-array to the left of the middle item. This process continues on the sub-array as well until the size of subarray reduces to zero.\nAs it is an improvisation of the existing BST algorithm, we are mentioning the steps to search the 'target' data value index, using position probing −\nRuntime complexity of interpolation search algorithm is \n as compared to \n of BST in favorable situations.\nTo understand the step-by-step process involved in the interpolation search, let us look at an example and work around it.\nConsider an array of sorted elements given below −\nLet us search for the element 19.\nUnlike binary search, the middle point in this approach is chosen using the formula −\n$$mid\\, =\\, Lo\\, +\\, \\frac{\\left ( Hi\\, -\\, Lo \\right )\\ast \\left ( X\\, -\\, A\\left [ Lo \\right ] \\right )}{A\\left [ Hi \\right ]\\, -\\, A\\left [ Lo \\right ]}$$\nSo in this given array input,\nApplying the formula to find the middle point in the list, we get\n$$mid\\, =\\, 0\\, +\\, \\frac{\\left ( 9\\, -\\, 0 \\right )\\ast \\left ( 19\\, -\\, 10 \\right )}{44\\, -\\, 10}$$\n$$mid\\, =\\, \\frac{9\\ast 9}{34}$$\n$$mid\\, =\\, \\frac{81}{34}\\,=\\,2.38$$\nSince, mid is an index value, we only consider the integer part of the decimal. That is, mid = 2.\nComparing the key element given, that is 19, to the element present in the mid index, it is found that both the elements match.\nTherefore, the element is found at index 2.\nInterpolation search is an improved variant of binary search. This search algorithm works on the probing position of the required value. For this algorithm to work properly, the data collection should be in sorted and equally distributed form.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/binary_search_algorithm.htm", "title": "Binary Search Algorithm", "content": "Binary search is a fast search algorithm with run-time complexity of (log n). This search algorithm works on the principle of divide and conquer, since it divides the array into half before searching. For this algorithm to work properly, the data collection should be in the sorted form.\nBinary search looks for a particular key value by comparing the middle most item of the collection. If a match occurs, then the index of item is returned. But if the middle item has a value greater than the key value, the right sub-array of the middle item is searched. Otherwise, the left sub-array is searched. This process continues recursively until the size of a subarray reduces to zero.\nBinary Search algorithm is an interval searching method that performs the searching in intervals only. The input taken by the binary search algorithm must always be in a sorted array since it divides the array into subarrays based on the greater or lower values. The algorithm follows the procedure below −\n − Select the middle item in the array and compare it with the key value to be searched. If it is matched, return the position of the median.\n − If it does not match the key value, check if the key value is either greater than or less than the median value.\n − If the key is greater, perform the search in the right sub-array; but if the key is lower than the median value, perform the search in the left sub-array.\n − Repeat Steps 1, 2 and 3 iteratively, until the size of sub-array becomes 1.\n − If the key value does not exist in the array, then the algorithm returns an unsuccessful search.\nThe pseudocode of binary search algorithms should look like this −\nSince the binary search algorithm performs searching iteratively, calculating the time complexity is not as easy as the linear search algorithm.\nThe input array is searched iteratively by dividing into multiple sub-arrays after every unsuccessful iteration. Therefore, the recurrence relation formed would be of a dividing function.\nTo explain it in simpler terms,\nDuring the first iteration, the element is searched in the entire array. Therefore, length of the array = n.\nIn the second iteration, only half of the original array is searched. Hence, length of the array = n/2.\nIn the third iteration, half of the previous sub-array is searched. Here, length of the array will be = n/4.\nSimilarly, in the i\n iteration, the length of the array will become n/2\nTo achieve a successful search, after the last iteration the length of array must be 1. Hence,\nThat gives us −\nApplying log on both sides,\nThe time complexity of the binary search algorithm is \nFor a binary search to work, it is mandatory for the target array to be sorted. We shall learn the process of binary search with a pictorial example. The following is our sorted array and let us assume that we need to search the location of value 31 using binary search.\nFirst, we shall determine half of the array by using this formula −\nHere it is, 0 + (9 - 0) / 2 = 4 (integer value of 4.5). So, 4 is the mid of the array.\nNow we compare the value stored at location 4, with the value being searched, i.e. 31. We find that the value at location 4 is 27, which is not a match. As the value is greater than 27 and we have a sorted array, so we also know that the target value must be in the upper portion of the array.\nWe change our low to mid + 1 and find the new mid value again.\nOur new mid is 7 now. We compare the value stored at location 7 with our target value 31.\nThe value stored at location 7 is not a match, rather it is less than what we are looking for. So, the value must be in the lower part from this location.\nHence, we calculate the mid again. This time it is 5.\nWe compare the value stored at location 5 with our target value. We find that it is a match.\nWe conclude that the target value 31 is stored at location 5.\nBinary search halves the searchable items and thus reduces the count of comparisons to be made to very less numbers.\nBinary search is a fast search algorithm with run-time complexity of (log n). This search algorithm works on the principle of divide and conquer. For this algorithm to work properly, the data collection should be in a sorted form.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/linear_search_algorithm.htm", "title": "Linear Search Algorithm", "content": "Linear search is a type of sequential searching algorithm. In this method, every element within the input array is traversed and compared with the key element to be found. If a match is found in the array the search is said to be successful; if there is no match found the search is said to be unsuccessful and gives the worst-case time complexity.\nFor instance, in the given animated diagram, we are searching for an element 33. Therefore, the linear search method searches for it sequentially from the very first element until it finds a match. This returns a successful search.\nIn the same diagram, if we have to search for an element 46, then it returns an unsuccessful search since 46 is not present in the input.\nThe algorithm for linear search is relatively simple. The procedure starts at the very first index of the input array to be searched.\n − Start from the 0th index of the input array, compare the key value with the value present in the 0th index.\n − If the value matches with the key, return the position at which the value was found.\n − If the value does not match with the key, compare the next element in the array.\n − Repeat Step 3 until there is a match found. Return the position at which the match was found.\n − If it is an unsuccessful search, print that the element is not present in the array and exit the program.\nLinear search traverses through every element sequentially therefore, the best case is when the element is found in the very first iteration. The best-case time complexity would be \n.\nHowever, the worst case of the linear search method would be an unsuccessful search that does not find the key value in the array, it performs n iterations. Therefore, the worst-case time complexity of the linear search algorithm would be \n.\nLet us look at the step-by-step searching of the key element (say 47) in an array using the linear search method.\nThe linear search starts from the 0\n index. Compare the key element with the value in the 0\n index, 34.\nHowever, 47  34. So it moves to the next element.\nNow, the key is compared with value in the 1st index of the array.\nStill, 47  10, making the algorithm move for another iteration.\nThe next element 66 is compared with 47. They are both not a match so the algorithm compares the further elements.\nNow the element in 3rd index, 27, is compared with the key value, 47. They are not equal so the algorithm is pushed forward to check the next element.\nComparing the element in the 4\n index of the array, 47, to the key 47. It is figured that both the elements match. Now, the position in which 47 is present, i.e., 4 is returned.\nThe output achieved is Element found at 4th index.\nIn this tutorial, the Linear Search program can be seen implemented in four programming languages. The function compares the elements of input with the key value and returns the position of the key in the array or an unsuccessful search prompt if the key is not present in the array.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/searching_algorithms.htm", "title": "Data Structures - Searching Algorithms", "content": "In the previous section, we have discussed various Sorting Techniques and cases in which they can be used. However, the main idea behind performing sorting is to arrange the data in an orderly way, making it easier to search for any element within the sorted data.\n is a process of finding a particular record, which can be a single element or a small chunk, within a huge amount of data. The data can be in various forms: arrays, linked lists, trees, heaps, and graphs etc. With the increasing amount of data nowadays, there are multiple techniques to perform the searching operation.\nVarious searching techniques can be applied on the data structures to retrieve certain data. A search operation is said to be successful only if it returns the desired element or data; otherwise, the searching method is unsuccessful.\nThere are two categories these searching techniques fall into. They are −\nSequential Searching\nInterval Searching\nAs the name suggests, the sequential searching operation traverses through each element of the data sequentially to look for the desired data. The data need not be in a sorted manner for this type of search.\n − Linear Search\nUnlike sequential searching, the interval searching operation requires the data to be in a sorted manner. This method usually searches the data in intervals; it could be done by either dividing the data into multiple sub-parts or jumping through the indices to search for an element.\n − Binary Search, Jump Search etc.\nUsually, not all searching techniques are suitable for all types of data structures. In some cases, a sequential search is preferable while in other cases interval searching is preferable. Evaluation of these searching techniques is done by checking the running time taken by each searching method on a particular input.\nThis is where asymptotic notations come into the picture. To learn more about Asymptotic Notations, please \nTo explain briefly, there are three different cases of time complexity in which a program can run. They are −\nBest Case\nAverage Case\nWorst Case\nWe mostly concentrate on the only best-case and worst-case time complexities, as the average case is difficult to compute. And since the running time is based on the amount of input given to the program, the worst-case time complexity best describes the performance of any algorithm.\nFor instance, the best case time complexity of a linear search is \n where the desired element is found in the first iteration; whereas the worst case time complexity is \n when the program traverses through all the elements and still does not find an element. This is labelled as an unsuccessful search. Therefore, the actual time complexity of a linear search is seen as \n, where n is the number of elements present in the input data structure.\nMany types of searching methods are used to search for data entries in various data structures. Some of them include −\nUbiquitous Binary Search\nWe will look at each of these searching methods elaborately in the following chapters.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/deque_data_structure.htm", "title": "Deque Data Structure", "content": "is a hybrid data structure that combines the features of a \n and a \n. It allows us to insert and delete elements from both ends of the queue. The name \n is an abbreviation of \n.\nImagine an event where you have two gates to enter and exit a place. People are entering from the \n and some are entering from the \n. Now, when people are leaving, they are leaving from the \n and some sneak from the \n. Now, we need to manage flow of people from both ends. This is where \n comes into play.\nFollowing are the major operations on Deque −\nLet's understand how we can implement deque using array. For this, we need to maintain two pointers, \n and \n, to keep track of the front and back of the deque. We also need to define the size of the deque.\nWhen we insert an element at the front of the deque, we need to shift all the elements to the right by one position. We will increment the \n pointer by one and insert the element at the front of the deque.\nFollowing are the steps to insert an element at the front of the deque −\nFollowing is the implementation of push_front(x) operation on deque.\n\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output is as follows −\nFollowing is the output of the above code −\nThis operation is used for inserting an element to the back of the deque. When we insert an element at the back of the deque, we need to increment the \n pointer by one and insert the element at the back of the deque.\nFollowing are the steps to insert an element at the back of the deque −\nFollowing is the implementation of push_back(x) operation on deque.\n\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output is as follows −\nFollowing is the output of the above code −\nThese operation is done when we need to remove elements from front or back. When we remove an element from the front of the deque, we need to increment the \n pointer by one.\nSimilarly, when we remove an element from the back of the deque, we need to decrement the \n pointer by one.\nFollowing are the steps to remove an element from the front or back of the deque −\nFollowing is the implementation of pop_front() and pop_back() operations on deque.\n\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output is as follows −\nFollowing is the output of the above code −\nWhen we want to get the element from the front or back of the deque, we can use the \n and \n operations.\nFollowing are the steps to get the element from the front or back of the deque −\nFollowing is the implementation code of peek_front() and peek_back() operations on deque.\n\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output is as follows −\nFollowing is the output of the above code −\nThe time complexity of the deque operations is as follows −\nThus, the deque operations have a time complexity of O(1).\nSome of the applications of deque are as follows −\nIn summary, we use \n when we need to perform insertion and deletion operations at both ends of the data structure.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/priority_queue.htm", "title": "Data Structure - Priority Queue", "content": "Priority Queue is more specialized data structure than Queue. Like ordinary queue, priority queue has same method but with a major difference. In Priority queue items are ordered by key value so that item with the lowest value of key is at front and item with the highest value of key is at rear or vice versa. So we're assigned priority to item based on its key value. Lower the value, higher the priority. Following are the principal methods of a Priority Queue.\n − add an item to the rear of the queue.\n − remove an item from the front of the queue.\nWe're going to implement Queue using array in this article. There is few more operations supported by queue which are following.\n − get the element at front of the queue.\n − check if queue is full.\n − check if queue is empty.\nWhenever an element is inserted into queue, priority queue inserts the item according to its order. Here we're assuming that data with high value has low priority.\nWhenever an element is to be removed from queue, queue get the element using item count. Once element is removed. Item count is reduced by one.\nIf we compile and run the above program then it would produce following result −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/circular_queue_data_structure.htm", "title": "Circular Queue Data Structure", "content": "A queue is an abstract data structure that contains a collection of elements. Queue implements the FIFO mechanism i.e the element that is inserted first is also deleted first.\nQueue can be one linear data structure. But it may create some problem if we implement queue using array. Sometimes by using some consecutive insert and delete operation, the front and rear position will change. In that moment, it will look like the queue has no space to insert elements into it.\nEven if there are some free spaces, that will not be used due to some logical problems. To overcome this problem, we will use the circular queue data structure.\nA circular queue is a type of queue in which the last position is connected back to the first position to make a circle. It is also known as a Ring Buffer. In a normal queue, once the queue becomes full, we cannot insert the next element even if there is a space in front of the queue. But using a circular queue, we can use the space to insert elements.\nIt is a linear data structure that follows the FIFO mechanism. The circular queue is a more efficient way to implement a queue in a fixed size array. In a circular queue, the last element points to the first element making a circular link.\nFollowing is the representation of a circular queue, where the front is the index of the first element, and the rear is the index of the last element.\nThere are mainly four operations that can be performed on a circular queue:\nFollowing is the implementation of a circular queue using an array:\nEnqueue operation is used for inserting an element into the circular queue. The steps to perform the enqueue operation are as follows:\nFollowing are the steps to perform the enqueue operation on a circular queue:\nWe have provided the implementation of Enqueue operation on a circular queue using C, C++, Java, and Python. You can choose the language of your choice and view the code.\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output obtained is as shown below −\nFollowing is the output of the above code −\nDequeue operation is used for deleting an element from the circular queue. The steps to perform the dequeue operation are as follows:\nFollowing are the steps to perform the dequeue operation on a circular queue:\nWe have provided the implementation of Dequeue operation on a circular queue using C, C++, Java, and Python. You can choose the language of your choice and view the code.\nFollowing are the programs to remove a element from the circular queue −\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output obtained is as shown below −\nFollowing is the output of the above code −\nFront operation is used to get the front element from the circular queue. The steps to perform the front operation are as follows:\nWe have provided the implementation of Front operation on a circular queue using C, C++, Java, and Python. You can choose the language of your choice and view the code.\nFollowing are the programs to look at the front element of the circular queue −\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output obtained is as shown below −\nFollowing is the output of the above code −\nRear operation is used to get the last element from the circular queue. The steps to perform the rear operation are as follows:\nWe have provided the implementation of Rear operation on a circular queue using C, C++, Java, and Python. You can choose the language of your choice and view the code.\nFollowing are the programs to looka at the rear element−\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output obtained is as shown below −\nFollowing is the output of the above code −\nFollowing are the implementation of a circular queue using a linked list:\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output obtained is as shown below −\nFollowing is the output of the above code −\nFollowing are the time complexities of the circular queue:\nFollowing are some of the applications of a circular queue:\n Circular queues are used in memory management to manage the memory efficiently. It is used to allocate memory to the processes when they are in need of memory.\n These queues are also useful in buffer management. Consider a situation where data is produced at a faster rate than it is consumed. In such cases, a circular queue is used to store the data temporarily.\n Suppose your system has a lot of processes to execute. In such cases, for the better management of processes, the operating system uses a circular queue to allocate the CPU to the processes.\n Traffic singals are controlled by the circular queue. Let's say there are three signals, red, yellow, and green. The signals are in a circular queue. When the green signal is on, the next signal will be yellow, and then red. This process will continue in a circular manner.\n When we play songs in a multimedia player, the songs are played in a circular manner. Once the last song is played, the first song will be played next. This is done using a circular queue.\nIn general, when certain tasks are repeated in a circular manner, a circular queue is used to manage the tasks efficiently. Also, when the data is produced at a faster rate than it is consumed, a circular queue is used to store the data temporarily.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/dsa_queue.htm", "title": "Queue Data Structure", "content": "A \n is a linear data structure where elements are stored in the FIFO (First In First Out) principle where the first element inserted would be the first element to be accessed. A queue is an Abstract Data Type (ADT) similar to stack, the thing that makes queue different from stack is that a queue is open at both its ends. The data is inserted into the queue through one end and deleted from it using the other end. Queue is very frequently used in most programming languages.\nA real-world example of queue can be a single-lane one-way road, where the vehicle enters first, exits first. More real-world examples can be seen as queues at the ticket windows and bus-stops.\nSimilar to the stack ADT, a queue ADT can also be implemented using arrays, linked lists, or pointers. As a small example in this tutorial, we implement queues using a one-dimensional array.\nQueue operations also include initialization of a queue, usage and permanently deleting the data from the memory.\nThe most fundamental operations in the queue ADT include: enqueue(), dequeue(), peek(), isFull(), isEmpty(). These are all built-in operations to carry out data manipulation and to check the status of the queue.\nQueue uses two pointers − \n and \n. The front pointer accesses the data from the front end (helping in enqueueing) while the rear pointer accesses data from the rear end (helping in dequeuing).\nThe \n is a data manipulation operation that is used to insert elements into the stack. The following algorithm describes the enqueue() operation in a simpler way.\nFollowing are the implementations of this operation in various programming languages −\nThe \n is a data manipulation operation that is used to remove elements from the stack. The following algorithm describes the dequeue() operation in a simpler way.\nFollowing are the implementations of this operation in various programming languages −\nThe peek() is an operation which is used to retrieve the frontmost element in the queue, without deleting it. This operation is used to check the status of the queue with the help of the pointer.\nFollowing are the implementations of this operation in various programming languages −\nThe isFull() operation verifies whether the stack is full.\nFollowing are the implementations of this operation in various programming languages −\nThe isEmpty() operation verifies whether the stack is empty. This operation is used to check the status of the stack with the help of top pointer.\nFollowing are the implementations of this operation in various programming languages −\nFollowing are the complete implementations of Queue in various programming languages −\nClick to check the implementation of \n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/expression_parsing.htm", "title": "Expression Parsing in Data Structure", "content": "An expression is any word or group of words or symbols that generates a value on evaluation. Parsing expression means analyzing the expression for its words or symbols depending on a particular criterion. Expression parsing is a term used in a programming language to evaluate arithmetic and logical expressions.\nThe way to write arithmetic expression is known as a \n. An arithmetic expression can be written in three different but equivalent notations, i.e., without changing the essence or output of an expression. These notations are −\nThese notations are named as how they use operator in expression. We shall learn the same here in this chapter.\nWe write expression in \n notation, e.g. a - b &plus; c, where operators are used \n-between operands. It is easy for us humans to read, write, and speak in infix notation but the same does not go well with computing devices. An algorithm to process infix notation could be difficult and costly in terms of time and space consumption.\nIn this notation, operator is \ned to operands, i.e. operator is written ahead of operands. For example, \n. This is equivalent to its infix notation \n. Prefix notation is also known as \n.\nThis notation style is known as \n. In this notation style, the operator is \ned to the operands i.e., the operator is written after the operands. For example, \n. This is equivalent to its infix notation \n.\nThe following table briefly tries to show the difference in all three notations −\nAs we have discussed, it is not a very efficient way to design an algorithm or program to parse infix notations. Instead, these infix notations are first converted into either postfix or prefix notations and then computed.\nTo parse any arithmetic expression, we need to take care of operator precedence and associativity also.\nWhen an operand is in between two different operators, which operator will take the operand first, is decided by the precedence of an operator over others. For example −\nAs multiplication operation has precedence over addition, b * c will be evaluated first. A table of operator precedence is provided later.\nAssociativity describes the rule where operators with the same precedence appear in an expression. For example, in expression a &plus; b − c, both &plus; and − have the same precedence, then which part of the expression will be evaluated first, is determined by associativity of those operators. Here, both &plus; and − are left associative, so the expression will be evaluated as \n.\nPrecedence and associativity determines the order of evaluation of an expression. Following is an operator precedence and associativity table (highest to lowest) −\nThe above table shows the default behavior of operators. At any point of time in expression evaluation, the order can be altered by using parenthesis. For example −\nIn \n, the expression part \n*\n will be evaluated first, with multiplication as precedence over addition. We here use parenthesis for \n to be evaluated first, like \n.\nWe shall now look at the algorithm on how to evaluate postfix notation −\nFollowing are the complete implementations of Expression Parsing (Conversion from infix notations to postfix notations) in various programming languages −\nWe can use different data structures to implement expression parsing. Check the implementation of \n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/stack_algorithm.htm", "title": "Stack Data Structure", "content": "A stack is a \n where elements are stored in the LIFO (Last In First Out) principle where the last element inserted would be the first element to be deleted. A stack is an Abstract Data Type (ADT), that is popularly used in most programming languages. It is named stack because it has the similar operations as the real-world stacks, for example − a pack of cards or a pile of plates, etc.\nStack is considered a complex data structure because it uses other data structures for implementation, such as Arrays, Linked lists, etc.\nA stack allows all data operations at one end only. At any given time, we can only access the top element of a stack.\nThe following diagram depicts a stack and its operations −\nA stack can be implemented by means of Array, Structure, Pointer, and Linked List. Stack can either be a fixed size one or it may have a sense of dynamic resizing. Here, we are going to implement stack using arrays, which makes it a fixed size stack implementation.\nStack operations are usually performed for initialization, usage and, de-initialization of the stack ADT.\nThe most fundamental operations in the stack ADT include: push(), pop(), peek(), isFull(), isEmpty(). These are all built-in operations to carry out data manipulation and to check the status of the stack.\nStack uses pointers that always point to the topmost element within the stack, hence called as the \n pointer.\nThe push() is an operation that inserts elements into the stack. The following is an algorithm that describes the push() operation in a simpler way.\nFollowing are the implementations of this operation in various programming languages −\n − In Java we have used to built-in method \n to perform this operation.\nThe \n is a data manipulation operation which removes elements from the stack. The following pseudo code describes the pop() operation in a simpler way.\nFollowing are the implementations of this operation in various programming languages −\n − In Java we are using the built-in method pop().\nThe \n is an operation retrieves the topmost element within the stack, without deleting it. This operation is used to check the status of the stack with the help of the top pointer.\nFollowing are the implementations of this operation in various programming languages −\nThe \n operation checks whether the stack is full. This operation is used to check the status of the stack with the help of top pointer.\nFollowing are the implementations of this operation in various programming languages −\nThe \n operation verifies whether the stack is empty. This operation is used to check the status of the stack with the help of top pointer.\nFollowing are the implementations of this operation in various programming languages −\nFollowing are the complete implementations of Stack in various programming languages −\nClick to check the implementation of \n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/circular_linked_list_algorithm.htm", "title": "Circular Linked List Data Structure", "content": "is a variation of Linked list in which the first element points to the last element and the last element points to the first element. Both Singly Linked List and Doubly Linked List can be made into a circular linked list.\nIn singly linked list, the next pointer of the last node points to the first node.\nIn doubly linked list, the next pointer of the last node points to the first node and the previous pointer of the first node points to the last node making the circular in both directions.\nAs per the above illustration, following are the important points to be considered.\nThe last link's next points to the first link of the list in both cases of singly as well as doubly linked list.\nThe first link's previous points to the last of the list in case of doubly linked list.\nFollowing are the important operations supported by a circular list.\n − Inserts an element at the start of the list.\n − Deletes an element from the start of the list.\n − Displays the list.\nThe insertion operation of a circular linked list only inserts the element at the start of the list. This differs from the usual singly and doubly linked lists as there is no particular starting and ending points in this list. The insertion is done either at the start or after a particular node (or a given position) in the list.\nFollowing are the implementations of this operation in various programming languages −\nThe Deletion operation in a Circular linked list removes a certain node from the list. The deletion operation in this type of lists can be done at the beginning, or a given position, or at the ending.\nFollowing are the implementations of this operation in various programming languages −\nThe Display List operation visits every node in the list and prints them all in the output.\nFollowing are the implementations of this operation in various programming languages −\nFollowing are the complete implementations of Circular Linked List in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/doubly_linked_list_algorithm.htm", "title": "Doubly Linked List Data Structure", "content": "Doubly Linked List is a variation of Linked list in which navigation is possible in both ways, forward as well as backward easily as compared to Single Linked List. Following are the important terms to understand the concept of doubly linked list.\n − Each link of a linked list can store a data called an element.\n − Each link of a linked list contains a link to the next link called Next.\n − Each link of a linked list contains a link to the previous link called Prev.\n − A Linked List contains the connection link to the first link called First and to the last link called Last.\nAs per the above illustration, following are the important points to be considered.\nDoubly Linked List contains a link element called first and last.\nEach link carries a data field(s) and a link field called next.\nEach link is linked with its next link using its next link.\nEach link is linked with its previous link using its previous link.\nThe last link carries a link as null to mark the end of the list.\nFollowing are the basic operations supported by a list.\n − Adds an element at the beginning of the list.\n − Adds an element at the end of the list.\n − Adds an element after an item of the list.\n − Deletes an element at the beginning of the list.\n − Deletes an element from the end of the list.\n − Deletes an element from the list using the key.\n − Displays the complete list in a forward manner.\n − Displays the complete list in a backward manner.\nIn this operation, we create a new node with three compartments, one containing the data, the others containing the address of its previous and next nodes in the list. This new node is inserted at the beginning of the list.\nFollowing are the implementations of this operation in various programming languages −\nIn this insertion operation, the new input node is added at the end of the doubly linked list; if the list is not empty. The head will be pointed to the new node, if the list is empty.\nFollowing are the implementations of this operation in various programming languages −\nThis deletion operation deletes the existing first nodes in the doubly linked list. The head is shifted to the next node and the link is removed.\nFollowing are the implementations of this operation in various programming languages −\nFollowing are the complete implementations of Doubly Linked List in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/linked_list_algorithms.htm", "title": "Linked List Data Structure", "content": "A linked list is a linear data structure which can store a collection of \"nodes\" connected together via links i.e. pointers. Linked lists nodes are not stored at a contiguous location, rather they are linked using pointers to the different memory locations.  A node consists of the data value and a pointer to the address of the next node within the linked list.\nA linked list is a dynamic linear data structure whose memory size can be allocated or de-allocated at run time based on the operation insertion or deletion, this helps in using system memory efficiently. Linked lists can be used to implment various data structures like a stack, queue, graph, hash maps, etc.\nA linked list starts with a \n node which points to the first node. Every node consists of data which holds the actual data (value) associated with the node and a next pointer which holds the memory address of the next node in the linked list. The last node is called the tail node in the list which points to \n indicating the end of the list.\nIn case of arrays, the size is given at the time of creation and so arrays are of fixed lenghth where as Linked lists are dynamic in size and any number of nodes can be added in the linked lists dynamically. An array can accommodate similar types of data types where as linked lists can store various nodes of different data types.\nFollowing are the various types of linked list.\nSingly linked lists contain two \"buckets\" in one node; one bucket holds the data and the other bucket holds the address of the next node of the list. Traversals can be done in one direction only as there is only a single link between two nodes of the same list.\nDoubly Linked Lists contain three \"buckets\" in one node; one bucket holds the data and the other buckets hold the addresses of the previous and next nodes in the list. The list is traversed twice as the nodes in the list are connected to each other from both sides.\nCircular linked lists can exist in both singly linked list and doubly linked list.\nSince the last node and the first node of the circular linked list are connected, the traversal in this linked list will go on forever until it is broken.\nThe basic operations in the linked lists are insertion, deletion, searching, display, and deleting an element at a given key. These operations are performed on Singly Linked Lists as given below −\n − Adds an element at the beginning of the list.\n − Deletes an element at the beginning of the list.\n − Displays the complete list.\n − Searches an element using the given key.\n − Deletes an element using the given key.\nAdding a new node in linked list is a more than one step activity. We shall learn this with diagrams here. First, create a node using the same structure and find the location where it has to be inserted.\nImagine that we are inserting a node B (NewNode), between A (LeftNode) and C (RightNode). Then point B.next to C −\nIt should look like this −\nNow, the next node at the left should point to the new node.\nThis will put the new node in the middle of the two. The new list should look like this −\nInsertion in linked list can be done in three different ways. They are explained as follows −\nIn this operation, we are adding an element at the beginning of the list.\nFollowing are the implementations of this operation in various programming languages −\nIn this operation, we are adding an element at the ending of the list.\nFollowing are the implementations of this operation in various programming languages −\nIn this operation, we are adding an element at any position within the list.\nFollowing are the implementations of this operation in various programming languages −\nDeletion is also a more than one step process. We shall learn with pictorial representation. First, locate the target node to be removed, by using searching algorithms.\nThe left (previous) node of the target node now should point to the next node of the target node −\nThis will remove the link that was pointing to the target node. Now, using the following code, we will remove what the target node is pointing at.\nWe need to use the deleted node. We can keep that in memory otherwise we can simply deallocate memory and wipe off the target node completely.\nSimilar steps should be taken if the node is being inserted at the beginning of the list. While inserting it at the end, the second last node of the list should point to the new node and the new node will point to NULL.\nDeletion in linked lists is also performed in three different ways. They are as follows −\nIn this deletion operation of the linked, we are deleting an element from the beginning of the list. For this, we point the head to the second node.\nFollowing are the implementations of this operation in various programming languages −\nIn this deletion operation of the linked, we are deleting an element from the ending of the list.\nFollowing are the implementations of this operation in various programming languages −\nIn this deletion operation of the linked, we are deleting an element at any position of the list.\nFollowing are the implementations of this operation in various programming languages −\nThis operation is a thorough one. We need to make the last node to be pointed by the head node and reverse the whole linked list.\nFirst, we traverse to the end of the list. It should be pointing to NULL. Now, we shall make it point to its previous node −\nWe have to make sure that the last node is not the last node. So we'll have some temp node, which looks like the head node pointing to the last node. Now, we shall make all left side nodes point to their previous nodes one by one.\nExcept the node (first node) pointed by the head node, all nodes should point to their predecessor, making them their new successor. The first node will point to NULL.\nWe'll make the head node point to the new first node by using the temp node.\nStep by step process to reverse a linked list is as follows −\nFollowing are the implementations of this operation in various programming languages −\nSearching for an element in the list using a key element. This operation is done in the same way as array search; comparing every element in the list with the key element given.\nFollowing are the implementations of this operation in various programming languages −\nThe traversal operation walks through all the elements of the list in an order and displays the elements in that order.\nFollowing are the implementations of this operation in various programming languages −\nFollowing are the complete implementations of Linked List in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/skip_list_data_structure.htm", "title": "Skip List: Alternative to Balanced Trees", "content": "is an advanced data structure that is used for storing a sorted list of elements.\nLet's suppose, you are working on a project called \"Music Player\". You have a list of songs that you want to play in a specific order. You want to store these songs in a data structure that allows you to quickly search for a song, insert a new song, and delete a song. You also want to maintain the order of the songs. This is where the \n data structure comes in use.\nA \n is a data structure that help us to search, insert, and delete elements in a sorted list. It is similar to a linked list, but with additional pointers that allow us to skip over some elements. This makes searching for an element faster than a linked list.\nA Skip List is a probabilistic data structure, which means that the structure of the list is determined by random choices. The Skip List is a versatile data structure that can be used in a variety of applications, such as databases, search engines, and network routing algorithms.\nSkip list has multiple levels. The bottom level is a sorted linked list that contains all the elements. Then, each higher level contains a subset of the elements from the lower level. The higher levels have pointers that allow us to skip over some elements. This makes searching for an element faster than a linked list. \nA Skip List is represented as a series of linked lists, where each list is a level of the Skip List. Each node in the Skip List contains a key and a value. The key is used to sort the elements in the list, and the value is the data associated with the key.\nEach node also contains pointers to the next node in the same level, and pointers to the next node in the level below. The top level of the Skip List contains only one node, which is the head of the list. The head node contains pointers to the first node in each level of the Skip List.\nThere are more than one type of Skip Lists:\nA Skip List supports the following operations:\nA Skip List can be implemented using a linked list data structure. Each node in the Skip List contains a key, a value, and an array of pointers to the next node in each level. The Skip List also contains a head node that points to the first node in each level. Here is an example of a Skip List implemented in C, C++, Java and Python:\nIn order to implement a Skip List, we need to follow these steps:\nFollowing is an example of a Skip List implemented in C, C++, Java, and Python that demonstrates the insertion of elements into the Skip List and displays the Skip List:\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output is as follows −\nFollowing is the output of the above code −\n output may vary as the level of the Skip List is generated randomly.\nThe search operation in a Skip List is similar to a linked list. We start from the top level and move to the next level if the key is greater than the current node. We continue this process until we find the key or reach the bottom level. If the key is found, we return the node; otherwise, we return NULL.\nFollowing are steps to search for a key in a Skip List:\nLet's search for the key 12 in the Skip List. The search operation is as follows:\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output is as follows −\nFollowing is the output of the above code −\nThe delete operation in a Skip List is similar to a linked list. We search for the key to be deleted and update the pointers to remove the node from the list. The delete operation is as follows:\nFollowing are the steps to delete a key in a Skip List:\nLet's delete the key 12 from the Skip List. The delete operation is as follows:\nThe output obtained is as follows −\nThe output produced is as follows −\nThe output is as follows −\nFollowing is the output of the above code −\nThe Skip List data structure is used in the following applications:\nFollowing are the advantages of Skip List −\nFollowing are the disadvantages of Skip List −\nIn this tutorial, we learned about the Skip List data structure, its operations, and its implementation in C, C++, Java, and Python. Skip Lists are efficient data structures that provide fast search, insert, and delete operations. Skip Lists are used in various applications like database indexing, search engines, and network routing algorithms.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/array_data_structure.htm", "title": "Array Data Structure", "content": "An array is a type of linear data structure that is defined as a collection of elements with same or different data types. They exist in both single dimension and multiple dimensions. These data structures come into picture when there is a necessity to store multiple elements of similar nature together at one place.\nThe difference between an array index and a memory address is that the array index acts like a key value to label the elements in the array. However, a memory address is the starting address of free memory available.\nFollowing are the important terms to understand the concept of Array.\n − Each item stored in an array is called an element.\n − Each location of an element in an array has a numerical index, which is used to identify the element.\nCreating an array in \n and \n programming languages −\nCreating an array in \n programming language −\nArrays are used as solutions to many problems from the small sorting problems to more complex problems like travelling salesperson problem. There are many data structures other than arrays that provide efficient time and space complexity for these problems, so what makes using arrays better? The answer lies in the random access lookup time.\nArrays provide \n random access lookup time. That means, accessing the 1\n index of the array and the 1000\n index of the array will both take the same time. This is due to the fact that array comes with a pointer and an offset value. The pointer points to the right location of the memory and the offset value shows how far to look in the said memory.\nTherefore, in an array with 6 elements, to access the 1st element, array is pointed towards the 0th index. Similarly, to access the 6\n element, array is pointed towards the 5\n index.\nArrays are represented as a collection of buckets where each bucket stores one element. These buckets are indexed from '0' to 'n-1', where n is the size of that particular array. For example, an array with size 10 will have buckets indexed from 0 to 9.\nThis indexing will be similar for the multidimensional arrays as well. If it is a 2-dimensional array, it will have sub-buckets in each bucket. Then it will be indexed as array_name[m][n], where m and n are the sizes of each level in the array.\nAs per the above illustration, following are the important points to be considered.\nIndex starts with 0.\nArray length is 9 which means it can store 9 elements.\nEach element can be accessed via its index. For example, we can fetch an element at index 6 as 23.\nThe basic operations in the Arrays are insertion, deletion, searching, display, traverse, and update. These operations are usually performed to either modify the data in the array or to report the status of the array.\nFollowing are the basic operations supported by an array.\n − print all the array elements one by one.\n − Adds an element at the given index.\n − Deletes an element at the given index.\n − Searches an element using the given index or by the value.\n − Updates an element at the given index.\n − Displays the contents of the array.\nIn C, when an array is initialized with size, then it assigns defaults values to its elements in following order.\nIn the insertion operation, we are adding one or more elements to the array. Based on the requirement, a new element can be added at the beginning, end, or any given index of array. This is done using input statements of the programming languages.\nFollowing is an algorithm to insert elements into a Linear Array until we reach the end of the array −\nHere, we see a practical implementation of insertion operation, where we add data at the end of the array −\nFor other variations of array insertion operation, \n.\nIn this array operation, we delete an element from the particular index of an array. This deletion operation takes place as we assign the value in the consequent index to the current index.\nConsider LA is a linear array with N elements and K is a positive integer such that K<=N. Following is the algorithm to delete an element available at the K\n position of LA.\nFollowing are the implementations of this operation in various programming languages −\nSearching an element in the array using a key; The key element sequentially compares every value in the array to check if the key is present in the array or not.\nConsider LA is a linear array with N elements and K is a positive integer such that K<=N. Following is the algorithm to find an element with a value of ITEM using sequential search.\nFollowing are the implementations of this operation in various programming languages −\nThis operation traverses through all the elements of an array. We use loop statements to carry this out.\nFollowing is the algorithm to traverse through all the elements present in a Linear Array −\nFollowing are the implementations of this operation in various programming languages −\nUpdate operation refers to updating an existing element from the array at a given index.\nConsider LA is a linear array with N elements and K is a positive integer such that K<=N. Following is the algorithm to update an element available at the Kth position of LA.\nFollowing are the implementations of this operation in various programming languages −\nThis operation displays all the elements in the entire array using a print statement.\nConsider LA is a linear array with N elements. Following is the algorithm to display an array elements.\nFollowing are the implementations of this operation in various programming languages −\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/data_structures_and_types.htm", "title": "Data Structures and Types", "content": "are introduced in order to store, organize and manipulate data in programming languages. They are designed in a way that makes accessing and processing of the data a little easier and simpler. These data structures are not confined to one particular programming language; they are just pieces of code that structure data in the memory.\nData types are often confused as a type of data structures, but it is not precisely correct even though they are referred to as Abstract Data Types. Data types represent the nature of the data while data structures are just a collection of similar or different data types in one.\nThere are usually just two types of data structures −\nLinear\nNon-Linear\nThe data is stored in linear data structures sequentially. These are rudimentary structures since the elements are stored one after the other without applying any mathematical operations.\nLinear data structures are usually easy to implement but since the memory allocation might become complicated, time and space complexities increase. Few examples of linear data structures include −\nArrays\nLinked Lists\nStacks\nQueues\nBased on the data storage methods, these linear data structures are divided into two sub-types. They are − \n and \n data structures.\nIn Static Linear Data Structures, the memory allocation is not scalable. Once the entire memory is used, no more space can be retrieved to store more data. Hence, the memory is required to be reserved based on the size of the program. This will also act as a drawback since reserving more memory than required can cause a wastage of memory blocks.\nThe best example for static linear data structures is an array.\nIn Dynamic linear data structures, the memory allocation can be done dynamically when required. These data structures are efficient considering the space complexity of the program.\nFew examples of dynamic linear data structures include: linked lists, stacks and queues.\nNon-Linear data structures store the data in the form of a hierarchy. Therefore, in contrast to the linear data structures, the data can be found in multiple levels and are difficult to traverse through.\nHowever, they are designed to overcome the issues and limitations of linear data structures. For instance, the main disadvantage of linear data structures is the memory allocation. Since the data is allocated sequentially in linear data structures, each element in these data structures uses one whole memory block. However, if the data uses less memory than the assigned block can hold, the extra memory space in the block is wasted. Therefore, non-linear data structures are introduced. They decrease the space complexity and use the memory optimally.\nFew types of non-linear data structures are −\nGraphs\nTrees\nTries\nMaps\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/data_structures_basics.htm", "title": "Data Structure Basics", "content": "This tutorial explains the basic terms related to data structure.\nData Definition defines a particular data with the following characteristics.\n − Definition should define a single concept.\n − Definition should be able to be mapped to some data element.\n − Definition should be unambiguous.\n − Definition should be understandable.\nData Object represents an object having a data.\nData type is a way to classify various types of data such as integer, string, etc. which determines the values that can be used with the corresponding type of data, the type of operations that can be performed on the corresponding type of data. There are two data types −\nThose data types for which a language has built-in support are known as Built-in Data types. For example, most of the languages provide the following built-in data types.\nThose data types which are implementation independent as they can be implemented in one or the other way are known as derived data types. These data types are normally built by the combination of primary or built-in data types and associated operations on them. For example −\nThe data in the data structures are processed by certain operations. The particular data structure chosen largely depends on the frequency of the operation that needs to be performed on the data structure.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/asymptotic_analysis.htm", "title": "Data Structures - Asymptotic Analysis", "content": "Asymptotic analysis of an algorithm refers to defining the mathematical foundation/framing of its run-time performance. Using asymptotic analysis, we can very well conclude the best case, average case, and worst case scenario of an algorithm.\nAsymptotic analysis is input bound i.e., if there's no input to the algorithm, it is concluded to work in a constant time. Other than the \"input\" all other factors are considered constant.\nAsymptotic analysis refers to computing the running time of any operation in mathematical units of computation. For example, the running time of one operation is computed as f(n) and may be for another operation it is computed as \n(n\n). This means the first operation running time will increase linearly with the increase in \n and the running time of the second operation will increase exponentially when \n increases. Similarly, the running time of both operations will be nearly the same if \n is significantly small.\nUsually, the time required by an algorithm falls under three types −\n − Minimum time required for program execution.\n − Average time required for program execution.\n − Maximum time required for program execution.\nExecution time of an algorithm depends on the instruction set, processor speed, disk I/O speed, etc. Hence, we estimate the efficiency of an algorithm asymptotically.\nTime function of an algorithm is represented by \n, where \n is the input size.\nDifferent types of asymptotic notations are used to represent the complexity of an algorithm. Following asymptotic notations are used to calculate the running time complexity of an algorithm.\n − Big Oh Notation\n − Big omega Notation\n − Big theta Notation\n − Little Oh Notation\n − Little omega Notation\nThe notation (n) is the formal way to express the upper bound of an algorithm's running time. is the most commonly used notation. It measures the \n or the longest amount of time an algorithm can possibly take to complete.\nA function \n can be represented is the order of \n that is \n, if there exists a value of positive integer \n as \n and a positive constant \n such that −\n$f(n)\\leqslant c.g(n)$ for $n > n_{0}$ in all case\nHence, function \n is an upper bound for function \n, as \n grows faster than \n.\nLet us consider a given function, $f(n) = 4.n^3 + 10.n^2 + 5.n + 1$\nConsidering $g(n) = n^3$,\n$f(n)\\leqslant 5.g(n)$ for all the values of $n > 2$\nHence, the complexity of \n can be represented as $O(g(n))$, i.e. $O(n^3)$\nThe notation Ω(n) is the formal way to express the lower bound of an algorithm's running time. It measures the \n or the best amount of time an algorithm can possibly take to complete.\nWe say that $f(n) = \\Omega (g(n))$ when there exists constant \n that $f(n)\\geqslant c.g(n)$ for all sufficiently large value of \n. Here \n is a positive integer. It means function \n is a lower bound for function \n ; after a certain value of \n will never go below \n.\nLet us consider a given function, $f(n) = 4.n^3 + 10.n^2 + 5.n + 1$.\nConsidering $g(n) = n^3$, $f(n)\\geqslant 4.g(n)$ for all the values of $n > 0$.\nHence, the complexity of \n can be represented as $\\Omega (g(n))$, i.e. $\\Omega (n^3)$\nThe notation (n) is the formal way to express both the lower bound and the upper bound of an algorithm's running time. Some may confuse the theta notation as the average case time complexity; while big theta notation could be \n accurately used to describe the average case, other notations could be used as well.\nWe say that $f(n) = \\theta(g(n))$ when there exist constants \n and \n that $c_{1}.g(n) \\leqslant f(n) \\leqslant c_{2}.g(n)$ for all sufficiently large value of \n. Here \n is a positive integer.\nThis means function \n is a tight bound for function \n.\nLet us consider a given function, $f(n) = 4.n^3 + 10.n^2 + 5.n + 1$\nConsidering $g(n) = n^3$, $4.g(n) \\leqslant f(n) \\leqslant 5.g(n)$ for all the large values of \n.\nHence, the complexity of \n can be represented as $\\theta (g(n))$, i.e. $\\theta (n^3)$.\nThe asymptotic upper bound provided by \n may or may not be asymptotically tight. The bound $2.n^2 = O(n^2)$ is asymptotically tight, but the bound $2.n = O(n^2)$ is not.\nWe use \n to denote an upper bound that is not asymptotically tight.\nWe formally define \n (little-oh of g of n) as the set \n for any positive constant $c > 0$ and there exists a value $n_{0} > 0$, such that $0 \\leqslant f(n) \\leqslant c.g(n)$.\nIntuitively, in the \n, the function \n becomes insignificant relative to \n as \n approaches infinity; that is,\n$$\\lim_{n \\rightarrow \\infty}\\left(\\frac{f(n)}{g(n)}\\right) = 0$$\nLet us consider the same function, $f(n) = 4.n^3 + 10.n^2 + 5.n + 1$\nConsidering $g(n) = n^{4}$,\n$$\\lim_{n \\rightarrow \\infty}\\left(\\frac{4.n^3 + 10.n^2 + 5.n + 1}{n^4}\\right) = 0$$\nHence, the complexity of \n can be represented as $o(g(n))$, i.e. $o(n^4)$.\nWe use \n to denote a lower bound that is not asymptotically tight. Formally, however, we define \n (little-omega of g of n) as the set \n for any positive constant \n and there exists a value $n_{0} > 0$, such that $0 \\leqslant c.g(n) \n\nFor example, $\\frac{n^2}{2} = \\omega (n)$, but $\\frac{n^2}{2} \\neq \\omega (n^2)$. The relation $f(n) = \\omega (g(n))$ implies that the following limit exists\n$$\\lim_{n \\rightarrow \\infty}\\left(\\frac{f(n)}{g(n)}\\right) = \\infty$$\nThat is, \n becomes arbitrarily large relative to \n as \n approaches infinity.\nLet us consider same function, $f(n) = 4.n^3 + 10.n^2 + 5.n + 1$\nConsidering $g(n) = n^2$,\n$$\\lim_{n \\rightarrow \\infty}\\left(\\frac{4.n^3 + 10.n^2 + 5.n + 1}{n^2}\\right) = \\infty$$\nHence, the complexity of \n can be represented as $o(g(n))$, i.e. $\\omega (n^2)$.\nFollowing is a list of some common asymptotic notations −\nApriori analysis means, analysis is performed prior to running it on a specific system. This analysis is a stage where a function is defined using some theoretical model. Hence, we determine the time and space complexity of an algorithm by just looking at the algorithm rather than running it on a particular system with a different memory, processor, and compiler.\nApostiari analysis of an algorithm means we perform analysis of an algorithm only after running it on a system. It directly depends on the system and changes from system to system.\nIn an industry, we cannot perform Apostiari analysis as the software is generally made for an anonymous user, which runs it on a system different from those present in the industry.\nIn Apriori, it is the reason that we use asymptotic notations to determine time and space complexity as they change from computer to computer; however, asymptotically they are the same.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/algorithms_basics.htm", "title": "Data Structures - Algorithms Basics", "content": "Algorithm is a step-by-step procedure, which defines a set of instructions to be executed in a certain order to get the desired output. Algorithms are generally created independent of underlying languages, i.e. an algorithm can be implemented in more than one programming language.\nFrom the data structure point of view, following are some important categories of algorithms −\n − Algorithm to search an item in a data structure.\n − Algorithm to sort items in a certain order.\n − Algorithm to insert item in a data structure.\n −  Algorithm to update an existing item in a data structure.\n − Algorithm to delete an existing item from a data structure.\nNot all procedures can be called an algorithm. An algorithm should have the following characteristics −\n − Algorithm should be clear and unambiguous. Each of its steps (or phases), and their inputs/outputs should be clear and must lead to only one meaning.\n − An algorithm should have 0 or more well-defined inputs.\n − An algorithm should have 1 or more well-defined outputs, and should match the desired output.\n − Algorithms must terminate after a finite number of steps.\n − Should be feasible with the available resources.\n − An algorithm should have step-by-step directions, which should be independent of any programming code.\nThere are no well-defined standards for writing algorithms. Rather, it is problem and resource dependent. Algorithms are never written to support a particular programming code.\nAs we know that all programming languages share basic code constructs like loops (do, for, while), flow-control (if-else), etc. These common constructs can be used to write an algorithm.\nWe write algorithms in a step-by-step manner, but it is not always the case. Algorithm writing is a process and is executed after the problem domain is well-defined. That is, we should know the problem domain, for which we are designing a solution.\nLet's try to learn algorithm-writing by using an example.\n − Design an algorithm to add two numbers and display the result.\nAlgorithms tell the programmers how to code the program. Alternatively, the algorithm can be written as −\nIn design and analysis of algorithms, usually the second method is used to describe an algorithm. It makes it easy for the analyst to analyze the algorithm ignoring all unwanted definitions. He can observe what operations are being used and how the process is flowing.\nWriting \n, is optional.\nWe design an algorithm to get a solution of a given problem. A problem can be solved in more than one ways.\nHence, many solution algorithms can be derived for a given problem. The next step is to analyze those proposed solution algorithms and implement the best suitable solution.\nEfficiency of an algorithm can be analyzed at two different stages, before implementation and after implementation. They are the following −\n − This is a theoretical analysis of an algorithm. Efficiency of an algorithm is measured by assuming that all other factors, for example, processor speed, are constant and have no effect on the implementation.\n − This is an empirical analysis of an algorithm. The selected algorithm is implemented using programming language. This is then executed on target computer machine. In this analysis, actual statistics like running time and space required, are collected.\nWe shall learn about \n algorithm analysis. Algorithm analysis deals with the execution or running time of various operations involved. The running time of an operation can be defined as the number of computer instructions executed per operation.\nSuppose \n is an algorithm and \n is the size of input data, the time and space used by the algorithm X are the two main factors, which decide the efficiency of X.\n − Time is measured by counting the number of key operations such as comparisons in the sorting algorithm.\n − Space is measured by counting the maximum memory space required by the algorithm.\nThe complexity of an algorithm \n gives the running time and/or the storage space required by the algorithm in terms of \n as the size of input data.\nSpace complexity of an algorithm represents the amount of memory space required by the algorithm in its life cycle. The space required by an algorithm is equal to the sum of the following two components −\nA fixed part that is a space required to store certain data and variables, that are independent of the size of the problem. For example, simple variables and constants used, program size, etc.\nA variable part is a space required by variables, whose size depends on the size of the problem. For example, dynamic memory allocation, recursion stack space, etc.\nSpace complexity S(P) of any algorithm P is S(P) = C &plus; SP(I), where C is the fixed part and S(I) is the variable part of the algorithm, which depends on instance characteristic I. Following is a simple example that tries to explain the concept −\nHere we have three variables A, B, and C and one constant. Hence S(P) = 1 &plus; 3. Now, space depends on data types of given variables and constant types and it will be multiplied accordingly.\nTime complexity of an algorithm represents the amount of time required by the algorithm to run to completion. Time requirements can be defined as a numerical function T(n), where T(n) can be measured as the number of steps, provided each step consumes constant time.\nFor example, addition of two n-bit integers takes \n steps. Consequently, the total computational time is T(n) = c ∗ n, where c is the time taken for the addition of two bits. Here, we observe that T(n) grows linearly as the input size increases.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/data_structure_environment.htm", "title": "Data Structures & Algorithms - Environment Setup", "content": "We have setup an online environment for you to compile and run your data Structure and Algorithms programs in four different programming languages: C, C++, Java, Python.\nIf you are still willing to set up your own environment for C programming language, you need the following two tools available on your computer, (a) Text Editor and (b) The C Compiler.\nThis will be used to type your program. Examples of few editors include Windows Notepad, OS Edit command, Brief, Epsilon, EMACS, and vim or vi.\nThe name and the version of the text editor can vary on different operating systems. For example, Notepad will be used on Windows, and vim or vi can be used on Windows as well as Linux or UNIX.\nThe files you create with your editor are called source files and contain program source code. The source files for C programs are typically named with the extension \"\n\".\nBefore starting your programming, make sure you have one text editor in place and you have enough experience to write a computer program, save it in a file, compile it, and finally execute it.\nThe source code written in the source file is the human readable source for your program. It needs to be \"compiled\", to turn into machine language so that your CPU can actually execute the program as per the given instructions.\nThis C programming language compiler will be used to compile your source code into a final executable program. We assume you have the basic knowledge about a programming language compiler.\nMost frequently used and free available compiler is GNU C/C&plus;&plus; compiler. Otherwise, you can have compilers either from HP or Solaris if you have respective Operating Systems (OS).\nThe following section guides you on how to install GNU C/C&plus;&plus; compiler on various OS. We are mentioning C/C&plus;&plus; together because GNU GCC compiler works for both C and C&plus;&plus; programming languages.\nIf you are using \n, then check whether GCC is installed on your system by entering the following command from the command line −\nIf you have GNU compiler installed on your machine, then it should print a message such as the following −\nIf GCC is not installed, then you will have to install it yourself using the detailed instructions available at \nThis tutorial has been written based on Linux and all the given examples have been compiled on Cent OS flavor of Linux system.\nIf you use Mac OS X, the easiest way to obtain GCC is to download the Xcode development environment from Apple's website and follow the simple installation instructions. Once you have Xcode setup, you will be able to use GNU compiler for C/C&plus;&plus;.\nXcode is currently available at \nTo install GCC on Windows, you need to install MinGW. To install MinGW, go to the MinGW homepage, \n, and follow the link to the MinGW download page. Download the latest version of the MinGW installation program, which should be named MinGW-<version>.exe.\nWhile installing MinWG, at a minimum, you must install gcc-core, gcc-g&plus;&plus;, binutils, and the MinGW runtime, but you may wish to install more.\nAdd the bin subdirectory of your MinGW installation to your \n environment variable, so that you can specify these tools on the command line by their simple names.\nWhen the installation is complete, you will be able to run gcc, g&plus;&plus;, ar, ranlib, dlltool, and several other GNU tools from the Windows command line.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."},
{"url": "https://www.tutorialspoint.com/data_structures_algorithms/data_structure_overview.htm", "title": "Data Structures & Algorithms - Overview", "content": "Data Structure is a systematic way to organize data in order to use it efficiently. Following terms are the foundation terms of a data structure.\n − Each data structure has an interface. Interface represents the set of operations that a data structure supports. An interface only provides the list of supported operations, type of parameters they can accept and return type of these operations.\n − Implementation provides the internal representation of a data structure. Implementation also provides the definition of the algorithms used in the operations of the data structure.\nHere are different type of data structures which we are going to learn in this tutorial:\nAlgorithm is a step-by-step procedure, which defines a set of instructions to be executed in a certain order to get the desired output. Algorithms are generally created independent of underlying languages, i.e. an algorithm can be implemented in more than one programming language.\nHere are different type of algorithms which we are going to learn in this tutorial: \n − Data structure implementation should implement its interface correctly.\n − Running time or the execution time of operations of data structure must be as small as possible.\n − Memory usage of a data structure operation should be as little as possible.\nThere are three cases which are usually used to compare various data structure's execution time in a relative manner.\n − This is the scenario where a particular data structure operation takes maximum time it can take. If an operation's worst case time is ƒ(n) then this operation will not take more than ƒ(n) time where ƒ(n) represents function of n.\n − This is the scenario depicting the average execution time of an operation of a data structure. If an operation takes ƒ(n) time in execution, then m operations will take mƒ(n) time.\n − This is the scenario depicting the least possible execution time of an operation of a data structure. If an operation takes ƒ(n) time in execution, then the actual operation may take time as the random number which would be maximum as ƒ(n).\n − Data are values or set of values.\n − Data item refers to single unit of values.\n − Data items that are divided into sub items are called as Group Items.\n − Data items that cannot be divided are called as Elementary Items.\n − An entity is that which contains certain attributes or properties, which may be assigned values.\n − Entities of similar attributes form an entity set.\n − Field is a single elementary unit of information representing an attribute of an entity.\n − Record is a collection of field values of a given entity.\n − File is a collection of records of the entities in a given entity set.\n\n               Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical\n               and non-technical subjects.\n            \n© Copyright 2025. All Rights Reserved."}
]