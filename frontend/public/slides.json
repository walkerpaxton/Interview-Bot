[
{"file": "dsa_slides/NotesKnapsack.pdf", "content": "CS 3510 Algorithms 2/29/2024 Lecture 12: Knapsack Lecturer: Abrahim Ladha Scribe(s): Adam Zamlynny Knapsack is a core problem to Dynamic Programming, and it’s pretty easy to under- stand. Knapsack is a hard problem though; we don’t have or believe there is a polynomial time solution. It’s also a good segway into the next unit because there are many problems like it. 1 Problem Suppose you are given n items (v ,w ) where v is the value of item i and w is the weight i i i i of item i. You have a capacity W. Your goal is steal a subset of the items to maximize the value subject to having the sum of the weights be less than or equal to the capacity. As an example consider if you have W = 6,w = (1,2,3,4),v = (12,17,18,25). 1 If i i you picked one and two, you’d have a value of 29. If you picked two and three, you’d have a value of 35 which is better than if you’d picked one and two. If you picked using a greedy strategy, you’d choose four and two which would give you 42. The optimal solution to this is picking one two and three which will give you 47. You could also brute force this by analyzing all 2n subsets. 2 Solution Let’s start with a two dimensional solution. T[i,j] needs to be an intermediary version of some capacity and some item, so that at the end we could index at T[W,n] giving us the maximum value for the weight and considering all items. Then let’s define our table T[i,j] where T[i,j] is the maximum profit with a capacity i, and only considering items 1...j. In this variant of knapsack, there is only one copy of an item. Steal or no steal. Then iteratively we can build up the smaller cells in the table to the bigger items. For our recurrence, let’s consider what the last possible step is in the process. You choose or don’t choose the nth item. If you don’t pick the item, the solution is same as if you had never considered the item. Your value will be the element of the table with the same capacity except for the item (T[W,n] = T[W,n−1]). If you do pick the item, your capacity goes up by the value of w , so your capacity before this should be W −w , then n n your new value is T[W,n] = T[W −w ,n−1]+v . This is essentially saying your solution n n to the optimal value at some weight is the optimal solution with the capacity minus the weight of some new item plus the value of that item, or the weight ignoring that item. (cid:26) (cid:27) T[i,j −1], if w > i T[i,j] = j max(T[i,j −1],T[i−w ,j −1]+v ), if w ≤ i j j j 1The weights are just indices here, but that’s not always true. 12: Knapsack-1\nIf you notice, in the chain matrix, we have a minimum over some things, but here we have the maximum over some things. Similarly, we’re not going to end up finding the items we’ve used using this table, but like in chain matrix, all it requires is storing pointers to which choices you made. This variant of knapsack is also called 0-1 Knapsack since you can only choose 0 or 1 amounts of any item. Algorithm 1 Knapsack No Repeat T ← table indexed from 0 to W and 0 to n ▷ Create the table while j in 0..n do ▷ Setup base cases T[0,j] = 0 end while while i in 0..W do T[i,0] = 0 end while while j in 1..n do ▷ Fill in the table while i in 1..W do if w ≤ i then j T[i,j] = max(T[i,j −1],T[i−w ,j −1]+v ) j j else T[i,j] = T[i,j −1] end if end while end while return T[W,n] The space of this is O(nW). If you look at this though, you might see that the runtime is dependent on the capacity which is a number. The size of the capacity is dependent on the size of the bits, making the runtime exponential in terms in the bits. W is given as input as k bits, so in terms of the size of the input the complexity is really O(2kn). Of course this seems like an easy problem, but there is currently no known solution to this problem which takes polynomial time with respect to the size of the input. 3 Example Suppose we have the weights we had at the beginning. We’d create the following table, and we’d fill in our base cases. 0 1 2 3 4 0 0 0 0 0 0 1 0 2 0 3 0 4 0 5 0 6 0 12: Knapsack-2\n0 1 2 3 4 0 0 0 0 0 0 1 0 12 12 12 12 2 0 12 17 17 17 3 0 12 29 29 29 4 0 12 29 30 30 5 0 12 29 35 37 6 0 12 29 47 47 For the first item, we can’t fit it in with capacity zero, but we can fit it in with any capacitymorethanonesoourtablebecomesthefollowing. Thenforitemtwowithcapacity two, it can’t be fit in with one capacity so 12 comes over. At capacity two, it can be fit so only item two is optimal because it has a greater value; then after capacity three, both are picked. We can then continue filling out the table to get the following. Let’s quickly zoom in on two decisions. 0 1 2 3 4 0 1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 1 0 12 12 1 0 12 12 12 12 2 0 12 17 2 0 12 17 17 17 3 0 12 3 0 12 29 29 29 4 0 12 4 0 12 29 30 30 5 0 12 5 0 12 29 35 37 6 0 12 6 0 12 29 47 47 Figure 1: On the left, picking from item two at capacity two; the algorithm compares 0+17 and 12 and finds that 17 is larger. On the right, picking from 47 and 17+25, finding that 47 is larger. 4 Repetition Now, we allow ourselves to repeat items, so our problem is the same, and our table is the same. Theonlydifferenceisthatourrecurrenceisnowdoneallowingforrepetition, making j−1 in the original equation j. The base cases will also be the same with similar reasoning. (cid:26) (cid:27) T[i,j −1], if w > i T[i,j] = j max(T[i,j −1],T[i−w ,j]+v ), if w ≤ i j j j The space of this is O(nW). The runtime is the same as before. 2 5 Linear Space To our knowledge we can’t improve runtime, so instead let’s try making the space better. Here we’ll create one table T[i] which is indexed by the capacity using all the items. Then 2Most variants of knapsack are as hard as each other; although one may think there is some greedy approach to this, sadly there is not. 12: Knapsack-3\nAlgorithm 2 Knapsack With Repeat T ← table indexed from 0 to W ▷ Create the table T[0] = 0 while i in 1..W do T[i] = max (T[i−w ]+v ) j j 1≤j≤n;wj≤i end while return T[W,n] our recursive statement will be as follows T[i] = max (T[i−w ]+v ) j j 1≤j≤n;wj≤i At each step, we consider all the possible elements which could be the last element. Sadly at each step we still need to consider each element, so our runtime is O(nW). However our space is O(W). 12: Knapsack-4"},
{"file": "dsa_slides/05_BFSDijkstraHeaps (1).pdf", "content": "CS 3510 Algorithms: Breadth-First Search, Dijkstra's algorithm, Heaps Aaron Hillegass Georgia Tech\n2/14 Depth-first vs Breadth-firet\n3/14 Breadth-First Search 1 define bfs(𝐺, 𝑣): 2 for all 𝑢 ∈ 𝑉 : hops[𝑢] ≔ ∞ 3 hops[𝑣] ≔ 0 4 𝑄.push_tail(𝑣) 5 6 while 𝑄 is not empty: 𝑢 ≔ 𝑄.pop_head() 7 8 for all edges (𝑢, 𝑣) ∈ 𝐸: 9 if hops[𝑣] = ∞: 𝑄.push_tail(𝑣) 10 hops[𝑣] ≔ hops[𝑢] + 1 11\n4/14 When edges have a cost\n5/14 Dijkstra's Algorithm 1 define dijkstra(𝐺, 𝑣): 2 for all 𝑢 ∈ 𝑉 : dist[𝑢] ≔ ∞ 3 prev[𝑢] ≔ nil 4 dist[𝑣] ≔ 0.0 5 6 𝐻 ≔ priority queue using dist 7 while 𝐻 is not empty: 𝑢 ≔ 𝐻.pop_min() 8 9 for all edges (𝑢,𝑣) ∈ 𝐸: 𝑑 ≔ dist[𝑢] + cost[(𝑢,𝑣)] 10 11 if dist[𝑣] > 𝑑: dist[𝑣] ≔ 𝑑 12 prev[𝑣] ≔ 𝑢 13 𝐻.decrease(𝑣,𝑑) 14\n6/14 Practice\n7/14 Solution\n8/14 Computational Complexity of Dijkstra's 1 define dijkstra(𝐺, 𝑣): 2 for all 𝑢 ∈ 𝑉 : dist[𝑢] ≔ ∞ 3 prev[𝑢] ≔ nil 4 dist[𝑣] ≔ 0.0 5 6 𝐻 ≔ priority queue using dist 7 while 𝐻 is not empty: |𝑉 | × pop + (|𝑉 | + |𝐸|) × decrease 𝑢 ≔ 𝐻.pop_min() 8 9 for all edges (𝑢,𝑣) ∈ 𝐸: 𝑑 ≔ dist[𝑢] + cost[(𝑢,𝑣)] 10 11 if dist[𝑣] > 𝑑: dist[𝑣] ≔ 𝑑 12 prev[𝑣] ≔ 𝑢 13 𝐻.decrease(𝑣,𝑑) 14\n9/14 Computational Complexity of Dijkstra's |𝑉 | × pop + (|𝑉 | + |𝐸|) × decrease Implementation pop decrease Dijstra’s Array 𝑂(|𝑉 |) 𝑂(1) 𝑂(|𝑉 |2) Binary Heap 𝑂(log|𝑉 |) 𝑂(log|𝑉 |) 𝑂((|𝑉 | + |𝐸|) log|𝑉 |) 𝑑log|𝑉| log|𝑉| log|𝑉| 𝑑-ary Heap 𝑂( ) 𝑂( ) 𝑂((𝑑 |𝑉 | + |𝐸|)( 𝑑) log𝑑 log𝑑 log Fibonacci Heap 𝑂(log|𝑉 |) 𝑂(1) 𝑂(|𝑉 | log|𝑉 | + |𝐸|) |𝐸| Realistic? 𝑑-ary Heap where 𝑑 = |𝑉|\n10/14 Binary Heap 𝑗 Parent of 𝑗? ⌊ ⌋ 2 Children of 𝑗? 2𝑗 and 2𝑗 + 1\n11/14 Insert into Binary Heap\n12/14 Popping from Binary Heap\n𝑑 13/14 -ary Heap Each node has 𝑑 children. 𝑗−1 Parent of 𝑗? ⌈ ⌉ 𝑑 Children of 𝑗? Start at (𝑗 − 1)𝑑 + 2 As 𝑑 increases: Inserts speed up a little, deletes slow down a little.\nQuestions? Slides by Aaron Hillegass"},
{"file": "dsa_slides/02_FFTArithmetic.pdf", "content": "CS 3510 Algorithms: Arithmetic and Fast Fourier Transform Aaron Hillegass Georgia Tech\n2/28 Unsigned Ints 𝑛 bits? Store 0..2𝑛 − 1 Storing 𝑥? Need 𝑛 = 𝑂(log 𝑥) bits\n3/28 Addition 1000101 +1110111 -------- 10111100 Your computer sums two 64-bit in a single clock cycle. What about 2048-bit numbers? Generally for 𝑛-bit numbers, 𝑇 (𝑛) ∈ 𝑂(𝑛)\n4/28 Doubling and Halving 9 × 2 Shift left, fill with zeros: 1001 = 9 10010 = 18 9 ⌊ ⌋ 2 Shift right: 1001 = 9 100 = 4 Your computer shifts a 64-bit number by 𝑘 in a single clock cycle.\n5/28 Multiplication: Naive 13 × 11 =? Left-shift and accumulate: 1011 = 11 x1101 = 13 ----- 1011 0000 1011 1011 -------- 10001111 = 143 For 𝑛-bit numbers with 𝑂(𝑛) addition, this multiplication is 𝑂(𝑛2).\n6/28 Multiplication Iterative Recursive 1 define mult(𝑥, 𝑦): 1 define mult2(𝑥, 𝑦): 2 y_stack ≔ bits of 𝑦 (MSB at top) 2 if y is zero: 3 sum ≔ 0 3 return zero 𝑦 4 while bit ≔ y_stack.pop(): 4 𝑧 ≔ mult2(𝑥, ⌊ ⌋) 2 5 sum ≔ 2 × sum 5 if 𝑦 is even: 6 if bit: 6 return 2𝑧 7 sum ≔ sum + 𝑥 7 else 8 return sum 8 return 2𝑧 + 𝑥 Still 𝑂(𝑛2).\n7/28 Multiply: Divide-and-Conquer 1 Multiplying 𝑥 and 𝑦, divide each into two halves: 𝑛 𝑛 𝑥 = 2 𝑥 + 𝑥 and 𝑦 = 2 𝑦 + 𝑦 2 2 𝐿 𝑅 𝐿 𝑅 Note: 𝑥𝑦 = (2 𝑛 𝑥 + 𝑥 )(2 𝑛 𝑦 + 𝑦 ) = 2𝑛𝑥 𝑦 + 2 𝑛 (𝑥 𝑦 + 𝑥 𝑦 ) + 𝑥 𝑦 2 2 2 𝐿 𝑅 𝐿 𝑅 𝐿 𝐿 𝐿 𝑅 𝑅 𝐿 𝑅 𝑅 𝑛 Recurrence: 𝑇 (𝑛) = 4𝑇 ( ) + 𝑂(𝑛), so 𝑎 = 4, 𝑏 = 2, 𝑑 = 1 2 𝑑 < log 𝑎 so 𝑂(𝑛log 𝑏 𝑎) = 𝑂(𝑛2) 𝑏\n𝑛 Is multiplication of two -bit numbers 2 𝑂(𝑛 ) always ?\n9/28 Multiply: Karatsuba 𝑥𝑦 = 2𝑛𝑥 𝑦 + 2 𝑛 (𝑥 𝑦 + 𝑥 𝑦 ) + 𝑥 𝑦 2 𝐿 𝐿 𝐿 𝑅 𝑅 𝐿 𝑅 𝑅 We note that: 𝑥 𝑦 + 𝑥 𝑦 = (𝑥 + 𝑥 )(𝑦 + 𝑦 ) − 𝑥 𝑦 − 𝑥 𝑦 𝐿 𝑅 𝑅 𝐿 𝐿 𝑅 𝐿 𝑅 𝐿 𝐿 𝑅 𝑅 Only three multiplications: 𝑥 𝑦 , 𝑥 𝑦 , (𝑥 + 𝑥 )(𝑦 + 𝑦 ) 𝐿 𝐿 𝑅 𝑅 𝐿 𝑅 𝐿 𝑅 𝑛 New recurrence: 𝑇 (𝑛) = 3𝑇 ( ) + 𝑂(𝑛), 𝑎 = 3, 𝑏 = 2, 𝑑 = 1 2 log 𝑎 > 1 so: 𝑏 𝑂(𝑛log 3) ≈ 𝑂(𝑛1.585) 2\n10/28 Karatsuba Pseudocode 1 define mult3(𝑥, 𝑦): 2 if 𝑥 == 0, return zero 3 if 𝑥 == 1, return 𝑦 4 𝑥 , 𝑥 ≔ split(𝑥), 𝑦 , 𝑦 ≔ split(𝑦) 𝐿 𝑅 𝐿 𝑅 LL ≔ mult3(𝑥 , 𝑦 ) 5 𝐿 𝐿 RR ≔ mult3(𝑥 , 𝑦 ) 6 𝑅 𝑅 PS ≔ mult3(𝑥 + 𝑥 , 𝑦 + 𝑦 ) 7 𝐿 𝑅 𝐿 𝑅 8 return 2𝑛 LL + 2 𝑛 2 (PS − LL − RR) + RR\n11/28 Multiply: Reality #include <gmp.h> Most big int libraries use: char inStr[1024] = \"392...511\"; • Hardware for 𝑛 ≤ 64 mpz_t n; • Toom-Cook (similar to Karatsuba) mpz_init(n); for 𝑛 ≤ 4, 000 mpz_set_ui(n, 0); mpz_set_str(n, inStr, 10); • Schönhage–Strassen (FFT-based) mpz_mul(n, n, n); /* n = n * n */ for 𝑛 > 4, 000 mpz_out_str(stdout, 10, n);\n12/28 Integer Exponentiation 𝑥 = [𝑥 ,𝑥 ,…,𝑥 ] 𝑛 𝑛−1 1 𝐴𝑥 = 𝐴[2𝑛−1𝑥 ]𝐴[2𝑛−2𝑥 ]…𝐴𝑥 𝑛 𝑛−1 1 Example: 313 = 3[1,1,0,1] = (323 )(322 )(30)(320 ) = (6,561)(81)(3) = 1,594,323 1 define exp(𝐴,𝑥): result ≔ 𝐼 2 factor ≔ 𝐴 3 4 x_stack ≔ bits of 𝑥 (LSB at top) 5 while bit ≔ x_stack.pop(): 6 if bit: result ≔ result ∗ factor 7 factor ≔ factor × factor 8 9 return result\n13/28 Naive Matrix Multiplication 𝑂(𝑛3)\n14/28 Strassen Algorithm (1969) 𝐴 𝐴 𝐵 𝐵 𝑀 +𝑀 −𝑀 +𝑀 𝑀 +𝑀 𝐴 = ( 11 12) 𝐵 = ( 11 12) 𝐴 × 𝐵 = ( 1 4 5 7 3 5 ) 𝐴 𝐴 𝐵 𝐵 𝑀 +𝑀 𝑀 −𝑀 +𝑀 +𝑀 21 22 21 22 2 4 1 2 3 6 where 𝑀 ≔ (𝐴 + 𝐴 ) × (𝐵 + 𝐵 ) 1 11 22 11 22 𝑀 ≔ (𝐴 + 𝐴 ) × 𝐵 5 11 12 22 𝑀 ≔ (𝐴 + 𝐴 ) × 𝐵 2 21 22 11 𝑀 ≔ (𝐴 − 𝐴 ) × (𝐵 + 𝐵 ) 6 21 11 11 12 𝑀 ≔ 𝐴 × (𝐵 − 𝐵 ) 3 11 12 22 𝑀 ≔ (𝐴 − 𝐴 ) × (𝐵 + 𝐵 ) 7 12 22 21 22 𝑀 ≔ 𝐴 × (𝐵 − 𝐵 ) 4 22 21 11 𝑇 (𝑛) = 7𝑇 ( 𝑛 ) + 𝑂(𝑛2), so 𝑂(𝑛log 2 7) ≈ 𝑂(𝑛2.8) 2\n15/28 Breaking News Wednesday’s story: “Google’s AlphaEvolve has found a faster way to multiply 4 × 4 matrices!” Hmm.\nAll of Fast Fourier Transforms!?\n17/28 Evaluating a Polynomial Given a polynomial 𝐴 = 𝑎 + 𝑎 𝑥 + 𝑎 𝑥2 + … + 𝑎 𝑥𝑛−1 0 1 2 𝑛−1 Naive evaluation of 𝐴(𝑥 ) is 𝑂(𝑛2): 0 2 𝑛−1 𝐴(𝑥 ) = 𝑎 + 𝑎 𝑥 + 𝑎 𝑥 + … + 𝑎 𝑥 0 0 1 0 2 0 𝑛−1 0 Smarter way is 𝑂(𝑛): 𝐴(𝑥 ) = 𝑎 + 𝑥(𝑎 + 𝑥(𝑎 + 𝑥(𝑎 + … + 𝑥𝑎 )…) 0 0 1 2 3 𝑛−1\n18/28 Adding Polynomials 𝐴(𝑥) + 𝐵(𝑥) = 𝑎 𝑥𝑛−1 + … + 𝑎 + 𝑏 𝑥𝑛−1 + … + 𝑏 𝑛−1 0 𝑛−1 0 = (𝑎 + 𝑏 )𝑥𝑛−1 + … + (𝑎 + 𝑏 ) 𝑛−1 𝑛−1 0 0 Assuming scalar addition is 𝑂(1), polynomial addition is 𝑂(𝑛)\n19/28 Multiplying Polynomials 𝐴(𝑥)𝐵(𝑥) = (𝑎 𝑥𝑛−1 + 𝑎 𝑥𝑛−1 + … + 𝑎 )(𝑏 𝑥𝑛−1 + 𝑏 𝑥𝑛−2 + … + 𝑏 ) 𝑛−1 𝑛−1 0 𝑛−1 𝑛−2 0 = 𝑐 𝑥2(𝑛−1) + … + 𝑐 2(𝑛−1) 0 𝑘 where 𝑐 = 𝑎 𝑏 + 𝑎 𝑏 + … + 𝑎 𝑏 = ∑ 𝑎 𝑏 𝑘 0 𝑘 1 𝑘−1 𝑘 0 𝑖 𝑘−𝑖 𝑖=0 This is 𝑂(𝑛2). Can we do it faster?\n20/28 Polynomials Are Nice Any 𝑛 points define the polynomial of degree 𝑛 − 1 What if instead of multiplying the coefficients, we: 1. Pick 𝑛 points 2. Evaluate 𝐴 and 𝐵 at those points 3. Multiply those results to get 𝐶 at those points 4. Did interpolation to figure out the coefficients of 𝐶 Can we chose those 𝑛 points such that steps 2 and 4 were crazy-fast?\n21/28 Polynomial Divide and Conquer Divide a polynomial 𝐴 into its even and odd terms: 𝐴(𝑥) = 9𝑥5 + 3𝑥4 − 2𝑥3 + 5𝑥2 + 7𝑥 − 4 𝐴 = 3𝑥2 + 5𝑥 − 4 𝐴 = 9𝑥2 − 2𝑥 + 7 𝑒(𝑥) 𝑜(𝑥) 𝐴(𝑥) = 𝐴 (𝑥2) + 𝑥𝐴 (𝑥2) 𝑒 𝑜 If I’ve already evaluated 𝐴 at 𝑥 = 1, what would be a really easy second value to evaluate?\n22/28 Repeated Divide-and-Conquer\n23/28 Roots of Unity What is {𝑥 | 𝑥8 = 1} ? 𝜔𝑘 = 𝑒 2𝑘𝜋𝑖 = cos 2𝑘𝜋 + 𝑖 sin 2𝑘𝜋 𝑛 𝑛 𝑛 where 𝑘 = 0, …, 𝑛 − 1\n24/28 Complexity At top: Evaluate 𝐴 of degree 𝑛 − 1 at 𝑛 points Recurrence: Evaluate 𝐴 of degree 𝑛 − 1 at 𝑛 points 2 2 Do 𝑛 multiplications and 𝑛 additions: 𝑂(𝑛1) 𝑇(𝑛) = 2𝑇(𝑛) + 𝑂(𝑛) 2 FFT is 𝑂(𝑛log𝑛)\n25/28 FFT Pseudocode Given: • 𝐴: a polynomial of degree ≤ 𝑛 − 1 • 𝜔 an 𝑛th root of unity 1 define FFT(𝐴, 𝜔): 2 if 𝜔 == 1: return 𝐴(1) 3 express 𝐴(𝑥) in the form 𝐴 (𝑥2) + 𝑥𝐴 (𝑥2) 𝑒 𝑜 4 evens ≔ FFT(𝐴 , 𝜔2) to evaluate 𝐴 at powers of 𝜔2 𝑒 𝑒 5 odds ≔ FFT(𝐴 , 𝜔2) to evaluate 𝐴 at powers of 𝜔2 𝑜 𝑜 𝑛 6 for 𝑗 in 0 to − 1: 2 result[𝑗] ≔ evens[𝑗] + 𝜔𝑗 odds[𝑗] 7 result[𝑗 + 𝑛 ] ≔ evens[𝑗] − 𝜔𝑗 odds[𝑗] 8 2 9 return result\n26/28 And back again... Now we have a way to go from 𝑛 coefficients to 𝑛 values in 𝑂(𝑛 log 𝑛): values ≔ FFT(coefficients, 𝜔) The FFT also does the inverse. Given values at powers of 𝜔: coefficients ≔ 1 FFT(values, 𝜔−1) 𝑛\n27/28 The whole trip Example: 𝐴 and 𝐵 be two polynomials of degree 7: • Give them 16 coefficients by adding zero coefficients. 2𝜋 2𝜋 • Calculate the 16th root of unity: 𝜔 ≔ cos + 𝑖 sin 16 16 • values ≔ FFT(𝐴, 𝜔) 𝑎 • values ≔ FFT(𝐵, 𝜔) 𝑏 • Element-wise multiplication gets us values 𝑐 • 𝜔−1 ≔ cos 2𝜋 − 𝑖 sin 2𝜋 16 16 • 𝐶 ≔ FFT(values , 𝜔−1) 𝑐\nQuestions? Slides by Aaron Hillegass"},
{"file": "dsa_slides/NotesNPGraphs.pdf", "content": "CS 3510 Algorithms 3/26/2024 Lecture 15: Independent Set, Clique, and Vertex Cover Lecturer: Abrahim Ladha Scribe(s): Richard Zhang Let’s look at some reductions and discover some more problems that are NP-complete. We will be looking at Independent Set, Clique, and Vertex Cover. 1 Independent Set ForagivengraphwithvertexsetV andedgesetE,asubsetofverticesI ⊆ V isindependent ifthereisnoedgebetweenanytwoverticesinthesubset. Letslookatsomeexampleswhere finding an independent set is simple. Figure 1: Independent set of Wheel graph Figure 2: Independent set of Bipartite Graph In the graphs from Figure 1 and 2, the black vertices would form an independent set since there is no edge that has both ends as black vertices. However, finding an independent set foragraphingeneralisareallyhardproblemandisinfactanNP-completeproblem, which 15: Independent Set, Clique, and Vertex Cover-1\nwe will show now. Let formalize it as a decision problem and define it as the following: INDEPENDENT-SET = {⟨G,g⟩| G has an independent set of sizeg} First, we will show that INDEPENDENT-SET ∈ NP. We will define a verifier that takes in as input a problem ⟨G,g⟩ and a witness ⟨v ,...,v ⟩, where G is a graph, g is an inte- 1 k ger, and ⟨v ,...,v ⟩ is a subset of vertices. We first check that k = g. We then check 1 k for each pair of distinct vertices v ,v ∈ {v ...v } that there does not exist an edge be- i j 1 k tween them. If both of these checks pass, then we can return true (the problem instance ⟨G,g⟩ ∈ INDEPENDENT-SET). Else, wereturnfalse. Sincethesecheckstakepolynomialtime, wehaveapolynomialtimeverifierandcanthereforeconcludethatINDEPENDENT-SET ∈ NP. Now, we will show that INDEPENDENT-SET is NP-hard. We will reduce from the 3SAT problem. So far, all of our reductions have been from boolean formula problems. How can we convert a CNF-formula to a graph? This does not seem too obvious. Let’s delve into the 3SAT problem. When you have a satisfying assignment of a formula, you cannot select literals x and x¯ to be true simultaneously. For example: (a∨x∨b)∧(c∨x¯∨d) Choosing x in the first clause turns off x¯ in the second. We want to do something similar in a graph. Choosing a vertex to be part of a candidate independent set turns off its neighbors from being included in the set. From the idea above, we can build a polynomial time reduction f : φ → ⟨G,g⟩ such that φ ∈ 3SAT ⇐⇒ ⟨G,g⟩ ∈ INDEPENDENT-SET f will take in as input φ, a formula in 3CNF. For each clause in φ, create one ”triangle” per clause where every vertex in the clause is unique, the vertices are labeled by the clause’s literals, and these vertices are connected to each other 1. If a literal x is a vertex in a triangle, put an edge to x¯ in all the other triangles. Lets denote this generated graph as G. We then set g to be the number of clauses in φ. We now return this new problem ⟨G,g⟩. Note that this reduction is computable in polynomial time. Building G takes linear time tobuildthetrianglesand,attheworstcase,O(n2)timeforbuildingtheotherpairsofedges. Let’s look at an example. If φ = (x¯∨y∨z¯)∧(x∨y¯∨z)∧(x∨y∨z)∧(x¯∨y¯), then the graph G would look like the one presented in Figure 3. 1Foraclausewith2variables,itwouldbetwoverticesconnectedbyanedge. Foraclausewith1variable, it would simply be the vertex itself 15: Independent Set, Clique, and Vertex Cover-2\ny y¯ y y¯ x¯ z¯ x z x z x¯ Figure 3: Constructed graph of formula (x¯∨y∨z¯)∧(x∨y¯∨z)∧(x∨y∨z)∧(x¯∨y¯) Ifφ ∈ 3SAT,thenthereexistsasatisfyingassignmentofthevariablesthatmakesφtrue. For each clause, select the corresponding vertex of one of the literals that would be evaluated to true in the satisfying assignment of the formula. These selected vertices would form an independent set since only one literal is selected per “triangle” of each clause and that it is impossible for a literal and its negation to be true and to be selected. This results in no edges having both ends being selected. Since one vertex of each clause will be selected, the size of the independent set will be equal to g (which was set to the number of clauses in the reduction). Therefore, φ ∈ 3SAT =⇒ ⟨G,g⟩ ∈ INDEPENDENT-SET. If ⟨G,g⟩ ∈ INDEPENDENT-SET2, then an independent size of size g exists. By construction, there is g “triangles” in the graph, and since you cannot pick two (or more) from a single triangle, this independent set must have one vertex exactly in each triangle. Each of the selected vertices corresponds to the assignment of each clause in the formula φ, so we see that ⟨G,g⟩ ∈ INDEPENDENT-SET =⇒ φ ∈ 3SAT. Sincewefoundavalidpolynomialreduction,wecanconcludethat3SAT ≤ INDEPENDENT-SET p and therefore INDEPENDENT-SET is NP-hard. SinceINDEPENDENT-SET ∈ NPandINDEPENDENT-SETisNP-hard,weconcludethatINDEPENDENT-SET is NP-complete. Now that we have one graph problem which is NP-complete, it will make more graph NP-complete problems easier to prove. 2 Clique A complete graph is a graph where every vertex is connected to every other vertex. A clique is a complete subgraph of a graph, meaning that every vertex in this subgraph is connectedtoeveryothervertexinthesubgraph. Figure4hasexamplesofcompletegraphs. Note that Kn denotes a complete graph with n vertices. 2To clarify, this is a G that was constructed from the reduction, not some general graph. 15: Independent Set, Clique, and Vertex Cover-3\nK2 K5 K3 K4 Figure 4: Complete Graphs K2, K3, K4, and K5 Figure 5 presents an example of a clique of size 3 (as indicated by the black nodes) present in a graph. Figure 5: Clique of a graph Determining if a clique of a certain size exists in a graph is a very hard problem and is in fact NP-complete. To prove this, let formalize it as a decision problem and define it as the following: CLIQUE = {⟨G,g⟩| G has a clique of size g} First, we will show that CLIQUE ∈ NP. We will define a verifier that takes in as input a problem ⟨G,g⟩ and a witness ⟨v ,...,v ⟩, where G is a graph, g is an integer, and ⟨v ,...,v ⟩ 1 k 1 k is a subset of vertices. The verifier checks that {v ...v } has size g. Next, it checks if there 1 k exists an edge between every distinct pair v ,v ∈ {v ...v }. If both of these conditions i j 1 k pass, then our witness ⟨v ,...,v ⟩ is a clique. This verifier takes polynomial time, so we can 1 k conclude that CLIQUE ∈ NP. 15: Independent Set, Clique, and Vertex Cover-4\nNow,wewillshowthatCLIQUEisNP-hardbyreducingnotfrom3SATbutfromINDEPENDENT-SET. We need to find a reduction f : ⟨G,g⟩ → ⟨G¯,g⟩ such that ⟨G,g⟩ ∈ INDEPENDENT-SET ⇐⇒ ⟨G¯,g⟩ ∈ CLIQUE Lets make G¯ the complement graph of G and make the variable g stay the same. The complement of a graph G¯ includes the edges that do not exist in G and excludes the edges that do exist in G. More precisely, G¯ has the same vertex set as G and an edge set that contains the unordered pairs of the vertices that do not exist in the original edge set of G. This reduction is polynomial since we are simply looping through pairs of vertices to find the edges that do and do not exist. Figure 6 presents an example of the complement of the graph. G G¯ Figure 6: Complement graph Notice that if a set of vertices are independent, then they share no edges. However, in the complement graph, these same set of vertices have every possible edge between two vertices in the set. If ⟨G,g⟩ ∈ INDEPENDENT-SET, then there exists a selection of the vertices of size g with no edges between then. Then if an edge e is not in the edge set of G, it must be in the edge set of G¯. Thus, those same vertices in G¯ share all edges. That is the definition of a clique, so G¯ has this clique of size g. Therefore, we can conclude that ⟨G,g⟩ ∈ INDEPENDENT-SET =⇒ ⟨G¯,g⟩ ∈ CLIQUE. Note that since the complement of G¯ (G ¯¯) is equal to G itself, this is a rare time we get the reverse argument for free. If ⟨G¯,g⟩ ∈ CLIQUE, then the clique of size g in G¯ is an indepen- dent set of size g in G. Therefore, we can conclude that ⟨G¯,g⟩ ∈ CLIQUE =⇒ ⟨G,g⟩ ∈ INDEPENDENT-SET. Since we found a valid polynomial reduction, we can conclude that INDEPENDENT-SET ≤ p CLIQUE and therefore CLIQUE is NP-hard. 15: Independent Set, Clique, and Vertex Cover-5\nSince CLIQUE ∈ NP and CLIQUE is NP-hard, we conclude that CLIQUE is NP-complete. 3 Vertex Cover A vertex cover is a selection of vertices such that every edge shares one end in the cover. Figure 7 presents some examples where the black vertices are vertex covers: Figure 7: Vertex Covers Notice that the opposite of a vertex cover (vertices not in the vertex cover) can have no edges in between. Otherwise, this would suggest that there exists an edge with both ends not in the vertex cover, which is a contradiction. That means that the vertices not in the vertex cover is just an independent set! The same logic can also be applied when go- ing from an independent set to a vertex cover. That means that S ⊂ V is a vertex cover ifandonlyifS¯ = V \\S isanindependentset. Thisideawillbeimportantforourreduction. Determining if a vertex cover of a certain size exists in a graph is a very hard problem and is in fact NP-complete. To prove this, let formalize it as a decision problem and define it as the following: VERTEX-COVER = {⟨G,g⟩| G has a vertex cover of size g} First, we will show that VERTEX-COVER ∈ NP. Our verifier takes in as input ⟨G,g⟩ and ⟨v ,...,v ⟩ and checks for each edge in the graph G if one endpoint is in {v ...v }. If it 1 k 1 k is, then the a vertex cover exists. This verifier runs in polynomial time since we are sim- ply looping through v ...v for each edge in the graph. Therefore, we can conclude that 1 k VERTEX-COVER ∈ NP. ToprovethatVERTEX-COVERisNP-hard,wewillreducefromINDEPENDENT-SETtoVERTEX-COVER. We will define a reduction f : ⟨G,g⟩ → ⟨G,|V|−g⟩ in which V is the vertex set of G and |V| is the total number of vertices. If ⟨G,g⟩ ∈ INDEPENDENT-SET, there exists a selection of V, say S where |S| = g, with no edge between them. Then S¯ = V \\S is a set of vertices where all edges have to touch. If no edge has both ends in S, every edge has one end in V \\S. That means that S¯ is a vertex cover of size |V|−g, so we have shown that ⟨G,g⟩ ∈ INDEPENDENT-SET =⇒ ⟨G,|V|−g⟩ ∈ VERTEX-COVER. 15: Independent Set, Clique, and Vertex Cover-6\nSimilarly, if ⟨G,|V|−g⟩ ∈ VERTEX-COVER, then there exists a selection S ⊂ V where |S| = |V| − g and every edge has at least one end in S. Since S is a vertex cover, S¯ = V −S has no edges, so S¯ is an independent set of size |V| −(|V| −g) = g. Thus, ⟨G,|V|−g⟩ ∈ VERTEX-COVER =⇒ ⟨G,g⟩ ∈ INDEPENDENT-SET. Since we found a valid polynomial reduction, we can conclude that INDEPENDENT-SET ≤ p VERTEX-COVER and therefore VERTEX-COVER is NP-hard. Since VERTEX-COVER ∈ NP and VERTEX-COVER is NP-hard, we conclude that VERTEX-COVER is NP-complete. 15: Independent Set, Clique, and Vertex Cover-7"},
{"file": "dsa_slides/Notes_BFS.pdf", "content": "CS 3510 Algorithms 2/01/2024 Lecture 6: BFS and Dijkstra’s Lecturer: Abrahim Ladha Scribe(s): Diksha Holla We want to calculate shortest paths in a graph. DFS is one way for us to search a graph. By using the stack, we can quickly explore deeply into a graph. However it is not particularly good at finding short paths. Here, we mean on an unweighted graph, the shortest path is the number of edges. A B D C If we start at A and run DFS to find the shortest path to get to each element from A, we get that the shortest path from A to D is 3. DFS visits A → B → C → D, and would miss the edge connecting A to D that returns a path of 1. We need a graph traversal algorithm which prioritizes the closest nodes first. This is called Breadth-First-Search or BFS. The shortest path algorithm will take on input a graph and a starting node, and it will compute the shortest paths from that node to all others. 1 BFS BFS utilizes a queue data structure to pop and push from whereas DFS uses a stack. The queue helps us to see all the children of a node first before seeing the grandchildren. This way we can explore the closest nodes first. G is our graph and s is our start node def bfs(G, s): for each u in V: dist(u) = ∞ dist(s) = 0 Q = [s] while Q ̸= ∅: u = eject(Q) for each (u,v) ∈ E: if dist(v) = ∞: inject v into Q dist(v) = dist(u) + 1 Each vertice is pushed and popped from the queue exactly once which takes O(1) time, so O(V). The adjacency lists are looped over once for each, their sum of lengths being O(E), givingusatotalruntimeofO(|V|+|E|). ThistakesthesametimeasDFSbutinadifferent 6: BFS and Dijkstra’s-1\norder Example : E S A D C B we start at S: [S] [A C D E] - pop S and add neighbors of S [C D E B] - pop A and add neighbors of A [D E B] - pop C and add neighbors of C [E B] - pop d and add neighbors of D [B] - pop E and add neighbors of E [] - pop B and add neighbors of B, nothing is left so we end Example : Lets do an example where we add the shortest path from A to each node in the corresponding circle B A C D ∞ → 1 0 ∞ → 2 ∞ → 3 ∞ → 2 ∞ → 1 ∞ → 2 ∞ → 3 E F G H we set node A to 0 and the rest of the nodes to infinity, and now we start BFS [A] [B F] - added B, F and set dist to A + 1 [F E] - added E and set E to B + 1 [E C G] - added C, G and set them to F + 1 [C G] - no nodes added [G D] - added D and set to C + 1 [D H] - added H and set to G + 1 [H] - no nodes added [] - finished 6: BFS and Dijkstra’s-2\n2 Dijkstra’s Consider a weighted graph now, with positive weights. We want to compute the shortest paths not counting the number of edges, but the total sum of the edges in the path. We can compute shortest paths again with BFS by transforming a weighted graph into an unweighted one. Notes/unweightedTransformation.png The problem now is it takes 100 steps to see that A is closer to S than B after one edge. Why not directly compare 100 with 200 then choose to BFS on A next? This is the heart of Dijkstra’s algorithm. It is simply weighted BFS. Instead of a simple queue, we use a priority queue with the following operations • Insert - into queue • Decrease key - decreases value of some key • Delete min - returns element with smallest key • Make-queue - Constructor for the queue, faster than making an empty queue and performing V insertions. G is our graph and s is our start node and l is the edge weights. The prev array contains back pointers to those actually store the shortest path. We want not only the distance, but the path itself. def dijkstras(G, l, s): for each u in V: dist(u) = ∞ prev(u) = null dist(s) = 0 H = makeQueue(V) with dist as keys while H ̸= ∅: u = deleteMin(H) for each (u,v) ∈ E: if dist(v) > dist(u) + l(u,v): dist(v) = dist(u) + l(u,v) prev(v) = u decreaseKey(H,v) Example : 2 B D 4 3 A 1 3 1 4 2 C E 5 6: BFS and Dijkstra’s-3\nA to every node: A: 0 B: ∞, 4, 3 C: ∞, 2 D: ∞, 6, 5 E: ∞, 7, 6 Here is the above graph with the edges used to find each nodes shortest paths 2 B D 3 A 1 2 C E Dijkstra’s is just BFS, but its runtime depends on the priority queue implementation. There are |V| deleteMin operations and |E| insert/decreaseKeys. We may ballpark a make- queue operation as |V| insert/decreaseKeys. data structure deleteMin decreaseKey total Dijkstra array O(|V|) O(1) O(V2) binary heap O(logV) O(logV) O((V+E)logV) d-ary heap O(dlogv) O(logv) O((Vd + E)logV ) logd logd logd If G is dense, E ≈ V2, then array is best to use. A binary heap becomes better to use at around E ≈ V2 logV Example : T X 1 ∞ ∞ 10 S 0 3 2 6 4 9 5 ∞ ∞ 2 Y Z 7 next iter: T = S + 10, Y = S + 5 6: BFS and Dijkstra’s-4\nT X 1 10 ∞ 10 S 0 3 2 6 4 9 5 5 ∞ 2 Y Z 7 next iter: T = Y + 3, X = Y + 9, Z = Y + 2 T X 1 8 14 10 S 0 3 2 6 4 9 5 5 7 2 Y Z 7 next iter: X = Z + 6 T X 1 8 13 10 S 0 3 2 6 4 9 5 5 7 2 Y Z 7 next iter: X = T + 1 6: BFS and Dijkstra’s-5\nT X 1 8 9 10 S 0 3 2 6 4 9 5 5 7 2 Y Z 7 Dijkstra’sonlyworksongraphswithnon-negativeedgeweights. Itreliesonthefollowing property: \"The shortest path s-t is less than or equal to the shortest path s-u-t ∀u\". As you perform updates, the current minimum shortest path computed for some node can only decrease as Dijkstra’s continues. This is not true for negative edge weighted graphs, which makes them a very difficult, much worse object of study, even in 2023.1 You may think that you can simply pad each edge by a constant value, and all compar- isons made in Dijkstra’s should remain the same. Here is a counter example. Notes/dijcount.png The top path is the shortest, but after adding two to each edge, the bottom path is now the shortest. 1https://www.quantamagazine.org/finally-a-fast-algorithm-for-shortest-paths-on-negative-graphs-20230118/ 6: BFS and Dijkstra’s-6"},
{"file": "dsa_slides/NotesSubsetSum.pdf", "content": "CS 3510 Algorithms 3/28/2024 Lecture 16: Subset Sum Lecturer: Abrahim Ladha Scribe(s): Diksha Holla You may remember we talked about the Knapsack problem during our Dynamic Pro- gramming section. You should recall that we had trouble solving Knapsack with better than an exponential runtime. Today we’ll talk about a problem similar to Knapsack called SubsetSum. We’ll proceed to relate this back to Knapsack by showing that the two problems are both NP-Complete. 1 SubsetSum SubsetSumis a problem where you’re given a list of numbers and you want to find a subset which can sum to some target number (think 2sum but for arbitrarily sized sets). This is formalized by having a set S, a subset S′ ⊂ S and a target t, where (cid:80) s = t. An s∈S′ exponentialtimedynamicprogrammingsolutionisthebestknownapproachtothisproblem. As it turns out, SubsetSum is actually NP-Complete; we’ll begin the proof. First we need to show that SubsetSum is in the class NP. We take the subset as a witness and we take the sum over the set. If the subset sum is equal to the target, then the witness is valid. We have shown that SubsetSum is in the class NP. Now we must show that SubsetSum is NP-Hard; we’ll reduce from 3Sat. Overflow, the carrying of bits will make things complicated. We choose the base arbitrarily high such that there is no overflow. We can model an exclusive or relationship by adding two ones together and testing whether they equal one or not. This is essentially the relationship between inverted literals, i.e., x and x . So we’ll create numbers for each literal. We create the target such that it 1 1 is one for all the literals, meaning either x or x need to be true. If both the numbers for 1 1 x and x are selected to be in the subset, it can not be the target because the target has a 1 1 one in the x place (again no overflow). 1 x x x 1 2 3 x 1 0 0 1 x 1 0 0 1 x 0 1 0 2 x 0 1 0 2 x 0 0 1 3 x 0 0 1 3 t 1 1 1 Figure 1: Figure showing just variables and literals. Nowweneedtoaddclauses;we’llusetheclauses(x ∨x ∨x )∧(x ∨x ∨x )∧(x ∨x ∨x ) 1 2 3 1 2 3 1 2 3 as an example. We add clauses on columns so that rows can represent literals in clauses. 16: Subset Sum-1\nThen for each clause and literal, we set the place to be the number of times that literal appears in the clause. We just need to extend the target for the clauses. x x x c c c 1 2 3 1 2 3 x 1 0 0 1 0 0 1 x 1 0 0 0 1 1 1 x 0 1 0 1 1 0 2 x 0 1 0 0 0 1 2 x 0 0 1 1 0 1 3 x 0 0 1 0 1 0 3 t 1 1 1 x x x Figure 2: Figure with literals for clauses with the number of times the literal appears in a clause. We can’t have it be three because that would imply that all three literals must be true. However if we make it one, then we can’t have two be true. For instance the following 3Sat instance would not be able to be solved x ∧(x ∨x )∧x because the middle clause would 1 1 2 2 need to have both of its literals be true. This is because we’d need to take both of the rows fortheliteralsx andx inthesubset, butthiswouldmakeclausehaveavalueoftwowhich 1 2 is not one. Then what do we do? Here comes our last trick: we add slack variables near the bottom and we require a clause to have three true variables. Since there are at three variables in 3Sat (we’re doing 3Sat with exactly three literals per clause), we only need two slack variables, making it so that if one value is taken in the subset, it can use the two slack variables. If three values with a positive value for a clause are taken, no slack variables need to be taken. x x x c c c 1 2 3 1 2 3 x 1 0 0 1 0 0 1 x 1 0 0 0 1 1 1 x 0 1 0 1 1 0 2 x 0 1 0 0 0 1 2 x 0 0 1 1 0 1 3 x 0 0 1 0 1 0 3 s 0 0 0 1 0 0 1 s 0 0 0 1 0 0 1 s 0 0 0 0 1 0 2 s 0 0 0 0 1 0 2 s 0 0 0 0 0 1 3 s 0 0 0 0 0 1 3 t 1 1 1 3 3 3 Figure 3: Figure with slack variables. Thenwe’vecompletedourreductionforSubsetSum,andnowwe’llshowthecorrectness. Saya3SatCNFinstanceissatisfiable,thiswouldimplythatthereisasatisfyingassignment of the variables such that all the clauses have at least one true literal. First, all literals by 16: Subset Sum-2\ndefinition are either true or false, meaning the columns with the variables will total one. Next we know that since there’s a satisfying assignment, meaning the clause has at least one literal which is true. Since there’s a taken literal, we know that each clause column is at least one. The slack variables could be used to make the total exactly three for each variable. Then the SubsetSum instance has this subset to sum to the target. If 3Sat is not satisfiable, then some clause column does not have a true value, meaning even with the slack variables it can sum to at most two. Since this is for every subset, no subset sums to the target t. Then we’ve completed the proof that SubsetSum is in the class NP-Complete. 2 Knapsack From here, showing Knapsack is in NP-Complete is very easy. We first formalize it as a decision problem (cid:88) (cid:88) Knapsack = {⟨I,W,V⟩ | I = {(w ,v ),...,(w ,v )} and v ≥ V and w ≤ W} 1 1 n n i i This is obviously a generalization of subsetsum, so our reduction is simple. For each element s ∈ S, add item (s ,s ) ∈ I. Create a set of items where the weight and value of i i i each item are equal. Then set W = V = t. Our transformation is quite literally f(⟨S,t⟩) = ⟨S ×S,t,t⟩. If ⟨S,t⟩ ∈ SubsetSum then there exists a subset S′ ⊂ S which sums to t. Then consider the set of items corresponding to this subset S′. The sum of the weights of these items are (cid:80) (cid:80) (cid:80) (cid:80) w = s = t = W ≤ W. The sum of the values are v = s = t = V ≥ V. So we i i i i see that ⟨S ×S,t,t⟩ ∈ Knapsack. ⟨S,t⟩ ̸∈ SubsetSum, then every single subset S′ ⊂ S has the sum of the items strictly (cid:80) (cid:80) greater than or less than t. If the sum of the items is greater than t, then w = s > i i t = W, so our capacity condition is not satisfied. If the sum of the items is less than t, then (cid:80) (cid:80) v = s < t = V so our value condition is not satisfied. Either way, for all possible i i choice of items, one of the conditions never holds, so we see that ⟨S ×S,t,t⟩ ̸∈ Knapsack. This reduction also takes polynomial time, so we see that Knapsack is NP-hard. It is also obviously in NP, as your witness would be the subset of items, that you sum and check two inequalities. We conclude that Knapsack is Np-complete. 16: Subset Sum-3"},
{"file": "dsa_slides/NotesChainMatMult.pdf", "content": "CS 3510 Algorithms 2/27/2024 Lecture 11: Chain Matrix Multiplication Lecturer: Abrahim Ladha Scribe(s): Tejas Pradeep 1 Problem Statement Suppose we are given a sequence of matrix dimensions, we want to compute the product of the matrices. We are not given the matrices, but the dimensions. We want to determine which order to multiply the matrices in to minimize the number of operations. So say you have the following matrices, A = (50×20), B = (20×1), C = (1×10), D = (10×100). Suppose we want to compute the product ABCD. Note that we can do this for rectangular matriciessincethedimensionoftherowsofAisthedimensionofthecolumnofBandsoon. Matrix multiplication is associative, but not communitive. It is true that (AB)C = A(BC), but not true that AB = BA. Given two matricies of dimensions (d ×d ) and (d ×d ), we may ballpark the cost 0 1 1 2 of multiplying these two matricies together as d d d fixed point operations. We are not 0 1 2 concerned with actually multiplying the matricies together, just computing which order to best multiply them according to this cost function. In the previous example, consider the following parantheticalizations • (A((BC)D)) = 20·1·10+20·10·100+50·20·100 = 120200 • ((A(BC))D) = 20·1·10+50·20·10+50·10·100 = 60200 • ((AB)(CD)) = 50·20·1+1·10·100+50·1·100 = 7000 Thisdifferenceallowsustotryandoptimize. Thenumberofpossibleassociationsgrows exponentially, making the search space non trivial. The number of parantheticalizations follows the Catalan numbers, which grows Ω(4n/n3/2). If we greedily split through the one, we get the smallest cost set of associations, but the greedy approach doesn’t actually yield the best associations in other series. We will approach the problem with dynamic programming. 2 Algorithm We can trivially create trees for these associations where the children are the left and right terms and the parent is the expression. 11: Chain Matrix Multiplication-1\nA B C D Figure 1: A tree representing an association for the matrices; each sub tree represents a multiplication. We can not however choose one binary tree from an algorithm; instead we need to consider all binary trees. If you’ve noticed though, each binary tree is made of other binary trees, meaning if we could store these in our dynamic programming structure, we can compute a solution by combining its minimal subproblems. We’ll define a dynamic programming table of dimensions T[1..n][1..n] where T[i,j] = minimum cost to multiply A ,A ,...,A i i+1 j Then to find the minimum cost over the whole array, we index into the table at T[1,n]. Our basecasealsocomesfromthisequation, ∀iT[i,i] = 0becausewe’redoingnomultiplications in that case. At each level we’re going to have some partition of the overall series, say A ...A . i j We’re going to have to find some middle point to multiply two sub matrices at, say (A ...A )(A ...A ). The cost for the overall level is the cost for the smaller two mul- i k k+1 j tiplications A ...A and A ...A plus the cost for the current multiplication. i k k+1 j With every dynamic programming algorithm, consider all possibilities, then take the min or the max or the sum over those. For some given subproblem? What is our last operation? (A )(A A A ...A ) 1 2 3 4 n (A A )(A A ...A ) 1 2 3 4 n (A A A )(A ...A ) 1 2 3 4 n (A A A A )(...A ) 1 2 3 4 n ... (A A A A ...)(A ) 1 2 3 4 n The very last step is where we split the sequence of matricies into two products to be computed separately and then combined. Of course we’re looking for the minimum here, so we need to take the minimum k over all the costs, we do this by making the table T[i,j] = min T[i,k]+T[k+1,j]+d d d i−1 k j i≤k≤j Thekforwhichthisisminimumcorrespondstothecoststocompute(A ...A ),(A ,...,A ), i k k+1 j plus the cost to multiply those together. 11: Chain Matrix Multiplication-2\n3 Pseudocode We’re going to end up ignoring half the table because the recurrence only fills in the top half since i ≤ j. This effects which cells in the table make sense, but also where we’ll find our solution. def chainmatrix(d_0, d_1, ..., d_n): intialize dp table of size n, n to all zeroes 0 for i in 1..n: dp[i,i] = 0 for s in 1..n for i in 1..n-s j = i + s dp[i, j] = min_{i <= k < j} {dp[i, k] + dp[k + 1, j] + d_{i - 1}d_kd_j} Figure 2: Algorithm to find the optimal way to multiply a sequence of matrices. We have weird loops because we need to fill in the table in a specific way. s is the size of the series at some level and i is the starting index of the partition. Runtime: In the algorithm, we fill in n2/2 cells, each taking n time, so O(n3) time. This may seem slow but is much better than the brute force approach, which takes O(4n) time. 4 Example Our original set of matrices had sizes [50,20,1,10,100]. We can fill in our base case below where the indices are equal to each other. 0 - - - - 0 - - - - 0 - - - - 0 We’ll do T[1,2] = m m m = 1000, T[2,3] = m m m = 200, T[3,4] = m m m = 0 1 2 1 2 3 2 3 4 1000. This makes our table the following 0 1000 - - - 0 200 - - - 0 1000 - - - 0 11: Chain Matrix Multiplication-3\nNow we can do T[1,3] = min(T[1,1] + T[2,3] + m m m = 1500,T[1,2] + T[3,3] + 0 1 3 m m m = 1500) = 1500, T[2,4] = min(T[2,2] + T[3,4] + m m m = 3000,T[2,3] + 0 2 3 1 2 4 T[4,4]+m m m = 3000) = 20200) = 3000. 1 3 4 0 1000 1500 - - 0 200 3000 - - 0 1000 - - - 0 Now for our final recurrence we’ll be doing the whole array.   dp[1,1]+dp[2,4]+m m m , if k = 1  0 1 4  dp[1,4] = min dp[1,2]+dp[3,4]+m m m , if k = 2 0 2 4  dp[1,3]+dp[4,4]+m m m , if k = 3  0 3 4   0+3000+100000, if k = 1   dp[1,4] = min 1000+1000+5000, if k = 2 = 7000  1500+0+50000, if k = 3  Finally this would make our table the following, and we can get the cost for the whole series by looking at the cell in the top right. 0 1000 1500 7000 - 0 200 3000 - - 0 1000 - - - 0 We haven’t stored enough information to actually find the associations though; instead we only have the cost. If you wanted this information, you would store the indices where you split (the ks), and reconstruct the answer using those. 11: Chain Matrix Multiplication-4"},
{"file": "dsa_slides/NotesMaxFlow.pdf", "content": "CS 3510 Algorithms 09/21/2023 Lecture 10: Max-Flow Min-Cut Theorem Lecturer: Abrahim Ladha Scribe(s): Richard Zhang This is the last lecture on this unit of graph algorithms, and one of the most important: Max-Flow Min-Cut. It comes from many real world problems, and can actually be applied to many more real world problems. In this lecture, we’ll discuss the problem setting, an algorithm to solve it, and go through some real world examples. Throughout the lecture, we’ll talk about the algorithm at a high level; it’s not terribly importantyouunderstandtheinternalsofthealgorithm,butitisimportantyouunderstand what it does and you can apply it. 1 Max-Flow A flow network G = (V,E) is a subset of weighted graphs with a single source s and a single sink t. Every edge in a flow network has a non-negative capacity 0 ≤ c(u,v). We then make the following assumptions, if there is an edge from u to v, there is no edge from v to u, and all vertices lies in a path between the source and the sink (s → ...v → t). 0≤f(u,v)≤c(u,v) A B Figure 1: Flow is less than or equal to capacity. Now, to define our problem, we have two constraints. The capacity constraint: each edge u,v in a flow network has some flow f(u,v) attached such 0 ≤ f(u,v) ≤ c(u,v). The flow conservation that the flow entering a node must be the same as the flow exiting a node (cid:80) (cid:80) ( f(u,v) = f(v,u)). Then, in the maximum-flow problem, given G, s, t, and c v∈V v∈V we try to maximize the flow moving across the network (or the total flow moving out of the source or into the sink). Austin 2 10 Houston 1 Dallas 2 1 San Antonio Figure 2: A simple flow network illustrating oil movement between cities in Texas. If you’re a little confused hopefully the following example can help; consider you’re shipping oil through pipes from an oil refinery to a consumer. Say your oil refiner is in 10: Max-Flow Min-Cut Theorem-1\nHouston, Texas and you want to transfer oil to a consumer in Dallas. You have pipes each with individual capacity (barrels/hour) connecting Houston to Austin (2 barrels/hour), Houston to San Antonio (2 barrels per hour), Austin to San Antonio (1 barrel/hour), Austin to Dallas (10 barrel/hour), and San Antonio to Dallas (1 barrel/hour). You don’t have to actually move this amount of oil through these pipes though, you’re simply limited by this capacity. Since Austin and San Antonio are pass through cities, the can not produce or consume oil. Therefore, if more oil flows into a city than it can move out, that oil is wasted, so it makes no sense to move in and out unequal amounts of oil. There are many flows which could go through this network. The most obvious is that we could move no oil through the network. Alternatively, you could move oil only through the top path (from Houston to Austin and Austin to Dallas), getting two barrels per hour. However these aren’t particularly helpful because moving nothing or moving a little of something does not make full use of the network. A D 2 3 10 1 2 1 S B 1 T 5 5 4 5 C E Figure 3: A large example flow network referred to throughout the lecture. Instead we’d like to move the maximum flow through the network. Since this network is fairly small, it’s not hard to see that you can move 3 barrels per hour through the network. It would be nice since realistically there are many more cities and many more pipes. For instance, consider the network above; the flow is 7 as we’ll find later, but this is hard to find without an algorithm. Furthermore since this can be applied to many more applications, it would be nice to find an algorithm to work on general graphs. 2 Min-Cut When we talk about Max-Flow, we often also talk about Min-Cut; we’ll understand why later, but for now we’ll introduce Min-Cut. An s-t cut of a flow network G = (V,E) are sets of vertices L ⊂ V and R = V −S, such that s ∈ L, t ∈ R. The capacity of an s-t cut (L,R) is (cid:88) c(L,R) = c(u,v) u∈L,v∈R 1 1For all vertices in L, and all vertices in R, if there is a capacity (or an edge) between them, add the capacity. Note that we don’t consider the edges that are solely in L, solely in R, or that go from R to L. 10: Max-Flow Min-Cut Theorem-2\nA D 2 3 10 1 2 1 S B 1 T 5 5 4 5 C E Figure 4: An s-t cut through the graph with L = {S,A,B}. Considerthefollowings-tcutsonthelargegraphabovewhereLisequalto{s},{s,a,b,c,d,e},{s,a,b}. Inthefirsts-tcut, therearethreeedgesfromLtoR, SA, SB, andSC. Thecapacityacross this s-t cut is 3+5+4 which is 12. The next s-t cut has two edges DT and ET which is 2+5 or 7. The last s-t cut has edges AD, BD and SC, making the capacity 7. LikeMax-Flow, Min-Cutalsohasmanyrealworldapplications. Wewon’tgetintothese too much, but you can imagine you’re a city planner and there a number of roads moving cars within the city. You want to make sure that construction on one of the roads will not cause a large outage. You can find the minimum s-t cut through the network and increase capacity of that s-t cut, then the smallest s-t cut will cause less of a problem. 2 We see though that this too is an important problem, and as it turns out, the two are related. 3 Max-Flow Min-Cut Theorem It turns out individual s-t cuts can be really high because the capacities can be arbitrarily high, but flow is limited by the global value of all the edges. A D 2 3 10 1 2 1 S B 1 T 5 5 4 5 C E Figure 5: A large flow network with Max-Flow 7 and Min-Cut 7. In the above graph, as we’ve mentioned the flow could zero to seven; the smallest s-t 2This of course assumes that each edge has equal rate or that no edge holds all the weight because then of course, you could cut that one edge and drastically decrease capacity. 10: Max-Flow Min-Cut Theorem-3\ncut capacity we’ve found is also seven, while the largest is around 20. You may come to the conclusion that the capacity of all s-t cuts is greater than or equal to the possible flows through the network, and this would be right. This might turn some gears, and actually the Max-Flow Min-Cut theorem states that the maximum flow is equal to the minimum cut. 4 Baseball Elimination We’ve already talked about a number of applications, but another appreciable example is for baseball elimination. Say you have the following data about the top four teams. i Team Wins Losses To Play PHL NYC BOS CHI HOU 1 Phillies 75 59 28 - 6 8 7 7 2 Yankees 71 63 28 6 - 8 7 7 3 Red Sox 69 66 31 8 8 - 8 7 4 Cubs 63 72 28 7 7 8 - 6 5 Astros 49 86 27 7 7 7 6 - Figure 6: The baseball statistics of several teams playing. From this, it might seem that the Astros have a chance to make it to the finals if they win all of their remaining 27 games, and the Phillies lose all their games, putting the two of them at 76 wins and 75 wins respectively. The issue with this analysis is that there are knock on effects to this, and the Phillies losing all of their games implies that the Red Sox would have 77 wins with 23 games remaining, meaning that the Red Sox would be first and the Astros still cannot be 1st with this scenario. This goes on, but it turns out that the Astros were eliminated the whole time. We could keep going but it’s not important. Instead of going through this, we can actually formulate this as a Max-Flow problem, and then solve it. First we define the baseball elimination problems such that we have a set of teams X, a particular team x ∈/ X, there are remaining games between the the teams in X. Each team has currently w wins for team i, r remaining games for team i, and g i i i,j remaining games between teams i,j. 10: Max-Flow Min-Cut Theorem-4\n1−2 t /∞ 1 1 t /∞ 2 g 1,2 1−3 w +r −w x x 1 g 1,3 2 g 1,4 S 1−4 T 3 g 3,4 ... 4 3−4 Figure 7: An example flow network for baseball. Intuitively, it’s easy to think of a single flow as a game being played, so let’s set up the network like that. Our network will have a first layer which represents the games being played between each two teams i,j ∈ X where i < j. The weight going from the source to those nodes will be the number of games left between the pair g ,j. Then, each team either i wins or loses a game, so we can draw two lines to another layer of nodes, and these lines will have infinite capacity. We have to call the flow through these lines them something, so let’s call them t ,t . Due to flow conservation, t +t = g where t would be the number 1 2 1 2 1,2 1 of games team 1 won against team 2. Lastly, we’ll take all of these team nodes and connect them to the sink. The capacity of these edges will be w +r −w . If w +r < w , then x x i x x i team x was eliminated to began with, so there is no need to find the flow of this graph. Therefore, we can assume that w +r −w is non-negative and represents the amount of x x i games that team i can win without exceeding the maximum number of wins that team x can achieve and eliminating team x all together. To cap it off3, we’ll define the following value R = (cid:80) r which is the total amount i∈X i of games remaining. Here’s the claim: if a flow of R exists in the network, then team x is not eliminated. The logic here is that if a flow of R exists, it must have filled all edges leaving S since the sum of those edges would also be the total amount of games remaining and equal to R. As a result, all the games have been played, and still the teams all have at most as many wins as team x because of the capacities on the final layer. In other words, the last layer, which is restricting how much the other teams can win, is not restricting the number of games that can be played for team x to win. Alternatively, if a flow of R does not exist in the network, then the team is eliminated. Thenthatmeansthatalltheteamshavemetthecapacitiesputuponthembythelastlayer, 3Pun not intended 10: Max-Flow Min-Cut Theorem-5\nmeaning that all the teams have at least tied team x. However, there are still R minus the flowofthegameslefttobeplayed, makingitimpossibleforxtohavethemaximumamount of wins. 5 Ford-Fulkerson Ford-Fulkerson sort of isn’t an algorithm; the pair wrote an article which outlined an ap- proachyoucouldtaketosolvingMax-FlowMin-Cut,buttheydidnotgiveastrictalgorithm forsolvingit. Thismeans,thatthedifferentimplementationsofthealgorithmhavedifferent runtimes, so you should not worry about the runtime or in-depth implementation details. 4 A D 2 3 10 1 2 1 S B 1 T 5 5 4 5 C E Figure 8: A large flow network for Ford-Fulkerson. At a high level, suppose we modified depth first search to try to greedily solve this problem. Say we do something along the lines of keeping track of the maximum flow entering nodes and then use that to calculate what we think the flow is. The issue with this approach is that we’ll need to correct for things we may have overcompensated for and we’ll need to do a lot of backtracking. Ford-Fulkerson is able to retain metadata so it doesn’t have to do this backtracking. More specifically, it maintains a residual network G = (V,E ) for the original graph G. f f G G f f f/c A B A B c−f Figure 9: Comparsion of edges in G vs G . f For each edge in G, G has forward edges representing the remaining capacity c−f and f backwards edges representing the current flow f for that edge. The Ford Fulkerson method then finds augmenting paths on G to continuously increase the flow of the graph. f 4If you’re curious what an implementation of Ford-Fulkerson looks like, refer to the website of Stephen Huan. The algorithm they implement is Edmonds-Karp. 10: Max-Flow Min-Cut Theorem-6\ndef Ford-fulkerson(G): initialize the residual graph Gf as G initialize flow to zero while Gf has an (s-t) path find a path and compute its bottleneck b augment Gf by subtracting the path from Gf with b add reverse edges of the path from Gf with b return flow Figure 10: Ford-Fulkerson Againwe’llgooverthisatahighlevel: givenan(s-t)path, findthebottleneck, augment the residual graph with this information, and add to the flow. The ways we generate (s-t) paths are again not explicitly mentioned. A popular way to do this is with BFS. 5 The bottleneck on the path is the edge with minimum capacity on that path. If you consider the path SADCET, the bottleneck is edge DC because that has capacity one. The last and most important thing about Ford-Fulkerson is the trick it does on the residual graph so it doesn’t have to do any big backtracking. When we have a path and a bottleneck, we augment the graph such that all the edges on that path are subtracted by the bottleneck, and add the reverse of the path with the bottleneck. Thismaysoundabitcomplicatedsowe’lldoanexample. Supposewehavethefollowing paths in sequence, 6 SADCET, SADET, SCET, SCEDT, and SBDT. We’ll go through the Ford-Fulkerson algorithm. A 1 D 1 1 10 2 1 2 1 S B 1 T 5 1 4 4 1 C E 4 Figure 11: Ford-Fulkerson with modifications from path SADCET. Our total flow is 1. 5If you’re interested think about why this might be. 6Again, the ways in which you generate (s-t) paths can cause different things to happen but you should get the same flow provided you’re doing everything else correctly. 10: Max-Flow Min-Cut Theorem-7\nA D 2 2 10 1 1 2 1 S B 1 T 5 2 3 4 1 C E 4 Figure 12: Ford-Fulkerson with modifications from path SADET. Our total flow is 2. A D 2 2 10 1 1 2 1 S B 1 T 5 5 3 4 1 C E 1 Figure 13: Ford-Fulkerson with modifications from path SCET. Our total flow is 5. A D 2 1 2 1 10 1 1 1 S B 1 T 5 5 4 5 C E Figure 14: Ford-Fulkerson with modifications from path SCEDT. Our total flow is 6. 10: Max-Flow Min-Cut Theorem-8\nA D 2 1 2 1 10 1 1 1 S B 1 T 5 5 4 5 C E Figure 15: Ford-Fulkerson with modifications from path SBDT. Our total flow is 7. Nowtherearenomore(s-t)paths,meaningtherecanbenomoreiterationonthegraph. We end with a final flow of 7. We need to justify that the flow returned is maximum. Certainly since we found a cut equaling a flow in this example, then that would have to be the maximum flow and the minimum cut. In general, we can show that the max flow is achieved when there is no (s-t) path in G . Suppose for contradiction that flow f was the maximum but the corresponding f G had an augmenting (s-t) path with bottleneck b. We then can add b to the flow for each f edge in G (not G ) corresponding to the (s-t) path and produce a greater flow of f +b, f contradicting that f was the max flow. Additionally, the algorithm also produces the minimum cut. Call explore(S) on G and f take the set of nodes which are reachable from it as L and everything else as R. Then then in G, these edges had to have totally saturated capacity since there don’t exist any L−R forward edges in G. 6 Proof of the Theorem We want to prove that the algorithm terminates when it finds a maximum flow, and along the way, show that the maximum flow is equal to the minimum cut. If we know that every flow f is less than cut c , and less than cut c , then we know that 1 2 f ≤ c and f ≤ c . We then also know that f ≤ min(c ,c ). We then know that every flow 1 2 1 2 is less than every cut, so we know that the max flow is less than or equal to the minimum cut. max(flows) ≤ min(cuts). It is then sufficient for us to show there is a maximum flow which is equivalent to some cut. First we prove that if f is a max flow, then G has no (s − t) path (the stopping f condition of the ford-fulkerson algorithm.) Suppse to the contrary that f was maximum with G having an (s−t)-path still. Compute the bottleneck b of the path and note that f we may increase the flow by this much exactly along this path. This gives us a new flow of f +b, contradicting the fact that f was maximum. Next we show that if f is a max flow, then it is equivalent to some cut. Since f is maximum, there is no (s−t) path in G . Call explore(G ,s) and note that since t is not f f reachable from s, this is a partition, a cut of G. We called explore on the residual graph but got a cut on the original graph. Consider the edges in G with one end in L and the f 10: Max-Flow Min-Cut Theorem-9\nother in R. Let these edges be like (u,v) with u ∈ L and v ∈ R. These edges have some capacity and flow f,c. Note that the forward edge c−f must zero in G , otherwise v would f be reached by the explore call, so c−f = 0 implies that c = f. The edges of this cut all are fully saturated, so the capacity of the cut is equivalent to the flow on the cut. Also notice that any edges R−L, going backwards, must be zero. The flow is then equivalent to the flows of the outgoing edges on this cut, minus the flow of the incoming edges. The outgoing edges sum to the capacity of the cut, and the incoming edges sum to zero, so we see that this maximum flow is equivalent to this cut, meaning the cut was minimal. We may conclude then that the maximum flow is equal to the minimum cut. 10: Max-Flow Min-Cut Theorem-10"},
{"file": "dsa_slides/12_GraphColoring.pdf", "content": "CS 3510 Algorithms: NP-Complete: Graph Coloring Aaron Hillegass Georgia Tech\n2/24 Review 1 𝑃 is the set of decision problems solvable in polynomial time. NP-Hard NP is the set of decision problems verifiable in polynomial time. When we reduce a problem 𝐻 to a problem 𝐽, we are saying “𝐽 NP is at least as hard as 𝐻” NP-Complete An easy problem like RELPRIME is reducible to a hard problem like SAT P A problem 𝐻 is in NP-Hard? Any problem in NP can be reduced to 𝐻. The intersection of NP and NP-Hard is NP-Complete.\n3/24 Review 2 Cook-Levin Theorem gave us one problem in NP-Complete: SAT Every decision problem in NP can be reduced to SAT. {A=True} {B=True} Proving a problem 𝐻 is in NP-Complete is two steps: a f(a) • Show that the problem is in NP • Show SAT (maybe indirectly) reduces to the problem f(a) a That is, given an expression Φ in SAT, you can convert it into an 𝑆 {A=False} {B=False} instance Φ in 𝐻 in polynomial time 𝐻 And Φ is true if and only if Φ is true. 𝐻 𝑆\n4/24 Review 3 So far: SAT in NP-Complete 3-SAT INDSET CIRCUIT-SAT SUBSET-SUM CLIQUE VERTEX-COVER KNAPSACK\n5/24 Graph Coloring Given a graph 𝐺 and a number of colors 𝑘: • Each vertex is assigned a color from 1..𝑘. • No two adjacent vertices have the same color.\n6/24 Two Colorable? Any class of graphs that you can be sure are 2- colorable? So what is the runtime to determine 2-colorable?\n7/24 Four Colorable? If a graph can laid out on a plane with no line crossing, it is said to be planar. Four-color Theorem: Every planar graph is 4- colorable. (So every map is 4-colorable.)\n8/24 Three Colors? R G The problem: Given 𝐺, is it 3-colorable? ? Is it in NP? It is NP-Complete. What can we reduce to 3-coloring? B\n9/24 Reducing 3SAT to 3-Color Given an expression Φ in SAT, we can convert it into a graph 𝐺 such that 𝐺 is 3-colorable if and only if Φ is satisfiable? Let’s use Tangerine, Fuschia, and Silver as colors. We will have nodes in the graph that represent each of the literals: 𝑥 , ¬𝑥 , 𝑥 , ¬𝑥 , …, 𝑥 , ¬𝑥 1 1 2 2 𝑛 𝑛 How can we guarantee that for an 𝑥 , either 𝑖 • 𝑥 = 𝑇 and ¬𝑥 = 𝐹 or 𝑖 𝑖 • 𝑥 = 𝐹 and ¬𝑥 = 𝑇 𝑖 𝑖\n10/24 Boolean Restrictions in 3-Colors F T S … x ¬x x ¬x x ¬x 1 1 2 2 n n\n11/24 OR-gate in 3-Colors Gadget T F/S ? T ? T S/F ? F T/S T F/S F T/F F S/T F S/T\n12/24 3-Clause Gadget ? ? ? ? F S T F T F T S F\nHow do we require each clause to be true?\n14/24 The Whole Graph Φ = (¬𝑥 ∨ 𝑥 ∨ ¬𝑥 ) ∧ (𝑥 ∨ ¬𝑥 ∨ 𝑥 ): 𝑥 = 𝑇 , 𝑥 = 𝐹 , 𝑥 = 𝐹 1 2 3 1 2 3 1 2 3 T F S ¬x x 3 3 ¬x x 2 ¬x 2 1 x 1\nIs Super Mario Brothers NP-Complete? Erik Demaine is the king of showing found objects are NP-Hard.\n16/24 Reducing 3SAT to MARIO For every Φ instance in 3SAT, can we make a MARIO level that can be completed if and only if Φ is satisfiable? We can! So MARIO is NP-Hard, but probably not in NP. https://youtu.be/oS8m9fSk-Wk?si=Av5Ndmel0en5n4zI\n17/24 Hamiltonian Cycle Problem Given a directed graph 𝐺 is there a path that starts and ends at the same vertex and touches every other vertex exactly once? Is it in NP?\n18/24 Reducing 3-SAT to Hamilton Cycle x e i u F Tr a ls e Given a 3-SAT expression Φ can we make a graph 𝐺 that will have a Hamiltonian cycle if and only if Φ is satisfiable? Here is the gadget we will use for each variable.\n19/24 Assigning all the variables x ue 1 F Tr a lse x ue 2 F Tr a lse x ue 3 F Tr a lse\n20/24 Clauses become vertices x e 1 u F Tr a ls e c c 1 2\n21/24 More than one var per clause x ue 1 F Tr a lse c 1 x ue 2 F Tr a lse c 2 x ue 3 F Tr a lse\n22/24 Travelling Salesperson Problem Given a directed graph 𝐺 with weighted edges and a number 𝑘, is there a circuit that visits every vertex exactly once and has a total cost less than or equalt to 𝑘?\n23/24 Hamiltonian Cycle reduces to TSP Given a graph 𝐺, how can we use TSP to determine if 𝐺 has a Hamiltonian cycle? We can use the following reduction: 1. Create a complete graph 𝐺′ with the same vertices as 𝐺. 2. Assign a weight of 1 to each edge in 𝐺, zero to each edge not in 𝐺. 3. Set 𝑘 be zero. If 𝐺 has a Hamiltonian cycle, then 𝐺′ has a circuit that visits every vertex exactly once and has a total cost of 0.\nQuestions? Slides by Aaron Hillegass Based on lectures by Abrahim Ladha"},
{"file": "dsa_slides/02a_Notes.pdf", "content": "CS 3510 Algorithms 1/15/2024 Lecture 3: Arithmetic Lecturer: Abrahim Ladha Scribe(s): Saahir Dhanani Arithmetic involves some basic operations on numbers, such as addition, subtraction, multi- plication, exponentiation, and so on. We will formalize these operations by discussing some algorithms for arithmetic. 1 Representations of Numbers In order to discuss algorithms that operate on numbers, we must discuss how numbers are their representation. What are the ways to write down a number? Like any other object, numbershaveanencoding. Oncomputers,thisencodingisasequenceofbits. Inthecontext of algorithms, we are concerned with the runtime in terms of the size of the input, not the input itself. Given n bits, the largest number we can represent is 2n −1. In other words, given a number, x, it takes n = logx bits to represent the number on a computer. Note that here we don’t particularly care about the base. Recall the change of base formula log n log n = a b log b a The only difference between log and log is multiplication by a constant. All logs have 10 2 the same asymptotics, but are scaled differently. We only use base 10 because we have 10 fingers. We will use base 2 most commonly because computers also do. 2 Algorithms for Addition The first arithmetic operation you learn is always addition. 1. Basic Addition Algorithm: This is the addition algorithm you learned in grade school Input: Two n-bit numbers: x,y Output: The sum x+y Example: input: x = 1000101 and y = 1110111 1000101(69) + 1110111(119) 10111100(188) For each bit, we loop right to left and do either 2 or 3 steps, depending on if we have to carry or not. This means that it takes ≤ 3n+1 steps to complete this algorithm, which is O(n). 3: Arithmetic-1\nCanwedobetter? No! Itsnotpossible, sinceittakesΩ(n)timetoevenreadtheinput and write the down the answer. Since we have a matching upper and lower bound, we know that addition takes Θ(n) time, and cannot be improved. Eventhoughcomputersmaybeabletoadd32-bitnumbersinonestep, constanttime, adding arbitrarily large numbers will be a function of the number of 32 bit blocks its represented as, so it really is asymptotically linear time. If you are working in an enviroment with fixed point arithmetic, then you may assume all the basic arithmetic operations take constant time. That is not the case for the algorithms in this lecture. 3 Algorithms for Multiplication The next obvious step after addition is moving on to multiplication. We give four multipli- cation algorithms. 1. Basic Multiplication Algorithm: This is the kind of multiplication you would’ve learned in grade school. Input: Two n-bit numbers: x,y Output: The product xy Example: input: x = 1101 and y = 1011 1101 ×1011 1101 1101 0000 1101 10001111 Note that, if we consider the bits of y as y ...y , then n 1 xy 2(n−1)+xy 2(n−2)+...+xy 2+xy = x(y 2(n−1)+y 2(n−2)+...+y 21+y 20) = xy n n−1 2 1 n n−1 2 1 As each y is a bit, we just add together up to n shifts of x. Think of y as a bit i i selecting whether we add that shift or not. Since there are n possible additions, and each one takes O(n) time, the total runtime for this multiplication algorithm is O(n2). Is it possible to do better than this? Al-Khwarizmi noticed that there was a recursive algorithm for multiplication. 3: Arithmetic-2\n2. Recursive Multiplication Algorithm: Input: Two n-bit numbers: x,y Output: The product xy Pseudocode : def mult(x, y): ## Base Case if y == 0: return 0 # Split problem in half, recursively call mult z = mult(x, floor(y/2)) if y is even: return 2*z if y is odd: return 2*z + x Letusnowprovethecorrectnessofthisalgorithm: First, thebasecaseisgoodbecause y anything times 0 is still 0. Next, if y is even, then z = x , which means that 2z = xy, 2 y−1 as desired. Otherwise, if y is odd, then z = x , which means that 2z + x = 2 x(y −1)+x = xy. The running time of the algorithm can be analyzed without the use of the master theorem. Notice that each recursive call shifts y by 1 bit (recall that shiftingthebitsleft/rightisthesameasmultiplying/dividingbypowersoftwo). Since there are n bits, there will be n recursive calls. In the worst case, each call performs an n bit addition. Therefore, with n calls, each taking O(n) time, the overall running time is O(n2) This running time isn’t any better than that of the previous algorithm! Is it possible to do better than this? Lets try a divide and conquer approach. 3. Divide and Conquer Multiplication Algorithm: Input: Two n-bit numbers: x,y Output: The product xy Here, wewillrepresentxandyslightlydifferently, byrepresentingthemwithanupper and lower half (left and right half). x = x x = (2n/2)x +x L R L R y = y y = (2n/2)y +y L R L R Note that it is quite easy to compute x and x given x and vice versa since we can L R just shift the bits to get the desired part. If x,y each have n bits, then x ,x ,y ,y L R L R each have n/2 bits. With this new representation, we can rewrite the computation of 3: Arithmetic-3\nxy as follows: xy = ((2n/2)x +x )((2n/2)y +y ) = L R L R = 2nx y +2n/2x y +2n/2x y +x y = L L L R R L R R = 2nx y +2n/2(x y +x y )+x y L L L R R L R R Thus, we have broken down the computation of xy into a few smaller subproblems. Four multiplications, each of numbers of size n/2. We can now translate this equation into an algorithm Pseudocode : def mult(x, y): compute n ## Base case: if n == 1: return x * y xL = x >> (n/2) xR = x % (2^(n/2)) #Note, more than one way to do this yL = y >> (n/2)1 yR = y % (2^(n/2)) LL = mult(xL, yL) LR = mult(xL, yR) RL = mult(xR, yL) RR = mult(xR, yR) return (LL << n) + ((LR + RL) << (n/2)) + RR Each component here is drawn directly from the equation above. Since this is a divide and conquer algorithm, we can use the master theorem to derive the runtime. To do this, we need to write out the recurrence: T(n) = aT(n/b)+O(nd). In this case, a = 4 since there are 4 recursive calls to mult, b = 2 since the size of the input to each call of mult is halving, and d = 1 since we are doing a linear amount of work per call. This gives us the following recurrence: T(n) = 4T(n/2)+O(n). Now, we can compare d with log b a: 1 < log 2 4 which means that the time complexity is O(nlog 2 4) = O(n2). 4. We got the same time complexity again! At this point, after three attempts, one might think that we cannot do better than O(n2). Kolmogorov conjectured that that multiplication has a lower bound of Ω(n2), and set out to prove this at a large conference. Then Karatsuba came along and realized he could reduce the number of subproblems that result from xy. He did this by noticing that: x y +x y = (x +x )(y +y )−x y −x y L R R L L R L R L L R R At first glance, this may seem like an increase in the number of multiplications, but recall that we have already computed x y and x y since they are used in other L L R R parts of the computation for xy. Now, using this revised component, we can create a 3: Arithmetic-4\nnew algorithm with less recursive calls! Karatsuba Algorithm (A Smarter Divide and Conquer Multiplication Algorithm): Pseudocode : def mult(x, y): compute n ## Base case: if n == 1: return x * y xL = x >> (n/2) xR = x % (2^(n/2)) yL = y >> (n/2) yR = y % (2^(n/2)) LL = mult(xL, yL) RR = mult(xR, yR) MM = mult(xL + xR, yL + yR) return (LL << n) + ((MM - LL - RR)<< (n/2)) + RR Once again, we can use the master theorem to do an analysis of the runtime, similar to the previous section. In this case, a = 3 since there are 3 recursive calls to mult, b = 2 since the size of the input to each call of mult is halving, and d = 1 since we are doing a linear amount of work per call. Note that the work done per call also includes computing the parameters for the recursive calls. This gives us the following recurrence: T(n) = 3T(n/2)+O(n). Now, we can compare d with log a: 1 < log 3 b 2 which means that the time complexity is O(nlog 2 3) = O(n1.59). If you generalize this Karatsuba style divconquer, you get the Toom-Cook algorithm. We’vesuccessfullybrokenthequadraticlowerbound! Weareunawareofanynontrivial lowerboundsonmultiplication. ItisconjecturedtobeΩ(nlogn). Onlyrecently(2020) was there an O(nlogn) algorithm found. 4 Algorithms for Exponentiation Let’s consider two algorithms for exponentiation. Suppose we want to compute Ax where A could be a matrix or a number and x is a number. If we consider the bits of x as x = x ...x , then n 1 Ax = A2n−1xn+2n−2xn−1+...+A21x2+A20x1 = A2n−1xnA2n−2xn−1...A21x2A20x1 For example, A17 = A16+1 = A16A, where A16 can be calculated by repeatedly squaring. This leads us into the following algorithm:Basic Exponentiation Algorithm: Pseudocode : 3: Arithmetic-5\n1. def exp(A, x): ans = 1 temp = A for i in [1...n]: if x_i == 1: ans *= temp temp *= temp return ans There are only n multiplications, however, repeated squaring of A grows very fast. We would like the multiplication to be of n-bit numbers, and not something growing large. Computing the run time of modular exponentiation is easier. Here, lets restrict ourselves to assuming A is a number. 2. Modular Exponentiation Algorithm: input: A,x,N where N is the modulus. output: Ax mod N Pseudocode : def modexp(A, x, N): if x == 0: return 1 z = modexp(A, floor(x/2), N) if x is even: return z*z mod N else: return A * z*z mod N Letusnowprovethecorrectnessofthisalgorithm: First, thebasecaseisgoodbecause anything to the power 0 is 1. Next, if x is even, then (Ax/2)2 = Ax, as desired. Otherwise, if x is odd, then (A(x−1)/2)2A = Ax−1A = Ax. The running time of the algorithm can be analyzed without the use of the master theorem. Notice that each recursive call shifts x by 1 bit (recall that shifting the bits left/right is the same as multiplying/dividing by powers of two). Since there are n bits of x, there will be n recursivecalls. Eachcallperformsannbitmultiplicationintheworstcase. Therefore, with n calls, each taking O(n2) time, the overall running time is O(n3). You can of courseuseKaratsubamultiplicationtogetabetterboundofO(n2.59)butthatdoesn’t make the O(n3) wrong, just worse. 5 Algorithms for Matrix Multiplication Let’s conclude with algorithms for matrix multiplication. Here, we want to compute C = AB, where A and B are both matrices of size n × n. In this model, we assume that multiplications and additions take unit time, and we are concerned more with the number of operations. The matricies may have n2 elements each, but we parametrize the problem by their dimension, as a function of just n. 1. 3: Arithmetic-6\n2. Basic Matrix Multiplication Algorithm: Input: Two n×n matrices, A and B Output: C, where C = AB Theclassicwaytocomputeamatrixmultiplicationisbycomputing, foreveryelement C of C, i,j n (cid:88) C = A B i,j i,k k,j k=0 The computation for each element of C takes a linear number of steps, and there n2 elements, so this gives us a total runtime of O(n3). Lets try a different approach. 3. Basic Divide and Conquer Matrix Multiplication Algorithm: Consider splitting A and B into smaller submatrices: (cid:20) (cid:21) (cid:20) (cid:21) A A B B A = 11 12 B = 11 12 A A B B 21 22 21 22 Then (cid:20) (cid:21) A B +A B A B +A B C = AB = 11 11 12 21 11 12 12 22 A B +A B A B +A B 21 11 22 21 21 12 22 22 We can see that computing C takes can be done with 8 subproblems, where the matrices in each call are of size n/2 by n/2. Once again, we can use the master theorem to do an analysis of the runtime. In this case, a = 8 since there are 8 recursive calls, b = 2 since the size of the input to each recursive call is halving, and d = 2 since there are multiple n2 sized additions happening during the combination step. Each addition takes unit time, but there are additions for each element of the matrix, giving quadratically many additions. This gives us the following recurrence: T(n) = 8T(n/2) + O(n2). Using the master theorem, we can tell that the time complexity is O(nlog 2 8) = O(n3). However, we can definitely definitely improve upon this runtime by being clever. 4. Better Divide and Conquer Matrix Multiplication Algorithm: We will employ the same idea that was used to improve the first divide and conquer algorithm for multiplication: reduce the number of subproblems! 3: Arithmetic-7\nM1 = (A +A )(B +B ) 11 22 11 22 M2 = (A +A )(B ) 21 22 11 M3 = (A )(B −B ) 11 12 22 M4 = (A )(B −B ) 22 21 11 M5 = (A +A )(B ) 11 12 22 M6 = (A −A )(B +B ) 21 11 11 12 M7 = (A −A )(B +B ) 12 22 21 22 Then (cid:20) (cid:21) M +M −M +M M +M C = AB = 1 4 5 7 3 5 M +M M −M +M +M 2 4 1 2 3 6 Using the master theorem, the runtime of this new approach is O(nlog 2 7) which is approximately O(n2.81). The current best known algorithm for matrix multiplication has a runtime of O(n2.37286). It is an open problem, and an active area of research to improve on this. 3: Arithmetic-8"},
{"file": "dsa_slides/04_Graphs (1).pdf", "content": "CS 3510 Algorithms: Graph Representations and DFS Aaron Hillegass Georgia Tech\n2/28 Graphs\n3/28 Undirected Graphs 𝑉 (the vertices) are a set: {𝑣 , …𝑣 } 1 𝑛 𝐸 (the edges) are a set of pairs of vertices: {{𝑢, 𝑣} | 𝑢, 𝑣 ∈ 𝑉 } 𝐺 (the graph) is the pair (𝑉 , 𝐸) A path 𝑝 on 𝐺 a list [𝑣 | 𝑣 ∈ 𝑉 ] such that {𝑣 , 𝑣 } ∈ 𝐸 𝑖 𝑖 𝑖 𝑖+1 Two vertices 𝑢 and 𝑣 are connected if there is a path from 𝑢 to 𝑣. The graph is connected if every pair of vertices in 𝑉 are connected.\n4/28 Directed Graphs 𝐸 (the edges) are a subset of 𝑉 × 𝑉 ((𝑢, 𝑣) not allowed if 𝑢 = 𝑣) A path 𝑝 from 𝑢 to 𝑤 a list [𝑣 | 𝑣 ∈ 𝑉 ] of length 𝑛 such that 𝑖 𝑖 (𝑣 , 𝑣 ) ∈ 𝐸 for all 𝑖 < 𝑛. 𝑖 𝑖+1 𝑢 and 𝑣 are strongly connected if there is a path from 𝑢 to 𝑣 and from 𝑣 to 𝑢. The graph is strong connected if every pair of vertices in 𝑉 are strongly connected. (What is a weakly connected graph? If you replaced the directed edges with undirected edges, the resulting undirected graph is connected.))\n5/28 Representations: Adjacency Matrix 0 3 2 0 ( ) 0 0 0 5 ( ) ( ) 0 4 0 1 0 2 0 0 ( ) Size: |𝑉 |2 Good for dense graphs; dense graphs are uncommon. Undirected graph? Symmetric matrix. Unweighted graph? 0 or 1.\n6/28 Representations: Adjacency List Size: |𝑉 | + |𝐸| Good for sparse graphs\n7/28 DFS 1 define dfs(𝐺): 1 define explore(𝐺, 𝑣, visited): 2 for 𝑣 ∈ 𝑉 : visited[𝑣] ≔ true 2 visited[𝑣] ≔ false 3 3 for each edge (𝑣, 𝑢) ∈ 𝐸: 4 for 𝑣 ∈ 𝑉 : 4 if not visited[𝑢]: 5 if not visited[𝑣]: explore(𝐺, 𝑢, visited) 5 explore(𝐺, 𝑣, visited) 6\n𝐺 explore If is undirected, will be called once for every connected component.\n9/28 Previsit and Postvisit Ordering 1 define explore(𝐺, 𝑣, pre, post, clock): 1 define dfs(𝐺): 2 pre[𝑣] ≔ counter.next() 2 Create pre, post tables 3 for each edge (𝑣, 𝑢) ∈ 𝐸: 3 Initialize counter to zero 4 if 𝑢 ∉ pre: 4 for 𝑣 ∈ 𝑉 : 5 explore(𝐺, 𝑢, pre, post, clock) 5 if 𝑣 ∉ pre: post[𝑣] ≔ counter.next() explore(𝐺, 𝑣, pre, post, counter) 6 6\n10/28 Practice\n11/28 Check\n12/28 Edge Types Forward, child: pre(𝑢) < pre(𝑣) < post(𝑣) < post(𝑢) Back: pre(𝑣) < pre(𝑢) < post(𝑢) < post(𝑣) Cross: pre(𝑣) < post(𝑣) < pre(𝑢) < post(𝑢)\n13/28 Directed Acyclic Graphs Every DAG has at least one source and one sink.\nCan we use DFS to figure out if a graph has a cycle?\n15/28 Is it a DAG? DFS finds no back edge if and only if graph is acyclic. Back edge implies a cycle seems obvious. Suppose there is a cycle 𝑣 → 𝑣 → … → 𝑣 . Let 𝑣 be the 0 1 0 𝑖 first vertex in the cycle found by DFS. All others in cycle are reachable from 𝑣 , including 𝑣 . There is an edge 𝑖 𝑖−1 from 𝑣 to 𝑣 – it is a back edge. 𝑖−1 𝑖\n16/28 DAGs and DFS In a DAG, every edge leads to a vertex with a lower post number. Vertex with lowest post number must be a sink. Vertex with highest post number must be a source.\n17/28 Topological Sort List vertices such that no edges go left. Smallest post number comes last.\n18/28 Strongly Connected Components We can break a directed graph into its strongly connected components.\n19/28 Strongly Connected Metagraph The result is a directed graph. Anything special about the metagraph?\n20/28 Strongly Connected Metagraph The metagraph of strongly connected components is always a DAG How do we know? Image if there were a cycle in the metagraph.\nHow can we identify the strongly connected components quickly?\n22/28 A sink component Explore on a sink component will explore exactly every vertex in the component. How can we find the sink component?\n23/28 Finding a source component If 𝐶 and 𝐶′ are components with an edge from 𝐶 to 𝐶′ , then the highest post number in 𝐶 is higher than the post number in 𝐶′ . Two cases: DFS starts in 𝐶 (explore sees all of 𝐶′ ) or DFS starts in 𝐶′ (explore sees none of 𝐶).\nThe vertex with the highest post number must be in a source component. But…we need to find a node in a sink component.\n25/28 Reverse Graph Let 𝐺𝑅 be the same as 𝐺, but with every edge reversed. 𝐺𝑅 and 𝐺 have the same connected components. However, a source component in 𝐺𝑅 is a sink component in 𝐺.\n26/28 Identifying Components • Make reverse graph 𝐺𝑅 • Run DFS on 𝐺𝑅 and note post numbering • While 𝐺 has vertices: • Vertex with highest post value is in a sink component • Run Explore on that vertex in 𝐺 to find vertices in sink component. • Delete components of current sink component\n27/28 Identifying Components\nQuestions? Slides by Aaron Hillegass"},
{"file": "dsa_slides/08_DynamicProgramming.pdf", "content": "CS 3510 Algorithms: Intro to Dynamic Programming Aaron Hillegass Georgia Tech\n2/56 Problem 4 A C 2 1 define search(𝐺,𝑣 ,𝑣 ): 𝑠 𝑓 2 2 prev ≔ new map, 𝑞 ≔ new priority queue 1 1 1 3 𝑞.insert((𝑣 ,False),0) 𝑠 F S 3 4 4 while (𝑣,has_used),𝑑 ≔ 𝑞.pop_head(): 5 if 𝑣 = 𝑣 and has_used = True: B 𝑓 6 return backtrack(prev,(𝑣 ,True)) 𝑓 2 A,T C,T 7 forall (𝑣,𝑢),𝑐 ∈ 𝐺.𝐸 0 8 if has_used: A,F C,F 0 2 2 9 if 𝑞.decrease_or_insert((𝑢,True),𝑑 + 𝑐): 1 0 10 prev[(𝑢,True)] ≔ (𝑣,True) 1 0 S,F 0 1 11 else: 3 1 0 F,T 12 if 𝑞.decrease_or_insert((𝑢,True),𝑑): B,F 13 prev[(𝑢,True)] ≔ (𝑣,False) 0 4 14 if 𝑞.decrease_or_insert((𝑢,False),𝑑 + 𝑐): 15 prev[(𝑢,False)] ≔ (𝑣,False) B,T\n3/56 Bad Recursion\nMemoization: Calculate once, fib(296) store it, and never calculate it again!\n5/56 Dynamic Programming fib(12) 1 define fib(𝑛): dp ≔ new array [0…𝑛] 2 dp[0] ≔ 0 3 dp[1] ≔ 1 4 5 for 𝑖 in 2…𝑛 dp[𝑖] ≔ dp[𝑖 − 2] + dp[𝑖 − 1] 6 7 return dp[𝑛]\n6/56 Dynamic Programming fib(n) • Complexity: 𝑂(𝑛) • Space: 𝑂(𝑛)\n7/56 What is “Dynamic Programming”? • Uses memoization to prevent redundant computation • Works from base cases up (inductively) instead of from the top down (recursively) Complexity and space tend to be the same in dynamic programming. (Assuming you aren’t reusing or freeing memory as you go.) (But you probably should, if you can.)\n8/56 Tidy Memoization Calculate fib(296) once, and store it until you don’t need it any more!\n9/56 Tidy Dynamic Programming fib(n) 1 define fib(𝑛): dp ≔ new array [0…1] 2 dp[0] ≔ 0 3 dp[1] ≔ 1 4 5 for 𝑖 in 2…𝑛 dp[𝑖 mod 2] ≔ dp[0] + dp[1] 6 7 return dp[𝑛 mod 2]\nYou can hop up 1, 2, or 3 steps. How many different ways are there to get to 𝑛 step ?\n11/56 Take 1,2 or 3 steps 1 define routes(𝑛): 2+4+7 = 13 n dp ≔ array[1..𝑛] 2 1+2+4 = 7 3 dp[1] ≔ 1 n-1 dp[2] ≔ 2 4 4 n-2 dp[3] ≔ 4 5 2 n-3 6 for i in [4... n]: 1 dp[𝑖] ≔ dp[𝑖 − 3] + dp[𝑖 − 2] + dp[𝑖 − 1] 1 7 8 return dp[𝑛] 0\nHow could you save space? What would be a good data structure for this?\n13/56 Base cases can be strange 1 define routes(𝑛): 2+4+7 = 13 n dp ≔ array[0..𝑛] 2 1+2+4 = 7 3 dp[0] ≔ 1 n-1 dp[1] ≔ 1 1+1+2 = 4 4 n-2 dp[2] ≔ 2 5 2 n-3 6 for i in [3... n]: 1 dp[𝑖] ≔ dp[𝑖 − 3] + dp[𝑖 − 2] + dp[𝑖 − 1] 1 7 8 return dp[𝑛] 1 0\n14/56 Example: Increment or double Starting at zero you can apply one of two operation: • Increment by one • Double Minimum number of operations to get from 0 to 𝑛? n 0 1 2 3 4 5 6 7 8 9 ops 0 1 2 3 3 4 4 5 4 5 What is the base case(s)? What is the recurrence?\n15/56 Example: Increment or Double 1 define inc_or_double(𝑛): dp ≔ array[0..𝑛] 2 dp[0] ≔ 0 3 4 for 𝑖 ∈ [1..𝑛]: 5 if 𝑖 is odd: dp[𝑖] ≔ dp[𝑖 − 1] + 1 6 7 else: 𝑖 dp[𝑖] ≔ min(dp[ ], dp[𝑖 − 1]) + 1 8 2 9 return dp[𝑛] Complexity and space: 𝑂(𝑛)\nYou are robbing houses on a street 𝑛 with houses. The value of what you {ℎ , ℎ , …, ℎ } can steal in each house is . 1 2 𝑛 You may not rob any houses that are adjacent to each other. How much can you get?\n17/56 Robbing Houses Define dp[𝑖] to be what you could steal if you only had {ℎ , …, ℎ }. 1 𝑖 Base cases: dp[0] = 0, dp[1] = ℎ 1 For any house ℎ , you can either rob it or not. 𝑖 If you rob ℎ , you can’t rob ℎ . 𝑖 𝑖−1 dp[𝑖] ≔ max(ℎ + dp[𝑖 − 2], dp[𝑖 − 1]) 𝑖 dp[𝑛] is the answer.\n18/56 Robbing Houses dp ≔ array of size 𝑛 1 2 dp[0] ≔ 0, dp[1] ≔ ℎ[1] 3 for 𝑖 ∈ [2…𝑛]: no_rob_v = dp[𝑖 − 1] 4 rob_v = dp[𝑖 − 2] + ℎ[𝑖] 5 dp[𝑖] = max(no_rob_v, rob_v) 6 7 return dp[𝑛] i 0 1 2 3 4 5 6 7 8 9 10 h 12 30 3 3 50 51 50 9 9 9 dp 0 12 30 30 33 80 84 130 130 139 139\n19/56 And…? Dynamic programs usually answer one question easily: “What is the max that I can make on this street?” Often there is a related question: “Which houses should I rob to achieve that max?” i 0 1 2 3 4 5 6 7 8 9 10 h 12 30 3 3 50 51 50 9 9 9 dp 0 12 30 30 33 80 84 130 130 139 139\n20/56 Backtracking on dp rob_set ≔ an empty set 1 𝑖 ≔ 𝑛 2 3 while 𝑖 ≥ 1: 4 if dp[𝑖] == dp[𝑖 − 1]: 𝑖 ≔ 𝑖 − 1 5 6 else: rob_set.add(𝑖) 7 𝑖 ≔ 𝑖 − 2 8 9 return rob_set i 0 1 2 3 4 5 6 7 8 9 10 h 12 30 3 3 50 51 50 9 9 9 dp 0 12 30 30 33 80 84 130 130 139 139\n21/56 But what if you are tidy?\n22/56 The idea: Gather as you go\n23/56 Gathering as you go dp ≔ circular queue of theft values 1 rob_sets ≔ circular queue of sets of houses to rob 2 3 dp.push_back(0), rob_sets.push_back({}) 4 dp.push_back(ℎ[1]), rob_sets.push_back({1}) 5 for 𝑖 ∈ [2..𝑛]: 𝑣 ≔ dp.pop_front() + ℎ[𝑖] 6 house_set = rob_sets.pop_front() 7 8 if 𝑣 > dp.peek_front(): dp.push_back(𝑣) 9 rob_sets.push_back(house_set ∪ {𝑖}) 10 11 else: dp.push_back(dp.peek_front()) 12 rob_sets.push_back(rob_sets.peek()) 13 14 return (dp.back(),rob_sets.pop())\n24/56 Example: Paths through a grid You are given a 𝑟 × 𝑐 grid. You can only move down or right. Some of the squares may not be entered. How many different routes from (0,0) to (r-1, c-1)?\n25/56 Use a 2D array for memoization dp[i,j] represents number of routes (0,0) to (i,j). Base case: • The cells on left are 1 if reachable, 0 if not • The cells on the top are 1 if reachable, 0 if not dp[i, j] = dp[i-1, j] + db[i, j-1] Answer is dp[r-1, c-1]\n26/56 Implementation: Base case dp := r by c array dp[0,0] = 0 blocked := false; for i in 1 to r-1: if (i, 0) in forbidden: blocked := true; if blocked dp[i,0] = 0 else db[i,0] = 1 ...same with start of each column...\n27/56 Implementation: Recurrence for i in 1 to r-1: for j in 1 to c-1: if (i, j) in forbidden: dp[i,j] = 0 else: dp[i,j] = dp[i-1, j] + dp[i, j-1] return dp[r-1, c-1]\n28/56 Tidy version Keep swapping two 1-dimenional arrays\nHow long is the longest common substring in “TRADERS” and “SPRADDE”?\n30/56 A Substring is a Suffix of a Prefix Approach: • Look at all possible prefixes for both strings • Find length of longest common suffix\n31/56 The bad recursive solution Define shortened() to return the string missing its last character len_com_sub_str(a,b) -> int: if a or b are empty strings, return 0 ender := len_of_common_suffix(a, b) more_a := len_com_sub_str(a, b.shortened()) more_b := len_com_sub_str(a.shortened(), b) return max(ender, more_a, more_b) How many times is len_com_sub_str called for two seven-character strings? 6863 times!\n32/56 Longest common substring Given strings [𝑎 , 𝑎 , …, 𝑎 ] and 1 2 𝑛 [𝑏 , 𝑏 , …, 𝑏 ]. 1 2 𝑚 dp[i, j] = length of common suffix of prefixes [𝑎 , 𝑎 , …, 𝑎 ] and [𝑏 , 𝑏 , …, 𝑏 ] 1 2 𝑖 1 2 𝑗\n33/56 Two strings have common suffix 𝑘 letters S P R A D D E long, you add 𝑎 to one and 𝑏 to the other: 𝑖 𝑗 T • If 𝑎 == 𝑏 , how long is the common 𝑖 𝑗 R suffix now? A 2 • If 𝑎 ≠ 𝑏 , how long is the common suffix 𝑖 𝑗 D 3 now? E 0 if a[i] == b[j]: R dp[i,j] := dp[i-1, j-1] + 1 else S dp[i,j] := 0\n34/56 Longest common substring: Base case? If one of the strings has zero length, how long is the common suffix?\n35/56 Longest common substring S P R A D D E 0 0 0 0 0 0 0 0 T 0 0 0 0 0 0 0 0 R 0 0 0 1 0 0 0 0 What is the length of the longest A 0 0 0 0 2 0 0 0 substring? D 0 0 0 0 0 3 1 0 E 0 0 0 0 0 0 0 2 R 0 0 0 1 0 0 0 0 S 0 1 0 0 0 0 0 0\n36/56 Longest common substring: pseudocode S P R A D D E 0 0 0 0 0 0 0 0 dp := array (n+1) x (m+1) of zeros T 0 0 0 0 0 0 0 0 for i in 1 to n: R 0 0 0 1 0 0 0 0 for j in 1 to m: A 0 0 0 0 2 0 0 0 if a[i] == b[j]: D 0 0 0 0 0 3 1 0 dp[i,j] := dp[i-1,j-1] + 1 E 0 0 0 0 0 0 0 2 return max(dp) R 0 0 0 1 0 0 0 0 S 0 1 0 0 0 0 0 0\nYeah, but, what is the longest common substring?\n38/56 Backtracking S P R A D D E 0 0 0 0 0 0 0 0 T 0 0 0 0 0 0 0 0 R 0 0 0 1 0 0 0 0 A 0 0 0 0 2 0 0 0 D 0 0 0 0 0 3 1 0 E 0 0 0 0 0 0 0 2 R 0 0 0 1 0 0 0 0 S 0 1 0 0 0 0 0 0\n39/56 Longest common substring: pseudocode S P R A D D E 0 0 0 0 0 0 0 0 (Reminder: 𝑎 , …, 𝑎 is one-based.) T 0 0 0 0 0 0 0 0 1 𝑛 R 0 0 0 1 0 0 0 0 m := max(dp) A 0 0 0 0 2 0 0 0 (m_a, m_b) := argmax(dp) D 0 0 0 0 0 3 1 0 s := m_a - m + 1 E 0 0 0 0 0 0 0 2 return substring(a,start=s, length=m) R 0 0 0 1 0 0 0 0 S 0 1 0 0 0 0 0 0\nHow long is the longest common subsequence in “ABCDABA” and “BAACADB”?\n41/56 Length of longest common subsequence = 4\n42/56 Longest common subsequence Given sequences [𝑎 , 𝑎 , …𝑎 ] and 1 2 𝑛 [𝑏 , 𝑏 , …𝑏 ]. 1 2 𝑚 dp[i, j] = length of max common subsequence of [𝑎 , 𝑎 , …𝑎 ] and 1 2 𝑖 [𝑏 , 𝑏 , …𝑏 ] 1 2 𝑗\n43/56 Recurrence: Longest common subsequence [𝑎 , …, 𝑎 ] and [𝑏 , …, 𝑏 ] have a common 1 𝑖−1 1 𝑗−1 subsequence of length 𝑘. What if 𝑎 == 𝑏 ? How long is the common 𝑖 𝑗 subsequence of [𝑎 , …, 𝑎 ] and [𝑏 , …, 𝑏 ]? 1 𝑖 1 𝑗 What if 𝑎 ≠ 𝑏 ? 𝑖 𝑗\n44/56 Longest common subsequence: psuedocode 1 define longest_common_subsequence(a, b): 𝑛 ≔ len(𝑎), 𝑚 ≔ len(𝑏) 2 3 for 𝑖 ∈ [0…𝑛] : dp[𝑖, 0] ≔ 0 4 for 𝑗 ∈ [0…𝑚] : dp[0, 𝑗] ≔ 0 5 for 𝑖 ∈ [1…𝑛]: 6 for 𝑗 ∈ [1…𝑚]: 7 if 𝑎[𝑖] == 𝑏[𝑗]: dp[𝑖, 𝑗] ≔ dp[𝑖 − 1, 𝑗 − 1] + 1 8 9 else: dp[𝑖, 𝑗] ≔ max(dp[𝑖 − 1, 𝑗], dp[𝑖, 𝑗 − 1]) 10 11 return dp[𝑛, 𝑚]\nYeah, but what are the longest common subsequences in “ABCDABA” and “BAACADB”?\n46/56 dp array as a DAG Every path represents a common subsequence. Paths that starts at a 1 and end at 4 is a maximal common subsequence.\n47/56 Maximal Common Subsequences BCDB ACDB BCAB ACAB\n48/56 1 define sequences_ending_at(𝑖, 𝑗, 𝑎, 𝑏, dp): 2 if 𝑎[𝑖] == 𝑏[𝑗]: 3 if dp[𝑖][𝑗] == 1: 4 return new set {(𝑎[𝑖])} 5 else: 𝑟 ≔ sequences_ending_at(𝑖 − 1,𝑗 − 1,𝑎,𝑏,dp) 6 7 Append 𝑎[𝑖] to each tuple in 𝑟 8 return 𝑟 9 else: 10 𝑟 ≔ empty set 11 if dp[𝑖 − 1][𝑗] == dp[𝑖][𝑗]: 𝑟 ≔ union(𝑟,sequences_ending_at(𝑖 − 1,𝑗,𝑎,𝑏,dp)) 12 13 if dp[𝑖][𝑗 − 1] == dp[𝑖][𝑗]: 𝑟 ≔ union(𝑟,sequences_ending_at(𝑖,𝑗 − 1,𝑎,𝑏,dp)) 14 15 return 𝑟\n49/56\nHow long is the longest palindomic subsequence of ? ABDCBBA ‘ABCBA’ or ‘ABBBA’\n51/56 Already done longest common subsequences? Longest pandomic subseq of [𝑎 , …, 𝑎 ] is longest common subsequence 1 𝑛 of [𝑎 , …, 𝑎 ] and [𝑎 , …, 𝑎 ] 1 𝑛 𝑛 1\n52/56 dp dp[i,j] will be length longest A B D C B B A pandromic subsequence of 1 ? A [𝑎 , …, 𝑎 ] for 𝑖 ≤ 𝑗 𝑖 𝑗 0 1 B Base cases: 0 1 D 0 1 C • 𝑖 = 𝑗: dp[i,j] = 1 0 1 B • 𝑖 > 𝑗: dp[i,j] = 0 0 1 B Answer: dp[1,n] 0 1 A gnirtsbus ni rettel tsrf last letter in substring\n53/56 Recurrence direction? for s in 1..n: for r in 1..n-s: c = r + s dp[r,c] = ?\n54/56 Recurrence A B D C B B A if 𝑎 = 𝑎 , a letter is added to both 1 ? A 𝑖 𝑗 sides of the palindrome of 0 1 B 𝑎 …𝑎 . 0 1 D 𝑖+1 𝑗−1 0 1 C else, use the max of 𝑎 …𝑎 and 𝑖+1 𝑗 0 1 B 𝑎 …𝑎 𝑖 𝑗−1 0 1 B 0 1 A gnirtsbus ni rettel tsrf last letter in substring\n55/56 Psuedocode dp ≔ 𝑛 × 𝑛 array o zeroes 1 2 for 𝑖 ∈ [1…𝑛]: dp[𝑖, 𝑖] ≔ 1 3 4 for 𝑠 ∈ [1..𝑛]: 5 for 𝑖 ∈ [1..𝑛 − 𝑠]: 𝑗 ≔ 𝑖 + 𝑠 6 7 if 𝑎[𝑖] = 𝑎[𝑗]: dp[𝑖, 𝑗] ≔ dp[𝑖 + 1, 𝑗 − 1] + 2 8 9 else: dp[𝑖, 𝑗] ≔ max(dp[𝑖 + 1, 𝑗], dp[𝑖, 𝑗 − 1]) 10 11 return dp[1, 𝑛]\nQuestions? Slides by Aaron Hillegass"},
{"file": "dsa_slides/Notes_kruskalPrim.pdf", "content": "CS 3510 Algorithms 2/06/2024 Lecture 7: Kruskal’s Algorithm for Minimum Spanning Trees Lecturer: Abrahim Ladha Scribe(s): Aditya Kumaran 1 Greedy Algorithms We’re going to talk about a class of algorithms which is very intuitive for people: when you have an optimization problem, pick the local best, and then solve the rest of the problem without that piece. The obvious issue though is that picking the local maximums might not lead to a global maximum. As an example, say you’re trying to pack the most amount of luggage into a given amount of space. Say you have suitcases of size 35, 40, 40, and 70, but you only have 100 units of space. If you picked the largest piece of luggage, you’d pick 70, even though you could pick the two 40s to get 80 units of luggage packed. A greedy algorithm could also try picking the smallest suitcases first, but would again, miss the optimal solution. This is an unlucky case, and is very close to a more interesting problem we’ll look at later. However it’s not the rule - in fact, in many situations, Greedy Algorithm’s work well. Two examples are Huffman Coding for compression and Kruskal’s algorithm for computing minimum spanning trees. These algorithms and most other greedy algorithms are simple, but that ease comes with the challenge of proving that greed will lead to the best outcome. 2 Minimum Spanning Tree What is the minimum spanning tree (MST) problem though? To put it succinctly, given a weighted undirected graph, give a graph which is still connected but with the smallest weights. An MST T ⊂ G is a set of edges such that it is • Minimum, the sum of the edge weights is less than or equal to any other MST • Spanning, each v ∈ V is an endpoint of atleast one edge in the MST • Tree, there are exactly |V|−1 edges and no cycle. This problem has many real world applications, in fact this algorithm and many other interesting graph problems came out of Bell Labs. If you think about laying out phone lines between towns, the cost of the phone lines, could be the distance between the towns, and you want to spend the least wire possible (and therefore, money) while connecting them. 7: Kruskal’s Algorithm for Minimum Spanning Trees-1\n7 9 7 9 14 8 8 12 20 23 11 16 9 9 6 4 6 4 Figure 1: A weighted graph and it’s MST. Of course, in that scenario, you might want more connections for redundancy but, for simply finding the minimum spanning tree, we don’t care. Although this exercise reveals a couple interesting properties about minimum spanning trees. There will be no cycles in the resulting MST because if there were a cycle, we could remove the highest value edge and get a smaller spanning tree. Also as the problem requires, the graph must be as small as possible while being connected, so it will have |V|−1 edges. Trees are graphs with special properties, and come up often in computer science. They have many equivalent characterizations. Trees are acyclic, have n−1 edges and n nodes, and for any two vertices u,v in the tree, there exists a unique path from u to v. (If there were two or more paths, compose them to make a cycle and now its not a tree anymore). 3 Kruskal’s Algorithm Since it’s so simple, we’ll present Kruskal’s algorithm now, and then prove it’s correctness later. Kruskal’s algorithm is rather simple and what you might come up with by thinking about this problem: at each step, add the smallest edge to a set which does not form a cycle with edges within that set. Of course, checking for cycles is easier said than done; although we humans can do it quickly, the best algorithm we’ve discussed talks linear time, meaning this would take quadratic time (good, but not great). 7: Kruskal’s Algorithm for Minimum Spanning Trees-2\ndef kruskals(G, w): for all v in V: makeset(v) X = {} sort E by weight for all (u, v) in E if find (u) is not find(v): x = x + {(u, v)} union (find(u), find(v)) Figure 2: Explore routine on a graph and node. To check for cycles, this algorithm relies on a unique data structure: union find. Now, this data structure isn’t terribly important for the class, but it’s very important for under- standing this algorithm. It operates on sets, and does two things: set union, and finding the set an element is within. Both operations are logarithmic. 1 7 A B 3 9 2 2 C D 4 11 7 E Figure 3: A simple weighted graph. Let’s walk through how this algorithm works. If we sort the edges in this graph we get [2,2,3,4,7,7,9,11]. As we iterate over this list we get the following results. 1Theproofoftheseruntimesisinthebook. It’sfairlyinteresting,butnotimportantfortoday. Ifyou’re interested in learning more refer to 5.1.4. 7: Kruskal’s Algorithm for Minimum Spanning Trees-3\n{A}, {B}, {C}, {D}, {E} 2 {A, D}, {B}, {C}, {E} 2 {A, D, C}, {B}, {E} 3 find(A) equals find(C) 4 {A, D, C}, {B, E} 7 find(A) equals find(C) 7 {A, D, C, B, E} 9 find(B) equals find(D) 11 find(D) equals find(E) Figure 4: The sets created by Kruskal’s algorithm while iterating over edges. Now let’s analyze the runtime of this algorithm. Making the sets for union find will take O(|V|) time. Sorting E by weight will be O(|E|log|E|). There will be |E| itera- tions over the sorted edges, each doing log|V|. Adding these together we’ll get O(|V|+ |E|log|E|+|E|log|V|). Note that we know O(log|E|) = O(log|V|), so the overall runtime is O(|E|log|E|). 4 Proof and The Cut Property Even though we have an intuitive algorithm, we need to show it is correct. How would we do this? Contradiction is one good option: we can assume a greedy approach does not give the optimal solution and show why the greedy approach would have chosen this other solution instead, creating a contradiction. In this proof, we’ll use this form with induction layered on top. Say we have some edges already chosen X which a part of some MST T (X ⊂ T). We want to show that the next e chosen by the algorithm will also be part of an MST (X+e ⊂ T′). 2 It’s important to note here that T is some MST, but no where in our proof do we try to show that T will be a certain MST. 2There can be multiple MSTs in a graph; if you’re not sure why this would be look at one of the graphs above, with the change that all edges are of equal weight. 7: Kruskal’s Algorithm for Minimum Spanning Trees-4\nS V −S e e′ Figure 5: A cut creating two parts of a graph with edges e and e′ going through the cut. An important of this proof is the Cut Property which states that if you have have a set of edges X, a set of vertices S such that no edge in X crosses S to V −S, and e be the lightest edge across the partition, then e will be part of an MST. Intuitively we know that one edge across the cut must be part of all spanning trees. If a spanning tree consisted of none of the edges of a cut, it would not be spanning. However why must the lightest edge be part of some MST? Say edges X are part of some MST T; if e is also part of T there is nothing to prove. However say e is not part of T, we’ll build a new MST T′. S V −S e e′ Figure 6: An MST crossing a cut with edge e′, and another edge crossing the cut e. Say we have the possible MST, T in red above, and we add edge e to this MST; we have now formed a cycle. This cycle has some other edge across the cut e′, and if we remove this edge, we get the MST T′ = X +e−e′. T′ is also a tree because it both spans the graph 7: Kruskal’s Algorithm for Minimum Spanning Trees-5\nand has |V|−1 edges. However we must also show it is the minimum spanning tree as we will do. We can calculate the weight of the new treeT′, as the weight of the old tree T minus the removed edge e′ plus the additional edge e: w(T′) = w(T)−w(e′)+w(e). We know that e is the lightest edge by definition, meaning w(e) ≤ w(e′). Then we know that w(T′) ≤ w(T), but since T is an MST, w(T) is minimal. It must be the case that they are of equal weight. 3 Now the justification of Kruskal’s algorithm is rather simple. At each step, we have a partial solution X and the lightest edge e which hasn’t yet been considered. We check that e does not form a cycle, implying that it would be the lightest edge in some cut because it is the first edge considered crossing said cut. Now, we know that it satisfies the cut property, making the algorithm find an MST. 3Notice here what must be true about the ordering of w(e′) and w(e) if w(T) = w(T′). Consider how this might effect the number of MSTs, and how the number of MSTs might change if all edge weights were unique. 7: Kruskal’s Algorithm for Minimum Spanning Trees-6"},
{"file": "dsa_slides/01b_Notes.pdf", "content": "CS 3510 Algorithms 1/11/2024 Lecture 2: Merge Sort and Master Theorem Lecturer: Abrahim Ladha Scribe(s): Aditya Kumaran Whendoweusedivideandconqueralgorithms? Thesealgorithmsdividethelargerproblem into smaller, easier-to-solve subproblems, and use their solutions to help find a solution to the larger problem. 1 Merge Sort Givenanarrayoflengthn,wewanttoorderthearraysuchthattheyaresortedinincreasing order. We assume here that all the elements are bounded, for easy comparisons. Pseudocode : def mergesort(A[1..n]): ## Base Case if n == 1: return A ## Split problem into two subarrays, recursively call mergesort on both L = mergesort(A[1..floor(n/2)] R = mergesort(A[floor(n/2)+1..n] ## Return the merge of the left and right return merge(L,R) Note : For odd length arrays, it doesn’t matter which half the middle value is folded into, as long as you make a consistent definition for your algorithm. Note : Arrays can be 0-indexed or 1-indexed depending on your own convention, as long as yourpseudocodeisconsistentwithyourdecidedconvention. Manytextbooksuse1-indexing. Toprovethecorrectnessofrecursivealgorithms,wetypicallyuseproofbyinduction. Wecan assume merge() is correct to prove correctness of mergesort(), and prove the correctness of merge() later. Lets consider the execution on A = [5,2,4,7,1,3,2,6] Notes/mergesorttrace.png When A in mergesort(A) is length 1, mergesort returns the individual arrays to the parent call. Therefore, for each parent call (Ex.- when A = [5,2]) the values of L and R would be the individual values (Ex.- L = [5], R = [2]). As we move up the callstack, we call merge() on these two halves and return the sorted arrays (Ex.- return = [2, 5]). We can see here that the heavy lifting is being done by the recombining method, what we call 2: Merge Sort and Master Theorem-1\nmerge(). 1.1 merge() merge() takes two parameters - x[1...k] and y[1...m] We can assume that both these halves are also sorted, by induction - for the smallest case of length 2, our base case will handle the P(2) case in induction. Pseudocode : def merge(x[1...k], y[1...m]): if x is length 0 return y if y is length 0 return x if x[1] <= y[1]: return x[1].join(merge(x[2...k], y[1... k])) else: return y[1].join(merge(x[1...k], y[2... k])) Note : It’s very important that every divide and conquer algorithm has a base case, or it can recurse endlessly. The beauty of this algorithm is that we are doing the smallest amount of work at every indivdual step, and combining this work in the most efficient way. Lets see if we can justify itscorrectness. Whydoesthismergethetwosortedarrays? Itisenoughforustoarguethat assuming x,y are sorted, that min(x[1],y[1]) is the first element of their sorted merger. Suppose without loss of generality that x[1] < y[1]. By assumption, note that x[1] is less than all of x, and y[1] is less than all of y. So we know since x[1] is less than the least element of y, that x[1] is less than all of y. So x[1] is less than all of the elements in both x and y meaning in the sorted merger, it should be the first element. 1.2 Example of how merge() works: LetX=[2,4,5,7]andY=[1,2,3,6],asinthelaststageofcombinationintheabovemergesort callstack. • Wefirstcomparex[1]andy[1],here2and1respectively,andreturn[1].join(merge([2,4,5,7], [2,3,6])) • We then compare 2 and 2, and return [2].join(merge([4,5,7],[2,3,6])) • We then compare 4 and 2, and return [2].join(merge([4,5,7],[3,6])) • We then compare 4 and 3, and return [3].join(merge([4,5,7],[6])) 2: Merge Sort and Master Theorem-2\n• We then compare 4 and 6, and return [4].join(merge([5,7],[6])) • We then compare 5 and 6, and return [5].join(merge([7],[6])) • We then compare 7 and 6, and return [6].join(merge([7],[])) • y is length 0, and return [7].join([],[]) The joins then return [1,2,2,3,4,5,6,7], which is our solution. merge() references every element in the array once, giving it a time complexity of O(n). But what is the time complexity of mergesort()? • This is the time taken to solve a problem of size n is T(n). • T(n) = 2T(n/2) + time complexity of merge. • Therefore, T(n) = 2T(n/2)+O(n). How do we analyze this recurrence to get the time complexity? We can look at the recursion tree and count the leaves. Rather than solve this specific recurrence, we show a general way to solve recurrences of this form. 2 Master Theorem Given a recurrence relation of the following form: T(n) = aT(n/b)+O(nd) where • a is the number of recursive calls • b is the size of each subproblem (how many pieces are you dividing the problem into?) • nd is the time it takes to divide and recombine the problem. Note : Usually, the time to divide the problem is negligible (python list slicing, etc.), and the recombination time makes up the majority of the nd. Consider the computation of a divide and conquer algorithm with such a general recur- rence. We want to count the total work done. Think of a like the arity, or the number of branches, and think of b and d like the thickness of the next level of branches. Notes/masterthm.png • At the top level, the subproblems work has already been completed, and only the final recombination needs to be done, so we see that the work done at this level is O(nd). 2: Merge Sort and Master Theorem-3\n• At the next level, there are a sub problems, and the size of each subproblem has also been reduced to size n/b, so we see the work done at this level is aO((n/b)d) • At the next level, each of the previous a subproblems has a subproblem of their own, giving us a2 subproblems. The size of the subproblem has been further divided to size n/b2, giving the total work done at this level to be a2O((n/b2)d) • Continuing this like a geometric series, the work done at the following intermediate levels is aiO((n/bi)d) • The work done at the last level is the number of leaves times the work done at each leaf. Each leaf takes O(1) to compute as a base case. The number of leaves we can compute from a little combinatorics. Given a binary tree of depth k, we see that it has 2k leaves. Our tree has arity a so the number of leaves will be ak where k is the depth. What is the depth of our tree? We continue to subdivide the problem until we divide out. That happens with (n/bi) runs out. For what i does this happen? When i = log b n. Then the number of leaves is alog b n = nlog b a. The work done at the level of the leaves is O(1)·(nlog b a) = O(nlog b a). • We sum each level to get the total work done to be lo (cid:88) g b n (cid:18) (cid:16)n(cid:17)d (cid:19) T(n) = aiO + O(nlog b a) bi i=0 • We want this in terms of big O, so we have three cases on what the dominating term is • Case 1, if the work done at each exceeds how fast the problem subdivides, then the dominatingtermwillbethefirstoneofthesummationintheseries. Thisoccurswhen d > log a and the work done is T(n) = O(nd) b • Case 2, if the work done at the leaves far exceeds the work required to recombine, the dominating term will be the number of leaves. Consider a tree with really thin branches but an insane amount of leaves. Each leaf may weigh nearly nothing but combined they are the heaviest part of this tree. This occurs when d < log a and the b work done is T(n) = log a. b • Case 3, if the work done subdivides quite neatly and doesn’t increase or decrease one way or the other, we have to weigh the entire tree. The number of subprob- lems increases to a similar ratio as the size of the subproblems and the work done. This occurs when d = log a. Every term of the sum is equivalent and we see then b that (cid:80) i lo = g 0 b n aiO((n/bi)d)+O(nlog b a) = (cid:80)l i o = g 0 b n O(nd)+O(nlog b a) = (log b n)O(nd)+ O(nd) = O(ndlogn) This gives us the final derivation of the master theorem. If T(n) = aT(n/b)+O(nd) then  O(nd) d > log a   b T(n) = O(ndlogn) d = log a b  O(nlog b a) d < log b a 2: Merge Sort and Master Theorem-4\nNote : We don’t write the base of logs when they aren’t in the exponent because asymptoti- cally(akawhenconcernedwithbigOandtimecomplexity)theyareequivalent. Thechange ofbaseformulaforlogarithmssimplymultipliesbyaconstant. Ex-O(log (n)) = O(log (n)) 2 3 In the exponent or when not “on the ground\", the base of the log does matter. 3 Examples 3.1 Mergesort We said that Mergesort has complexity from the recurrence: T(n) = 2T(n/2)+O(n) With respect to the Master Theorem, here: a = 2,b = 2,d = 1. Using the Master Theorem cases above, wehaved = log (2) = 1Therefore, thetimecomplexityofmergesortisO(n1logn) = 2 O(nlogn). We could have computed this for the specific case of mergesort, but by doing it for a general recurrence, we can easily apply this to many problems. This also tells us if modify mergesort to split the array into thirds, and solve via three recursive calls, the time complexity won’t change! log a doesn’t change when a = b. b 3.2 Binary Search As a sanity check, lets compute the run time of binary search, just to make sure that the master theorem works. You may have seen this implemented iteratively in the past, but it can be visualized recursively as well. We would split the whole array into 2 parts, and make a single recursive call on one of the halves, and the work done at each level is O(1) given that no real work is being done, we are just searching. Therefore, T(n) = 1T(n/2) + O(1) = T(n/2) + O(n0) We see that a = 1,b = 2,d = 0. Which case of the Master Theorem applies? Here d = log (1) = 0. Therefore, the time complexity of binary 2 searchisO(ndlogn) = O(logn). Thisisinfactthecorrecttimecomplexityofbinarysearch. Note : We can see from this theorem that there are 3 ways to optimize a problem: reduce work at each level, reduce number of subproblems, reduce size of each subproblem. It is not uncommon for the number of subproblems to be reduced by making smarter ones, as we will see next time. 2: Merge Sort and Master Theorem-5"},
{"file": "dsa_slides/07_BellmanFordFloydWarshaw.pdf", "content": "CS 3510 Algorithms: Bellman-Ford and Floyd-Warshall Algorithms Aaron Hillegass Georgia Tech\n2/21 What makes Dijkstra's Algorithm great? 1 dist[𝑣] ≔ ∞ for all 𝑣 ∈ 𝐺.𝑉 dist[𝑣 ] ≔ 0.0 2 0 𝑅 ≔ {𝑣 } 3 0 • Single source shortest path 4 while 𝑅 ≠ 𝐺.𝑉 : • Each node is popped off the priority 5 Pick 𝑣 ∉ 𝑅 with lowest dist queue once: when it is the cheapest 6 forall (𝑣, 𝑢), 𝑤 leaving 𝑣: unpopped. 7 if dist[𝑣] + weight[𝑉 ] < dist[𝑢]: • Runtime: 𝑂((|𝑉 | + |𝐸|) log(|𝑉 |)) dist[𝑢] ≔ dist[𝑣] + weight[𝑒] 8 prev[𝑢] ≔ 𝑣 9 10 Add 𝑣 to 𝑅\n3/21 Negative weights? 1 dist[𝑣] ≔ ∞ for all 𝑣 ∈ 𝐺.𝑉 dist[𝑣 ] ≔ 0.0 2 0 𝑅 ≔ {𝑣 } 3 0 4 while 𝑅 ≠ 𝐺.𝑉 : 5 Pick 𝑣 ∉ 𝑅 with lowest dist 6 forall (𝑣, 𝑢), 𝑤 leaving 𝑣: 7 if dist[𝑣] + weight[𝑉 ] < dist[𝑢]: dist[𝑢] ≔ dist[𝑣] + weight[𝑒] 8 prev[𝑢] ≔ 𝑣 9 10 Remove 𝑢 from 𝑅 11 Add 𝑣 to 𝑅\n4/21 Negative cycles 1 dist[𝑣] ≔ ∞ for all 𝑣 ∈ 𝐺.𝑉 dist[𝑣 ] ≔ 0.0 2 0 𝑅 ≔ {𝑣 } 3 0 4 while 𝑅 ≠ 𝐺.𝑉 : 5 Pick 𝑣 ∉ 𝑅 with lowest dist 6 forall (𝑣, 𝑢), 𝑤 leaving 𝑣: 7 if dist[𝑣] + weight[𝑉 ] < dist[𝑢]: dist[𝑢] ≔ dist[𝑣] + weight[𝑒] 8 prev[𝑢] ≔ 𝑣 9 10 Remove 𝑢 from 𝑅 11 Add 𝑣 to 𝑅\n5/21 Add a constant to all weights?\n6/21 What if we just update repeatedly? 1 for 𝑣 in 𝐺.𝑉 : dist[𝑣] = ∞ 2 prev[𝑣] = null 3 dist[𝑣 ] = 0.0 4 0 is_changed ≔ True 5 6 while is_changed: is_changed ≔ False 7 8 for (𝑢, 𝑣), 𝑤 in 𝐺.𝐸: 9 if dist[𝑢] + 𝑤 < dist[𝑣]: is_changed ≔ True 10 dist[𝑣] ≔ dist[𝑢) + 𝑤 11 prev[𝑣] ≔ 𝑢 12 13 return dist, prev\n7/21 How many times? The “truth” starts at 𝑣 and moves one edge per iteration. 0 If the graph has |𝑉 | vertices, how long can the shortest path to any 𝑢 ∈ 𝑉 be? Answer: |𝑉 | − 1 If a graph has no negative cycles, dist stops changing after |𝑉 | − 1 iterations.\n8/21 Bellman-Ford Algorithm 1 define BellmanFord(𝐺, 𝑣 ): 0 2 for 𝑣 in 𝐺.𝑉 : dist[𝑣] = ∞ 3 prev[𝑣] = null 4 dist[𝑣 ] = 0.0 5 0 6 loop |𝑉 | − 1 times: 7 for (𝑢, 𝑣), 𝑤 in 𝐺.𝐸: 8 if dist[𝑢] + 𝑤 < dist[𝑣]: dist[𝑣] ≔ dist[𝑢) + 𝑤 9 prev[𝑣] ≔ 𝑢 10 11 return dist, prev\nBut what if there is a negative cycle?\n10/21 Bellman-Ford Algorithm 1 define BellmanFord(𝐺,𝑣 ): 0 2 for 𝑣 in 𝐺.𝑉 : 3 dist[𝑣] = ∞, prev[𝑣] = null dist[𝑣 ] = 0.0 4 0 5 count ≔ 0, is_changed ≔ True 6 while is_changed and count ≠ |𝑉 |: is_changed ≔ False 7 Worst case: 𝑂(|𝑉 ‖𝐸|) > 𝑂((|𝑉 | + |𝐸|)log(|𝑉 |)) 8 for (𝑢,𝑣),𝑤 in 𝐺.𝐸: 9 if dist[𝑢] + 𝑤 < dist[𝑣]: is_changed ≔ True 10 11 dist[𝑣] ≔ dist[𝑢) + 𝑤, prev[𝑣] ≔ 𝑢 count ≔ count + 1 12 13 if count = |𝑉 | and is_changed: 14 return failure (negative cycle) 15 return dist,prev\n11/21 Bellman-Ford Algorithm\nWhat if you know the the graph is a DAG?\n13/21 SSSP on a DAG 1 define SSSPDAD(𝐺,𝑣 ): 0 2 for 𝑣 in 𝐺.𝑉 : 3 dist[𝑣] = ∞, prev[𝑣] = null dist[𝑣 ] = 0.0 4 0 ℎ ≔ topsort(𝐺) 5 6 for 𝑢 in ℎ: 7 for (𝑢,𝑣),𝑤 ∈ 𝐺.𝐸: 8 if dist[𝑢] + 𝑤 < dist[𝑣]: dist[𝑣] ≔ dist[𝑢) + 𝑤 9 prev[𝑣] ≔ 𝑢 10 11 return dist,prev 𝑂(|𝑉 | + |𝐸|)\nWhat about shortest paths between all (𝑢, 𝑣) pairs ?\n15/21 ASSP: Using what you know Dijkstra’s algorithm from every vertex: 𝑂(|𝑉 |(|𝑉 | + |𝐸|) log|𝑉 |)) ≈ 𝑂(|𝑉 |3 log|𝑉 |) We can do a little better: O(|V|^3)\n16/21 Result is a table Fill in the given stuff. (Not necessarily shortest!) To A To B To C To D To E From A 0 ∞ 3 3 9 From B 2 0 ∞ 6 ∞ From C ∞ 4 0 ∞ 2 From D ∞ 3 ∞ 0 1 From E 7 ∞ 3 ∞ 0\n17/21 Floyd-Warshall Let the vertices be numbered: [𝑣 , 𝑣 , …, 𝑣 ] 1 2 𝑛 Let shortestPath(𝑖, 𝑗, 𝑘) be the shortest path from 𝑣 𝑖 to 𝑣 by only passing through vertices from the set 𝑗 {𝑣 , 𝑣 , …, 𝑣 }. 1 2 𝑘 When looking for shortestPath(𝑖, 𝑗, 𝑘) there are only two possibilities: • It contains 𝑣 : It is shortestPath(𝑖, 𝑘, 𝑘 − 1) + 𝑘 shortestPath(𝑘, 𝑗, 𝑘 − 1) • It does not contain 𝑣 : It is shortestPath(𝑖, 𝑗, 𝑘 − 1) 𝑘 Figure out both, and take the minimum.\n18/21 Step 1: Fill in the base cases 1 forall 𝑖, 𝑗: To A To B To C To D To E dist[𝑖, 𝑗] ≔ ∞ 2 From A 0 ∞ 3 3 9 3 for 𝑖 ∈ 1..|𝑉 |: From B 2 0 ∞ 6 ∞ dist[𝑖, 𝑖] ≔ 0 From C ∞ 4 0 ∞ 2 4 From D ∞ 3 ∞ 0 1 5 for (𝑣 , 𝑣 ), 𝑤 in 𝐸: 𝑖 𝑗 From E 7 ∞ 3 ∞ 0 dist[𝑖, 𝑗] ≔ 𝑤 6\n19/21 Step 2: Loop! 1 for 𝑘 ∈ 1..|𝑉 |: 2 for 𝑖 ∈ 1..|𝑉 |: 3 for 𝑗 ∈ 1..|𝑉 |: dist[𝑖, 𝑗] ≔ min(dist[𝑖, 𝑗], dist[𝑖, 𝑘] + dist[𝑘, 𝑗]) 4 To A To B To C To D To E From A 0 ∞ 3 3 9 From B 2 0 ∞ 6 ∞ From C ∞ 4 0 ∞ 2 From D ∞ 3 ∞ 0 1 From E 7 ∞ 3 ∞ 0\n20/21 Voila! To A To B To C To D To E From A 0 6 3 3 5 From B 2 0 5 6 7 From C 9 4 0 10 2 From D 8 3 4 0 1 From E 7 7 3 10 0\nQuestions? Slides by Aaron Hillegass"},
{"file": "dsa_slides/11_GraphAndConstraintNPC.pdf", "content": "CS 3510 Algorithms: NP-Complete: Graph and Constraint Problems Aaron Hillegass Georgia Tech\n2/32 Decision Problems Sound Lame Decision Problems return True or False. Most fun problems return solutions – the result of some search. But decision problems are easy to work with. Are we messing things up by sticking to Decision Problems?\n3/32 Decision Problems and Search Problems Decision SAT: Given a CNF boolean expression Φ, is there a satisfying assignment? Search SAT: Given a CNF boolean expression Φ, what is a satisfying assignment? Claim: Search SAT is in 𝑃 if and only if Decision SAT is in 𝑃. Proof: Easy direction is assume Search SAT is in 𝑃. To solve Decision SAT, get a solution using Search SAT. Hard direction: Assume Decision SAT is in 𝑃. How can we get a solution to Search SAT?\n4/32 Using Decision SAT to solve Search SAT Assuming Decision SAT is in 𝑃. • Given an expression Φ, first use Decision SAT to see if it is satisfiable. No? You’re done. • Otherwise, find 𝑥 : 1 • Replace 𝑥 in Φ with True. 1 • Use Decision SAT to see if that is satisifiable? • Yes? 𝑥 ≔ True. No? 𝑥 ≔ False. 1 1 • Substitute that value in for 𝑥 , and repeat for 𝑥 . And 𝑥 . etc. 1 2 3 Can solve Search SAT by running Decision SAT polynomial times.\n5/32 INDSET Given an undirected graph 𝐺 = (𝑉 , 𝐸) and 𝑘, is there a subset 𝑆 ⊂ 𝑉 of size 𝑘 such that no two vertices in 𝑆 share an edge?\nHow do we prove INDSET is NP- Complete?\n7/32 INDSET is NP Given an undirected graph 𝐺 = (𝑉 , 𝐸) and 𝑘, is there a subset 𝑆 ⊂ 𝑉 of size 𝑘 such that no two vertices in 𝑆 share an edge? Witness? Can we reduce 3-SAT to INDSET? Given a boolean expression Φ, can we construct a graph 𝐺 such that finding an assignment for Φ is equivalent to finding an independent set of some size 𝑘 in 𝐺?\n8/32 Construction Φ = (¬𝑥 ∨ 𝑦 ∨ ¬𝑧) ∧ (𝑥 ∨ ¬𝑦 ∨ 𝑧) ∧ (𝑥 ∨ 𝑦 ∨ 𝑧) ∧ (¬𝑥 ∨ ¬𝑦) y ¬y y ¬y ¬x ¬z x z x z ¬x\n9/32 Construction (continued) Φ = (¬𝑥 ∨ 𝑦 ∨ ¬𝑧) ∧ (𝑥 ∨ ¬𝑦 ∨ 𝑧) ∧ (𝑥 ∨ 𝑦 ∨ 𝑧) ∧ (¬𝑥 ∨ ¬𝑦) y ¬y y ¬y ¬x ¬z x z x z ¬x For this, how big could an independent set be?\n10/32 Construction (continued) Φ = (¬𝑥 ∨ 𝑦 ∨ ¬𝑧) ∧ (𝑥 ∨ ¬𝑦 ∨ 𝑧) ∧ (𝑥 ∨ 𝑦 ∨ 𝑧) ∧ (¬𝑥 ∨ ¬𝑦) 𝑥 = False, 𝑦 = True, 𝑧 = True Four clauses? Can we find an independent set of size 4 in 𝐺? y ¬y y ¬y ¬x ¬z x z x z ¬x\n11/32 Proof Φ, a 3CNF expression is satisfiable if and only if 𝐺 has an independent set of size 𝑘. Assume there is an assignment 𝑎 , 𝑎 , …, 𝑎 that satisfies Φ. Prove there is an 1 2 𝑛 independent set of size 𝑘 in 𝐺. By construction, 𝐺 has at most one vertex per clause. For each clause, the assignment must make at least one literal true, so choose one vertex corresponding to a true literal. That is an independent set of size 𝑘. y ¬y y ¬y ¬x ¬z x z x z ¬x\nHow to prove converse?\n13/32 Proof continued Assume 𝐺 has an independent set of size 𝑘: No two members of the set are in the same clause-triangle, so exactly one vertex per clause is chosen. If a vertex representing ¬𝑥 is chosen, then no 𝑥 vertex is chosen. 1 1 y ¬y y ¬y ¬x ¬z x z x z ¬x INDSET is NP-Complete.\n14/32 What's a clique? Given an undirected graph 𝐺, a clique 𝑄 is a subset of vertices where for any 𝑢, 𝑣 ∈ 𝑄, there is an edge between 𝑢 and 𝑣. What is the trivial clique?\n15/32 What's CLIQUE? Given an undirected graph 𝐺 and 𝑘, is there a clique of size 𝑘? Verifying? 1 define verify(𝐺, 𝑘, 𝑄): 2 if |𝑄| < 𝑘: 3 return False 4 for 𝑢 in 𝑄: 5 for 𝑣 in 𝑄 − {𝑢}: 6 if (𝑢, 𝑣) ∉ 𝐺.𝐸: 7 return False 8 return True\nHow do we prove CLIQUE is NP- Complete? (We can use anything we already know is NP-Complete.)\n17/32 INDSET reduces to CLIQUE? Given a independent set of size 𝑘 on 𝐺, can you convert it to a CLIQUE problem? If 𝑄 is a clique in 𝐺, what is that set of vertices in the complement of 𝐺? G G’\n18/32 Proof: INDSET reduces to CLIQUE? Given an independent set 𝐼 of 𝐺 of size 𝑘, 𝐼 is a clique in 𝐺′ . (If there is no edge between 𝑢, 𝑣 in 𝐺, then there is an edge between them in 𝐺′ .) Given a clique 𝑄 in 𝐺, 𝑄 is an independent set in 𝐺′ . (If there is an edge between 𝑢, 𝑣 in 𝐺, then there is no edge between them in 𝐺′ .)\n19/32 VERTEXCOVER Given a graph 𝐺, is there a set of 𝑘 vertices 𝐶 such that every edge touches a member of 𝐶? What is the trivial vertex cover? Verifier?\nWhat do we reduce to VERTEXCOVER to show it is NP-Complete? 3-SAT, INDSET, CLIQUE?\n21/32 INDSET reduces to VERTEXCOVER Given a graph 𝐺, there is an independent set of vertices 𝐼 if and only if 𝑉 − 𝐼 is a vertex cover of 𝐺. Proof: If you have an edge (𝑢, 𝑣) in 𝐺, at least one end is not in 𝐼. Thus, at least one end is in the complement of 𝐼. Thus the complement of 𝐼 is a vertex cover of 𝐺.Aaron Conversely, if 𝐶 is a vertex cover, every edge (𝑢, 𝑣) has at least one end in 𝐶, thus at most one end must be in the complement of 𝐶. Therefore the complement of 𝐶 is an independent set.\n22/32 How did I get here!? SAT in NP 3-SAT INDSET CIRCUIT-SAT CLIQUE VERTEX-COVER\n23/32 SUBSET-SUM Given a set 𝑆 of integers and a target 𝑇 , does there exist a 𝑆′ ⊂ 𝑆 such that the sum of the elements in 𝑆′ is exactly equal to 𝑇 ? Example: Let 𝑆 = {8, 6, 4, 4, 2}, 𝑇 = 11 Witness and Verifier? NP-Complete? Let’s work from 3-SAT.\n24/32 Reducing 3-SAT to SUBSET-SUM Given a 3CNF expression, how can we encode it as a single SUBSET-SUM problem? Given variables 𝑥 , 𝑥 , 𝑥 . How to express the constraint “We must choose 1 2 3 exactly one from each pair: (𝑥 , ¬𝑥 ), (𝑥 , ¬𝑥 ), (𝑥 , ¬𝑥 )”? 1 1 2 2 3 3 𝑥 1 0 0 1 ¬𝑥 1 0 0 1 No column can exceed 2, so 𝑥 0 1 0 2 interpret as base-2: ¬𝑥 0 1 0 2 𝑥 0 0 1 𝑆 = {4, 4, 2, 2, 1, 1}, 𝑇 = 7 3 ¬𝑥 0 0 1 3 Sum 1 1 1\n25/32 3-SAT Clauses? Expression: (𝑥 ∨ 𝑥 ∨ 𝑥 ) ∧ (¬𝑥 ∨ 𝑥 ∨ ¬𝑥 ) ∧ (¬𝑥 ∨ ¬𝑥 ∨ 𝑥 ) 1 2 3 1 2 3 1 2 3 𝑥 1 0 0 1 0 0 1 ¬𝑥 1 0 0 0 1 1 1 𝑥 0 1 0 1 1 0 2 But it has to be exact. ¬𝑥 0 1 0 0 0 1 2 And one sum… 𝑥 0 0 1 1 0 1 3 ¬𝑥 0 0 1 0 1 0 3 Sum 1 1 1 > 0 > 0 > 0\n26/32 Slack variables Expression: (𝑥 ∨ 𝑥 ∨ 𝑥 ) ∧ (¬𝑥 ∨ 𝑥 ∨ ¬𝑥 ) ∧ (¬𝑥 ∨ ¬𝑥 ∨ 𝑥 ) 1 2 3 1 2 3 1 2 3 𝑥 1 0 0 1 0 0 1 ¬𝑥 1 0 0 0 1 1 1 𝑥 0 1 0 1 1 0 2 ¬𝑥 0 1 0 0 0 1 2 Slack variables can only contribute 2 to each sum. 𝑥 0 0 1 1 0 1 3 ¬𝑥 0 0 1 0 1 0 Real variables must contribute one to each sum. 3 𝑠 0 0 0 1 0 0 1 𝑠 0 0 0 1 0 0 Will binary encoding still work? 2 𝑠 0 0 0 0 1 0 3 Use base-5. 𝑠 0 0 0 0 1 0 4 𝑠 0 0 0 0 0 1 5 𝑠 0 0 0 0 0 1 6 Sum 1 1 1 3 3 3\n27/32 Making Sums Expression: (𝑥 ∨ 𝑥 ∨ 𝑥 ) ∧ (¬𝑥 ∨ 𝑥 ∨ ¬𝑥 ) ∧ (¬𝑥 ∨ ¬𝑥 ∨ 𝑥 ) 1 2 3 1 2 3 1 2 3 𝑥 1 0 0 1 0 0 1 ¬𝑥 1 0 0 0 1 1 1 𝑥 0 1 0 1 1 0 111333 in base-5 is 1 × 55 + 1 × 54 + 1 × 53 + 3 × 52 + 3 × 51 + 3 × 2 ¬𝑥 0 1 0 0 0 1 50 = 3, 125 + 625 + 125 + 75 + 15 + 3 = 3, 968 2 𝑥 0 0 1 1 0 1 3 100100 = 3, 125 + 25 = 3, 150 ¬𝑥 0 0 1 0 1 0 3 100011 = 3, 125 + 5 + 1 = 3, 131 𝑠 0 0 0 1 0 0 1 𝑠 0 0 0 1 0 0 010110 = 625 + 25 + 5 = 655 2 𝑠 0 0 0 0 1 0 3 … 𝑠 0 0 0 0 1 0 4 𝑠 0 0 0 0 0 1 𝑆 = {3150, 3131, 655, …}, 𝑇 = 3968 5 𝑠 0 0 0 0 0 1 6 Sum 1 1 1 3 3 3\n28/32 Proof Given a 3CNF expression Φ if there exists an assignment 𝐴 that satisfies Φ, prove that the set 𝑆 and limit 𝑇 as constructed in the previous slide, would have a solution: Pick the items corresponding to the assignment. They will sum to 𝑇 . Conversely, if there is a subset that will sum to 𝑇 , then pick the assignments corresponding to the itmes in the subset. They will satisfy Φ.\nWhat about KNAPSACK?\n30/32 KNAPSACK Given 𝐼 = {(𝑤 , 𝑣 ), …, (𝑤 , 𝑣 )}, a limit 𝑊 and a value 𝑉 , 1 1 𝑛 𝑛 Is there an 𝐼′ ⊂ 𝐼 such that ∑ 𝑤 ≤ 𝑊 and ∑ 𝑣 ≥ 𝑉 ? 𝑖 𝑖 {𝑖∈𝐼′} {𝑖∈𝐼′} Wait…isn’t it 𝑂(𝑛𝑊 )? That sounds polynomial. In NP?\n31/32 Reducing SUBSET-SUM to KNAPSACK Given 𝑆 = {𝑠 , 𝑠 , …, 𝑠 } and 𝑇 . 1 2 𝑛 Let 𝐼 = {(𝑠 , 𝑠 ), (𝑠 , 𝑠 ), …, (𝑠 , 𝑠 )}. Let 𝑊 = 𝑉 = 𝑇 . 1 1 2 2 𝑛 𝑛 Boom. If you can solve KNAPSACK in polynomial time, you can solve SUBSET-SUM in polynomial time.\nQuestions? Slides by Aaron Hillegass Based on lectures by Abrahim Ladha"},
{"file": "dsa_slides/Notes_DFSTopSort.pdf", "content": "CS 3510 Algorithms 1/30/2024 Lecture 5: DFS, Topological Sort, and Strongly Connected Components Lecturer: Abrahim Ladha Scribe(s): Joseph Gulian, Saahir Dhanani Welcome to the first lecture of unit 2 on graph algorithms! Graphs are useful topological and combinatorial devices to study many problems. 1 Graphs Graphs are described by a set of vertices V and by a set of edges which are pairs of vertices. This is represented as G = (V,E). Graphs with ordered edges (where there is a direction) are called directed graphs; graphs with unordered edges (where there is no direction) are calledundirectedgraphs. Thoughnotinthislecture,itissometimesusefultoapplyaweight or value to edges; the weight of an edge is denoted as w where e is the edge. e A B D C Figure 1: A undirected unweighted graph. A 5 4 B D 3 3 C 5 Figure 2: A directed weighted graph. We consider graphs as these pictures, dots and lines embedded onto the page, but this is not how computers look at graphs. We need to discuss the way that graphs may be encoded. We have two main choices in how to represent graphs like these. 5: DFS, Topological Sort, and Strongly Connected Components-1\n2 Adjacency Matrix The first is called an adjacency matrix. This is a square matrix of size |V| × |V|. We associate each element of the matrix as an edge in the graph. The above graph14 could be represented as the following matrix   0 5 0 0 0 0 0 4   0 3 0 5 0 3 0 0 Notice here, that there are a lot of zeroes. This format can be wasteful for graphs without a lot of edges. Graphs without many edges are considered sparse, opposed to dense graphs which have many edges. We say a graph is dense if |E| ≈ |V|2, and that a graph is sparse if it is not dense. The following is an example of a dense graph 1 B C 1 1 2 3 A D 2 Figure 3: A weighted fully-connected graph. This would be represented by the following matrix   0 1 1 2 1 0 1 3   1 1 0 2 2 3 2 0 You may notice, that the matrix above is symmetric. That is that A = AT. This is the case of all undirected graphs. 1 3 Adjacency List We come back to this issue though, how do we represent sparse graphs? We could simply store edges instead of all possible edges by using |V| linked lists. These would represent the outgoing edges from each node. The adjacency list representation for the above directed graph is as follows: 1You may notice something else as well: the diagonal has all zeroes. This is because there are not edges from nodes back to themselves in this graph. Some graphs may have edges like these; say if you were to model a state machine. 5: DFS, Topological Sort, and Strongly Connected Components-2\nA B, 5 B D, 3 C B, 3 D, 5 D B, 3 Figure 4: The adjacency list representation of a graph. This seems like a more efficient option because there’s no wasted space, but let’s think about the space-time trade-off. If you want a particular edge in this graph, you need to iterate through the adjacency list. If the adjacency list is big, this could take a long time; this happens when graphs are dense. If a graph is sparse, this representation is better. A adjacency matrix takes constant time to read if an edge exists and its weight. This representation is especially good for many real world situations. Think of the world wide web. We have billions (109) of websites with very few links (maybe 10 as an example). If you wanted to represent this as an adjacency matrix, you’d need 1018 bytes (assuming each edge has a weight within a byte) or 1 exabyte! Few people have this kind of memory on their computers. If we use the adjacency list representation we need to store roughly 1011 (or 10 edges/node∗109 nodes); this is fairly feasible. Although still large, it’s orders of magnitude smaller, and more reasonable. This is just one example of where adjacency lists are better than adjacency matrices. 2 Most of this class will focus on adjacency lists for this reason. The size of an adjaceny list is O(|V|+|E|) because there are |V| linked list entry pointers, and each edge appears in a list once (if directed) or twice (if directed). 4 Graph Traversal Say we’re at some node on the graph, we want to visit all vertices we can reach from that node. Much like divide and conquer, our trick here will rely on recursion. Instead of implicitly using any advanced data structures, we’ll simply use the call stack. We will define the following algorithm: 2Of course, there are many other options for representing graphs, all of which can be further fine-tuned for particular applications. 5: DFS, Topological Sort, and Strongly Connected Components-3\ndef explore(G, v): marked[v] = true previsit(v) for edge (v, u) in E: if not marked[u]: explore(G, u) postvisit(V) Figure 5: Explore routine on a graph and node. Notice two things about this: marked and pre/post. Marked is a static set available in all calls to the routine. pre/post are two points in routine where modification allows for more applications. We will change pre/post to help a lot with future algorithms. DFS isn’t an algorithm so much as it is a primitive to build other algorithms. Its just a way to compute and iterate over the graph. We have a default implementation of pre/post as: counter = 1 def previsit(v): pre[v] = counter counter++ def postvisit(v): post[v] = counter counter++ Figure 6: Explore routine on a graph and node. It keeps two numbers per vertice, we denote as the pre and post number. It’s also worth noting that this algorithm works on both directed and undirected graphs. The adjacency list representation for directed graphs would only contain outgoing transitions. 5: DFS, Topological Sort, and Strongly Connected Components-4\nB A C I K E F D J G H Figure 7: An example graph for explore. Starting with A, we generate the following recursion tree, with the pre and post numbers for each vertex. A1,16 B2,11 C12,15 E3,10 D13,14 F4,7 H8,9 G5,6 Figure 8: An example exploration of a graph with pre and post labels. Ifweweretomarkthestepsweenteredandexitedexploreforanode,we’dgetthenumbers above. There’s an issue with this graph though. We want to see all nodes, but we’re missing I, J, and K. This is because there are disconnected components in the graph. Similarly if we started at K, we would not reach much of the graph. To achieve this, we simply loop over nodes on top of this. 5: DFS, Topological Sort, and Strongly Connected Components-5\ndef dfs(G): for v in V: marked[v] = false for v in V: if not marked[v]: explore(G, v) Figure 9: dfs routine on a graph Let’s show that this algorithm is correct. If we start with A as in the previous example, we’ll run explore as we did and cover that component. We then return to dfs and continue iterating until we hit I; at this point we explore J. Then we leave again, iterate, and reach K. Then, we know it works for our graph above, but how can we show something like it works for all graphs? Let’s setup a proof by contradiction showing explore does reach all vertices reachable from V. Assume to the contrary that explore does not reach all vertices reachable from V. So, if our dfs makes some mistake, then there is a node u reachable from v which is not touched by dfs. Since it is reachable from v, there exists some path like v → ... → u v → ... → z → z′ → ... → u Here, let z is the last marked vertex in the path and w is the first non-marked vertex in a path from v to u. Explore on z was called, meaning it must call explore on all not marked verticeswithanedgefromz. z′ hasanedgefromz,anditisnotmarked. Thereforeexplore does reach all vertices reachable from v, contradiction. Therefore, dfs is correct. There are multiple types of edges in the DFS tree which correspond to where in the DFS tree they can or cannot be found. • Tree edge (A, B) if it appears in the dfs tree. • Forward edge (E, G) if it exists in the graph but not in the tree because it goes from a parent to a child that was already explored. • Back Edge (D, A) if it exists in the graph but not in the tree because it goes from a child to a parent that was already explored. • Cross Edge (D, H) if it exists in the graph but not in the tree because the other vertex was marked, even though it doesn’t go to a parent or child. 5: DFS, Topological Sort, and Strongly Connected Components-6\nA B C 1,8 2,7 9,12 D E F F 4,5 3,6 10,11 Figure 10: An example of pre and post values generated after running dfs 5 Cycles Pre and post numbers give us a lot of information, like it allows us to find a cycle (a path where the first and last vertex are the same) in a graph. Checking for cycles has very real world applications like ensuring packages in a package manager don’t have dependency cycles. Now do we do this? Suprisingly we already have all the tools we need. We will now show that a graph has a cycle in it if and only if there is a backedge. We’ll do this proof in two parts. If a graph has a cycle, it will have a back edge. Define a cycle v → ... → v → v . Let v be 0 k 0 i the first traversed vertex in this cycle. The v → v will be traversed next and v → v i i+1 i−1 i will be the backedge because it will point to it’s parent. Now let’s go the other way. Suppose there exists a backedge (B,A). Then the dfs tree path A,...,B +(B,A) would produce a cycle. Then we have shown that there is a cycle if there is a backedge. Thisisverypowerful,wecancheckforcyclesinO(n)withamodifiedexplore: explore-cd. Now as well as marked, we will maintain a second set, t-marked which will store whether or not a vertex is a parent. explore-cd is as follows def explore(G, v): marked[v] = true t-marked[v] = true for edge (v, u) in G connected to v: if t-marked[u]: print(’cycle found!!!’) if not marked[u]: explore(G, u) t-marked[v] = false Figure 11: Explore-cd routine for cycle detection. You may be asking about disconnected components for this, say we run explore on one component but there is a cycle in another disconnected component. It turns out we can 5: DFS, Topological Sort, and Strongly Connected Components-7\nbasically just do the same strategy we did above for dfs, but have a global t-marked. 6 Topological Sort What is topological sort3? IF we have a directed acyclic graph, and we’d like to sort the graph by precedence we could use a topological sort. This could be very useful in a situation like evaluating a series of variables in an unspecified order. Essentially what we’re going to do is use dfs and the dfs tree again. Here is that the dfs tree is a subset of the graph which is a directed acyclic graph (DAG4). Our strategy for this will be to add them to a list as we return. We’ll call this modification top-sort as shown here def top-sort(G, v): marked[v] = true for edge (v, u) in G connected to V: if not marked[u]: explore(G, u) L = [v] + L Figure 12: Top-Sort routine for topological sorting. Consider the DAG below A C E B D F Figure 13: A directed acyclic graph. We’re not going to run top sort on the source because then we’d have to scan the graph, so instead we’ll start on A like we normally do and iterate. This is still correct for disconnected components. 3The DPV book uses the term linearization. 4Know this acronym. 5: DFS, Topological Sort, and Strongly Connected Components-8\nWhen we run the algorithm we get the following list when we finish each vertex. A → C → E : [E] A → C → F : [F,E] A → C : [C,F,E] A : [A,C,F,E] B → D : [D,A,C,F,E] B : [B,D,A,C,F,E] This is a topological sort.5 Importantly, we see the source of the graph first and the sinks last. If you wanted to draw this on paper, you could do B D A C F E Figure 14: A linearization of a directed acyclic graph. Note that the topological sort of a DAG can be given by the post numbers from highest to lowest, that is what is returned here essentially. 7 Runtime The last thing we’ll discuss is the runtime of these algorithms. Note that we visit every node once in the algorithm and cross every edge, so the runtime of these algorithms are O(|V|+|E|). 8 Connected Components Most graphs we cover in this class are connected because if we had a graph that was not connected, we could decompose it into it’s connected parts and study those. It’s trivial to find connected components in an undirected graph: run DFS. 6 The nodes found in each explore call will be the connected components; if you wanted to count the number of components, you could count the number of top level explore calls made from DFS. In a directed graph, we say two nodes are strongly connected if there exists a path to and from the nodes with respect to all other nodes. Nodes v,u are connected if there is a path from v to u and also from u to v. In other words, if there is a cycle. This means that your graph won’t always be disconnected, sometimes it may simply be that a node can reach another node, but the second node can not go back. 5Note that there could be many depending on which way you traverse 6There’sanimportantdistinctionexploreanddfsinthecontextofdisconnectedgraphs. Theexplorerou- tinethatwe’veoutlinedwillonlyexplorewithinoneconnectedcomponent;dfswillexploreallcomponents. Confusing these is a common mistake, don’t make it. 5: DFS, Topological Sort, and Strongly Connected Components-9\nA B C D E F G H I J K L Figure 15: A directed graph. A B C D E F G H I J K L Figure 16: The components in a directed graph. 5: DFS, Topological Sort, and Strongly Connected Components-10\nIn this graph, the strongly connected components can be found by looking at it. Note that a vertex can not be contained in two components. This is because if a vertex is part of one cycle and part of another cycle, then there is a path between all nodes on both cycles meaning its one large component. Now, let’s construct the metagraph, a graph with these components as vertices, and edges between components where they appear in the original graph. A BE CF D GHIJKL Figure 17: Components in a directed graph. 9 Metagraphs and Graph properties Here’s our metagraph; it happens to be a DAG. It’s interesting to consider whether all metagraphs are DAGs, so let’s try to prove it. Assume to the contrary that there exists a metagraph with a cycle. This would mean, the two components would themselves be strongly connected as we’ve deduced earlier. This means those nodes should have been in the same metanode, a contradiction to our correctness. We want to create an algorithm to find strongly connected components in directed graphs. This isn’t that simple. Say you call DFS on the graph like we did for connected components with undirected graphs. 7 Instead we use an algorithm which is a little more advanced than dfs called the SCC algorithm. Asanaside, manypeoplehavethisintuitionthatthere’sanentranceandanexittoagraph. This could be formalized by the concept of sources and sinks. A source is a vertex with in- degree 0 (no edges entering), and a sink is a vertex with out-degree 0 (no edges exiting). Think about why this could be useful along with some of the algorithms we’ve previously devised by running explore from the sink and source (although we already ran it from the sink). Doing this you might notice that running explore from the sink of the metagraph reveals one strongly connected component of the graph. Of course now, we have another problem, how do we get only the sinks of a graph. Recall what we did for top-sort: noting when a vertex was entered and exited. Performing this on the above graph, we get the following tree. 7Think about this. Calling on the first graph from the letter A would produce A, B, C, D, E, F, G, H, I, J, K, and L. Clearly this is not what we want as we can discover by looking at the answer. 5: DFS, Topological Sort, and Strongly Connected Components-11\nA1,24 B2,23 D3,4 E5,22 F6,21 C7,8 H9,20 K10,19 L11,18 J12,17 G13,14 I15,16 Figure 18: The dfs traversal of the graph. If you thought about how you would top-sort this (ignoring cycles), it would put the source in the back. We’re not guaranteed the sink would be the last element. Consider ordering by post label the following graph with a dfs which explores in alphabetical order. You’d get the ordering A, B, and C which is not correct because C is actually in the source of the graph. B A C Figure 19: A graph whose loose topological sort does not yield a sink as the last vertex. 10 Proof and Algorithm We will need to prove one thing before we can actually get a correct algorithm: If C and C′ are strongly connected components, and there is an edge from a node in C to a node in C′, then the highest post number in C is bigger than the highest post number in C′. We’ll prove this by cases. Say DFS finds C before C′, it will explore C → C′, and C′ will have a smaller post label. Say DFS finds C′ before C, C′ will be added and then C will be added later through a separate explore call, so it will have a higher post label. Considering that the first element in a “top-sort” will be a source, but we’re looking for a sink. Consider what reversing all the edges will do; it will make the sources sinks and the 5: DFS, Topological Sort, and Strongly Connected Components-12\nsinks sources. Now, we can sort by post labels, and our last element will be a source of the reverse graph GR, but a sink in our real graph G. An interesting proof arises with G and GR wherein you might think that G and GR have the same metagraph, simply with the reversed edges. This is correct and consider why. Consider the definition of a strongly connected component: there are cycles between all the vertices in a strongly connected component. Now consider if you reverse all the edges. All the edges in the cycle will be reversed but it will still form a cycle (simply in the reverse direction). All other edges will be reversed. Considering all of this we come to the following algorithm for computing strongly connected comonents. def scc(G): dfs(G) to get post labels in decreasing order compute reversal GR from G while posts not empty: v = max(posts) component = explore(GR,v) print or mark the component remove component from GR and posts Figure 20: Strongly connected components algorithm This algorithm works how you might think given the ideas we’ve devised. We find a source of G, so its guaranteed to be in the sink of GR. We then explore this entire sink component in GR. We remove this explored component from GR, and its nodes from our topsort, and repeat until we have all components. A common confusion is that even though top sort doesn’t work on cyclic graphs, as we’ve devised top-sort, it will sort by post labels which is all we care about. Note carefully that we call our “top-sort\" on G, but we explore on GR. It’s also worth noting that this is essentially doing the same amount of work as two DFS calls, meaning strongly connected components run in linear time. 5: DFS, Topological Sort, and Strongly Connected Components-13"},
{"file": "dsa_slides/Notes_Quicksort.pdf", "content": "CS 3510 Algorithms 8/29/2024 Lecture 4: Quicksort Lecturer: Abrahim Ladha Scribe(s): Richard Zhang Previously, we have learned about the mergesort algorithm, a divide and conquer algorithm for sorting an array of values. In this lecture, we will show another divide and conquer sorting algorithm called quicksort. 1 What is Quicksort? Quicksort works by repeatedly partitioning the current array into one left subarray and one right subarray where each element on the left subarray is less than or equal to some chosen pivot element and each element on the right subarray is greater than the pivot. Quicksort is then recursed on the half sized left and right subarrays, sorting them in place. Below is the pseudocode for Quicksort. function quicksort(A, l, u) if l < u pivotIndex = partition(A, l, u) quicksort(A, l, pivotIndex - 1) quicksort(A, pivotIndex + 1, u) quicksort(A,1,n) The partition function will perform the partition and return the index (pivotIndex) of the pivot element used. It has three jobs. It places the pivot element exactly in its correct index, it ensures everything below this is less than the pivot, and everything above this is greater than the pivot. The base case is implicit here: if the subarray A[l...u] is of size zero or one (l ≥ u), then the subarray is already sorted and we do not do anything. 1.1 Partitioning The partition function works by selecting a pivot element in the array, and then moving the elements until the elements to the left of the pivot are less than or equal to the pivot and the elements to the right are greater than the pivot. Below is the pseudocode for the partition function that rearranges the subarray A[l...u] and returns the index of the pivot. function partition(A, l, u) pivot = A[u] i = l - 1 for j in l...u-1 if A[j] <= pivot i++ swap A[i] and A[j] 4: Quicksort-1\nswap A[i+1] and A[u] return i + 1 The element A[u] is chosen as the pivot and is used to partition the array. In the beginning ofeachiterationoftheforloop(andattheendwherewecanthinkthatj = u),thefollowing is guaranteed: • The indices from l to i have their elements less than or equal to the pivot. • The indices from i+1 to j −1 have their elements greater than the pivot. After the loop terminates, all the nonpivot elements will be partitioned such that the ele- mentsfromindicesl toiwillbelessthanorequaltothepivotandtheelementsfromindices i+1 to u−1 will be greater than the equal. We swap the pivot element with the leftmost element greater than pivot (which is at index i+1) to put the pivot at the right place. Since the pivot is at i+1, we return this index. Below is an example with the array [2,8,7,1,3,5,6,4]. Red indicates elements less than or equal to the pivot while blue indicates elements greater than the pivot. Notes/partition example.png The loop has O(n) iterations and does O(1) work at each iteration. Therefore, the runtime of the partition function is O(n). 2 Quicksort Runtime Analysis The runtime analysis for quicksort gets a little tricky since it is not always the case that the input is split equally among the subproblems. Let’s consider the best case and worst case scenarios regarding how the input is split. 2.1 Best Case Analysis The best case occurs when the partitioning leads to both subproblems of equal size (or that the sizes of the subproblems only differ by one). This happens when the median of the array (or number closest to the median) is selected as the pivot. If this always occurs, we can treat it as two subproblems with each of their inputs being halved and have the partitioning be the work done at each subproblem. This results in the following recurrence: (cid:16)n(cid:17) T(n) = 2T +O(n) 2 We can apply the Master theorem and get that T(n) = O(nlogn). This is unfortunately only under the assumption we always pick good pivots. 4: Quicksort-2\n2.2 Worst Case Analysis The worst case occurs when the partitioning leads to one subproblem with n−1 elements and one with 0 elements. This happens when the chosen pivot is the maximum or minimum element of the array. Using the partition function defined in the previous section, this scenario will always occur when the array is already sorted. If this happens every time we partition, we get the following recurrence: T(n) = T(n−1)+T(0)+O(n) = T(n−1)+O(n) (Since T(0) would be constant) We can show that T(n) = O(n2) via two ways: • Induction: We can show through induction that for all positive values of n, there existsaconstantcsuchthatT(n) ≤ cn2. Forthebasecasewheren = 1, thisistrivial. For the induction case, suppose that T(n−1) ≤ c(n−1)2. We can substitute this in the recurrence and get the following: T(n) = T(n−1)+O(n) ≤ c(n−1)2+dn = cn2−2cn+c+dn ≤ cn2+n2 = (c+1)n2 Since T(n) ≤ (c+1)n2, we con conclude that T(n) = O(n2) based on the definition of Big O. • Repeated Substitution: We can repeatedly substitute the definition of the recur- rence for smaller values (ex: T(n−1) = T(n−2)+O(n−1)) until we reach the base case. T(n) = T(n−1)+O(n) T(n) = T(n−2)+O(n)+O(n−1) T(n) = T(n−3)+O(n)+O(n−1)+O(n−2) T(n) = O(n)+O(n−1)+O(n−2)+···+O(1) (cid:18) (cid:19) n(n+1) = O 2 = O(n2) 2.3 Average Case Intuition The average runtime for quicksort is be O(nlogn). A more rigorous analysis for this will be shown later, but this section will provide some intuition. 4: Quicksort-3\nSuppose that the partitioning of elements always produces a 999-1 split between the two subproblems, resulting in the following recurrence: (cid:18) (cid:19) (cid:18) (cid:19) 999 1 T(n) = T n +T n +cn 1000 1000 This seems really lopsided and not very optimal. But notice the pivot is not chosen a con- stant away from the maximum (or minimum). Its still chosen very close, but an amount which varies as a function of n. However, this T(n) would still be O(nlogn)! Notes/lopsided-recursion-tree.png There would be log n = O(logn) levels in the recursion tree. At the first log n 1000/999 1000 levels of the recursion tree, cn work would be done. At the later levels, less than cn work would be done but it would still be upper bounded by cn. Therefore, the total amount of work done would be upper bounded by cnlog n, which is O(nlogn). 1000/999 Additionally, the probability that the maximum element is always chosen decreases very quickly as n grows. Let we assume that the probability of each possible permutation of the elements is uniform. Without loss of generality assume the elements are distinct. There are n! possible permutations, each equally likely. Then, we can also assume that each element in the array is equally likely to be at index u during the partition operation and thus be the pivot. The probability that the pivot value is the maximum value for one partition operation would be 1. The probability that every partition operation would choose the n maximum element during Quicksort would therefore be: (cid:18) (cid:19)(cid:18) (cid:19)(cid:18) (cid:18) (cid:19) 1 1 1 1 1 ... = n n−1 n−2 1 n! which would become very small as n grows large. Intherealworld,notallpermutationsoftheelementsareequallyprobable. However,wecan stillhaveeachelementhaveanequalprobabilityofbeingthepivotbyrandomselection. The partition function would change in that an element from index l to u would be randomly selected as the pivot and be swapped with A[u] before the partitioning begins. This allows quick sort to be robust and still be O(nlogn) on average. 2.4 Average Case Analysis With each element being chosen as a pivot with probability 1, we can do a more rigorous n analysis of the average runtime. If a chosen pivot element would be at index i in the sorted array (using zero-indexing), then the left sub problem would be of size i and the right sub problemwouldbeofsizen−i+1, whichresultsinarecurrenceofT(n) = T(i)+T(n−i+1). 4: Quicksort-4\nWecomputeourrecurrenceasanaverageoverallpossiblechoicesofi. Belowisthederivation showing that the average runtime T(n) would be O(nlogn). (cid:34)n−1 (cid:35) 1 (cid:88) T(n) = [T(i)+T(n−i+1)] +n n i=0 (cid:34)n−1 n−1 (cid:35) 1 (cid:88) (cid:88) T(n) = T(i)+ T(n−i+1) +n n i=0 i=0 Notice the summations are simply a permutation of one another. So the summations are equal. n−1 n−1 (cid:88) (cid:88) T(i) = T(n−i+1) i=0 i=0 We substitute one summation for the other, multiply by n, then substitute n for n−1 (cid:34)n−1 n−1 (cid:35) (cid:34)n−1 (cid:35) 1 (cid:88) (cid:88) (cid:88) T(n) = T(i)+ T(n−i+1) +n = 2 T(i) +n n i=0 i=0 i=0 We multiply both sides by n and substitute n for n−1. (cid:34)n−1 (cid:35) (cid:88) nT(n) = 2 T(i) +n2 i=0 (cid:34)n−2 (cid:35) (cid:88) (n−1)T(n−1) = 2 T(i) +(n−1)2 i=0 Take the difference of the previous two, and the sum will telescope out. We then simplify. (cid:34) (cid:34)n−1 (cid:35) (cid:35) (cid:34) (cid:34)n−2 (cid:35) (cid:35) (cid:88) (cid:88) nT(n)−(n−1)T(n−1) = 2 T(i) +n2 − 2 T(i) +(n−1)2 i=0 i=0 nT(n)−(n−1)T(n−1) = 2T(n−1)+2n−1 nT(n) = (n+1)T(n−1)+2n Divide out by n(n+1) T(n) T(n−1) 2 = + n+1 n n+1 4: Quicksort-5\nSubstitute n for n−1 and compute T(n−1) as a function of T(n−2). Then substitute this into T(n). Repeated substitution will make this no longer a recurrence. T(n) T(n−2) 2 2 = + + n+1 n−1 n n+1 T(n) T(n−3) 2 2 2 = + + + n+1 n−2 n−1 n n+1 ... n+1 T(n) T(1) (cid:88) 1 = +2 n+1 2 i i=3 A simple proof by induction will show 2n (cid:88) 1 ≤ 1+n i i=1 . WeapproximateoursumbyO(logn). YoucanalsousedtheaproximationoftheHarmonic numbers.1 T(n) < C +2log (n+1) n+1 2 T(n) < C(n+1)+2(n+1)log (n+1)) 2 T(n) = O(nlogn) 1https://en.wikipedia.org/wiki/Harmonic_number#Calculation 4: Quicksort-6"},
{"file": "dsa_slides/01_IntroBigOMergeSortMaster.pdf", "content": "CS 3510 Algorithms: This Class + Big-O Notation Aaron Hillegass Georgia Tech\n2/47 I Was In Over My Head\n3/47 What Came Next • Worked on Wall Street • Worked with Steve Jobs • Part of the NeXT/Apple merger • Founded Big Nerd Ranch • Published four books • Sold Big Nerd Ranch • Went back to school at Georgia Tech\n4/47 This Class Study theoretical properties of algorithms: • Correctness • Computational complexity • Memory requirements Psuedocode. Mathematical Proofs. No actual coding. 4 exams, 18% each = 72% Homeworks = 20% In-class worksheets: 8%\n5/47 Office Hours Me: Room 3 in Institut Lafayette. My hours will be Tuesday 1:30-3pm and Wednesday 9-11am. TA: Ameen Agbaria, Thursday from 3:00-4:30. Location TBD\n6/47 Why? • The best need to know if their solutions will scale. • This class will get you through a technical interview. • The algorithms are…fun!\n7/47 Parts • Divide and Conquer • Graph Algorithms • Dynamic Programming • NP-Completeness\n8/47 Correctness Always the first: Does the algorithm give correct answers? • Sound: If it returns an answer, the answer is correct. • Complete: • If a correct answer exists, the algorithm finds one in finite time. • If a correct answer doesn’t exist the algorithm reports this in finite time. Then we talk about speed/space.\nWhat we want to know: How long will this algorithm take?\n10/47 Linear scan of an array bool contains(uint16_t *buf, usize n, uint16_t x) { for (usize i = 0; i < n; i++) { if buf[i] == x { return true; } } return false; } • From RAM to cache: 12 clock cycles (brings 64 bytes) • Cache to register: 1 clock cycle • Comparing two registers: 1 clock cycle\n11/47 Linear scan of array • Best case? • Average case? • Worst case?\n12/47 Problems with counting clock cycles • Hardware-dependent • Hassle • The differences we are looking for are BIG\nWhat we are satified knowing: How much longer will this algorithm take if we double the size of the data?\n14/47 Find in an unsorted array If 𝑛 doubles, the time doubles. 𝑂(𝑛) means “In the worst case, time scales linearly with 𝑛” bool contains(uint16_t *buf, usize n, uint16_t x) { for (usize i = 0; i < n; i++) { if buf[i] == x { return true; } } return false; }\n15/47 Intuition 𝑂(1): Double 𝑛? Time stays the same. 𝑂(log 𝑛): Double 𝑛? Time increases by a constant amount. 𝑂(𝑛): Double 𝑛? Time doubles. 𝑂(𝑛2): Double 𝑛? Time × 4 𝑂(𝑛3): Double 𝑛? Time × 8 𝑂(2𝑛): Increase 𝑛 by 1? Time doubles. 𝑂(𝑛𝑛): Increase 𝑛 by 1? Weep.\n16/47 Definition of Big-O 𝑓(𝑛) ∈ 𝑂(𝑔(𝑛)) means: There exists a 𝐶 > 0 and an 𝑛 such that 0 𝑓(𝑛) ≤ 𝐶𝑔(𝑛) for all 𝑛 ≥ 𝑛 0\n17/47 Properties Multiplying by a constant doesn’t matter. In sum, only fastest growing (“dominating”) term matters. 1 Note: the base of the log doesn’t matter: log 𝑛 = ( )(log 𝑛) 𝑎 log 𝑎 𝑏 𝑏 Exponentials always dominate polynomials Polynomials always dominate logs 𝑂(1) < 𝑂(log 𝑛) < 𝑂(𝑛) < 𝑂(𝑛2) < 𝑂(2𝑛) < 𝑂(3𝑛) < 𝑂(𝑛!) < 𝑂(𝑛𝑛)\n𝑂 Θ Ω 18/47 Big vs Big vs Big 𝑓(𝑛) ∈ Ω(𝑔(𝑛)) 𝑓(𝑛) ∈ 𝑂(𝑔(𝑛)) 𝑓(𝑛) ∈ Θ(𝑔(𝑛)) There ∃𝐶, 𝑛 such that There ∃𝐶, 𝑛 such that There ∃𝐶 , 𝐶 , 𝑛 such 0 0 1 2 0 that 𝑓(𝑛) ≥ 𝐶𝑔(𝑛) 𝑓(𝑛) ≤ 𝐶𝑔(𝑛) 𝐶 𝑔(𝑛) ≤ 𝑓(𝑛) ≤ 1 for all 𝑛 > 𝑛 for all 𝑛 > 𝑛 𝐶 𝑔(𝑛) 0 0 2 Or: 𝑔(𝑛) ∈ 𝑂(𝑓(𝑛)) for all 𝑛 > 𝑛 0 Or: 𝑔(𝑛) ∈ 𝑂(𝑓(𝑛)) and 𝑓(𝑛) ∈ 𝑂(𝑔(𝑛))\n19/47 Fibonacci Numbers 0,1,1,2,3,5,8,13,… Inductive definition: • 𝐹 = 𝐹 + 𝐹 𝑛 𝑛−1 𝑛−2 • 𝐹 = 0, 𝐹 = 1 0 1\n20/47 Fibonacci Numbers Recursively 1 define fib(n): 2 if 𝑛 == 0 or 𝑛 == 1: 3 return 𝑛 4 return fib(n-1) + fib(n-2) What can we say about 𝑇 (𝑛), the number of steps to compute 𝐹 ? 𝑛\n21/47 Fibonacci Numbers Recursively 𝑇 (𝑛) = 𝑇 (𝑛 − 1) + 𝑇 (𝑛 − 2) + 𝐶 1 𝑇 (0) = 𝑇 (1) = 𝐶 0 For big enough 𝑛: 𝑇(𝑛 − 1) > 𝑇(𝑛 − 2) + 𝐶 > 𝑇(𝑛 − 2) − 𝐶 1 1 2𝑇(𝑛 − 1) > 𝑇(𝑛) > 2𝑇(𝑛 − 2) 2𝑛 > 𝑇(𝑛) > 2 𝑛 2 √ 𝑛 2𝑛 > 𝑇 (𝑛) > ( 2)\n22/47 Fibonacci Numbers Recursively 𝑇(𝑛) = 𝑇(𝑛 − 1) + 𝑇(𝑛 − 2) + 𝐶 1 𝑇(0) = 𝑇(1) = 𝐶 0 For big enough 𝑛: √ 𝑛 2𝑛 > 𝑇(𝑛) > ( 2) fib(𝑛) ∈ 𝑂(2𝑛) √ 𝑛 fib(𝑛) ∈ Ω(( 2) )\n23/47 Fibonacci Numbers Recursively 𝑇(𝑛) = 𝑇(𝑛 − 1) + 𝑇(𝑛 − 2) + 𝐶 1 𝑇(𝑛) = 𝑎𝑛 𝑎𝑛 = 𝑎𝑛−1 + 𝑎𝑛−2 + 𝐶 1 𝐶 𝑎2 = 𝑎 + 1 + 1 𝑎𝑛−2 √ 1+ 5 𝑎 = = 𝜑 ≈ 1.618 2 fib(n) ∈ Θ(𝜑𝑛)\n24/47 Dumb Recursion\n25/47 Fibonacci Numbers: Dynamic Programming 1 define fib2(n): 𝑎 ≔ array[0.. = 𝑛] 2 3 𝑎[0] ≔ 0, 𝑎[1] ≔ 1 4 for 𝑖 from 2 to 𝑛: 𝑎[𝑖] ≔ 𝑎[𝑖 − 1] + 𝑎[𝑖 − 2] 5 fib2(𝑛) ∈ 𝑂(𝑛) 6 return 𝑎[𝑛]\n26/47 Fibonacci Numbers: Using Matrices If you know 𝐹 and 𝐹 you can compute 𝐹 using a matrix: 𝑘 𝑘+1 𝑘+2 𝐹 1 1 𝐹 ( 𝑘+2) = ( )( 𝑘+1) 𝐹 1 0 𝐹 𝑘+1 𝑘 Starting with 𝐹 = 0 and 𝐹 = 1, we can compute 𝐹 and 𝐹 for any 𝑛: 0 1 𝑛+1 𝑛 𝑛 𝐹 1 1 1 ( 𝑛+1) = ( ) ( ) 𝐹 1 0 0 𝑛 How does matrix exponentation scale?\n27/47 Exponentiation by Squaring You want to know 𝑎20 ? What is 20 in binary? 10100 So 𝑎20 = 𝑎16𝑎4 Exponentiation by 𝑛 is 𝑂(log 𝑛).\n28/47 Exponentiation Diagonalizable Matrices Matrix 𝐵 is diagonalizable if it can be written as 𝐵 = 𝑃 𝐷𝑃 {−1} where 𝐷 is a diagonal matrix. Assume 𝐵 is diagonalizable. 𝐵3 = (𝑃 𝐷𝑃 −1)(𝑃 𝐷𝑃 −1)(𝑃 𝐷𝑃 −1) = 𝑃 𝐷3𝑃 −1\n29/47 Fibonacci: Binet's Formula 1 1 ( ) is diagonalizable: 1 0 𝜑 −𝜑−1 𝑃 = ( ) 1 1 𝜑 0 𝐷 = ( ) 0 −𝜑−1 𝑃 −1 = ( 1 )( 1 𝜑−1 ) √ 5 −1 𝜑 √ 1+ 5 (Reminder: 𝜑 = ≈ 1.6180339887) 2 𝑛 𝐹 1 1 1 1 ( 𝑛+1) = ( ) ( ) = 𝑃 𝐷𝑛𝑃 −1( ) 𝐹 1 0 0 0 𝑛 √ 𝑛 √ 𝑛 1 1+ 5 1− 5 𝐹 = ( )(( ) − ( ) ) √ 𝑛 5 2 2\n30/47 Comparison • Recursive: 𝑂(𝜑𝑛) • Dynamic Programming: 𝑂(𝑛) • Matrix Exponentiation: 𝑂(log 𝑛) • Binet’s Formula: 𝑂(1)\n31/47 Questions we won’t ask in this class bool contains(uint16_t *buf, usize n, uint16_t x) { for (usize i = 0; i < n; i++) { if buf[i] == x { return true; } } return false; } • Can it be parallelized? Vectorized? • Locality of reference? Cache hits? • Could we speed it up via prefetching? • How hard would it be to make the data persistent?\nWhy are computer science professors so obsessed with sorting!?\n33/47 Comparison Sorting Algorithms • Serial? 𝑂(𝑛 log 𝑛) • Stable: equal values stay in the same order • 𝑂(1) memory usage for in place, 𝑂(𝑛) otherwise • Most good algorithms are 𝑂(𝑛) for best case\n34/47 1 define Merge(𝑔, ℎ) -> sorted list: 𝑐 ≔ new List 2 3 while 𝑔 and ℎ are non-empty: 4 if head(𝑔) ≤ head(ℎ): 𝑐.append(𝑔.pop_head()) 5 6 else 𝑐.append(ℎ.pop_head()) 7 8 if 𝑔 is not empty: 9 Append everying in 𝑔 to 𝑐 10 if ℎ is not empty: 11 Append everything in ℎ to 𝑐 12 return 𝑐 𝑂(𝑛) where 𝑛 is len(𝑔) + len(ℎ)\n35/47 MergeSort 1 define MergeSort(𝑔) → sorted list: 𝑛 ≔ length of 𝑔 2 3 if 𝑛 == 0 or 𝑛 == 1, return g 𝑛 𝑠 = ⌊ ⌋ 4 2 left ≔ MergeSort(prefix of 𝑔 of length 𝑠) 5 right ≔ MergeSort(rest of 𝑔) 6 7 return Merge(left, right) Time complexity?\n36/47 MergeSort Time Complexity • How many layers of merges? log (𝑛) 2 • Complexity of each layer? 𝑂(𝑛) Total time complexity: 𝑂(𝑛 log 𝑛)\nCan we come up with a general rule for the time complexity of divide-and- conquer algorithms?\n38/47 MergeSort Revisited 1 define MergeSort(𝑔) -> sorted list: 𝑛 ≔ length of 𝑔 2 • MergeSort divides into 3 if 𝑛 == 0 or 𝑛 == 1: two MergeSorts on half 𝑛 4 return g • The cost of this is the 𝑂(𝑛) 𝑠 = ⌊𝑛⌋ 5 2 merge. left ≔ MergeSort(𝑔[1..𝑠]) 6 right ≔ MergeSort(𝑔[𝑠 + 1..𝑛]) 7 𝑇 (𝑛) = 2𝑇 ( 𝑛 ) + 𝑂(𝑛1) 2 8 return Merge(left,right) Generally: 𝑇 (𝑛) = 𝑎𝑇 ( 𝑛 ) + 𝑂(𝑛𝑑) 𝑏\n39/47 Master Theorem Stated Given: 𝑇 (𝑛) = 𝑎𝑇 ( 𝑛 ) + 𝑂(𝑛𝑑) 𝑏 Then: 𝑂(𝑛𝑑) if 𝑑>log 𝑎 𝑏 𝑇 (𝑛) = {𝑂(𝑛𝑑 log𝑛) if 𝑑=log 𝑎 𝑏 𝑂(𝑛log 𝑏 𝑎) if 𝑑<log 𝑎 𝑏\n40/47 Applied to MergeSort 𝑇 (𝑛) = 𝑎𝑇 ( 𝑛 ) + 𝑂(𝑛𝑑) 𝑏 For MergeSort: 𝑎 = 2, 𝑏 = 2, 𝑑 = 1 Case: 𝑑 = log 𝑎 𝑏 MergeSort has 𝑂(𝑛1 log 𝑛) or just 𝑂(𝑛 log 𝑛)\n41/47 Proof: Consider 𝑇 (𝑛) = 𝑎𝑇 ( 𝑛 ) + 𝑂(𝑛𝑑) 𝑏 𝑇 (𝑛) = ∑ log 𝑏 𝑛 𝑎𝑖( 𝑛 ) 𝑑 + 𝑂(𝑛log 𝑎) 𝑏 𝑖=0 𝑏𝑖\n𝑎 log 𝑛 = 𝑛 log 𝑎 42/47 Why 𝑏 𝑏 Remember change of base formula: log 𝑦 log 𝑦 = 𝑐 𝑥 log 𝑥 𝑐 Thus log 𝑎 1 log 𝑎 = 𝑎 = 𝑏 log 𝑏 log 𝑏 𝑎 𝑎 Thus log𝑎𝑛 log 𝑎 𝑎log 𝑛 = 𝑎 = 𝑎(log 𝑛)(log 𝑎) = (𝑎log 𝑛) 𝑏 = 𝑛log 𝑎 𝑏 log𝑎𝑏 𝑎 𝑏 𝑎 𝑏\n43/47 Simplify 𝑇 (𝑛) = ∑ log 𝑏 𝑛 𝑎𝑖( 𝑛 ) 𝑑 + 𝑂(𝑛log 𝑎) 𝑏 𝑖=0 𝑏𝑖 𝑇 (𝑛) = 𝑛𝑑 ∑ log 𝑏 𝑛 ( 𝑎 ) 𝑖 + 𝑂(𝑛log 𝑎) 𝑏 𝑖=0 𝑏𝑑 • 𝑑 > log 𝑎, 𝑎 < 1, first term of geometric series dominates: 𝑂(𝑛𝑑) 𝑏 𝑏𝑑 • 𝑑 < log 𝑎 : leaf term dominates: 𝑂(𝑛log 𝑏 𝑎) 𝑏 • 𝑑 = log 𝑎 : 𝑎 = 1, every term has the same value: 𝑂(𝑛𝑑 log 𝑛) 𝑏 𝑏𝑑\nMergeSort: What if instead of dividing 𝑘 the list in half, we divide it into parts?\n45/47 Apply It Apply Master Theorem to Binary Sort 𝑂(𝑛𝑑) if 𝑑>log 𝑎 𝑏 𝑇 (𝑛) = {𝑂(𝑛𝑑 log𝑛) if 𝑑=log 𝑎 𝑏 𝑂(𝑛log 𝑏 𝑎) if 𝑑<log 𝑎 𝑏\n46/47 Next Read chapter 0 and chapter 1 Next lecture: FFT and Arithmetic\nQuestions? Slides by Aaron Hillegass"},
{"file": "dsa_slides/03_ClosestPair (1).pdf", "content": "CS 3510 Algorithms: Closest Pair Aaron Hillegass Georgia Tech\n2/17 Finding the closest pair You are given a set of 𝑛 points in the plane: {(𝑥 , 𝑦 ), …, (𝑥 , 𝑦 )} 1 1 𝑛 𝑛 Find the two that are closest to each other. Assume finding distance between two points is 𝑂(1). Obvious solution is 𝑂(𝑛2). Can we do better? Goal: 𝑂(𝑛 log 𝑛) Credit: Michael Shamos, 1975\n3/17 Divide?\n4/17 What about pairs that straddle the divide?\n5/17 So far: 1 define closest_with_sorted(𝑃): 2 if 𝑛 == 2, return (𝑑(𝑃[1],𝑃[2]),(𝑃[1],𝑃[2])) 3 if 𝑛 == 3, return shortest_of_three(𝑃) mid ≔ ⌊𝑛⌋ 4 2 (𝛿 ,pairLeft) ≔ closest_with_sorted(𝑃[1..mid]) 5 𝐿 (𝛿 ,pairRight) ≔ closest_with_sorted(𝑃[(mid + 1)..𝑛]) 6 𝑅 7 if 𝛿 < 𝛿 then 𝛿 ≔ 𝛿 , best ≔ pairLeft 𝐿 𝑅 𝐿 8 else 𝛿 ≔ 𝛿 , best ≔ pairRight 𝑅 split ≔ (𝑃[mid].𝑥 + 𝑃[mid + 1].𝑥)/2 9 (𝛿 ,pairSplit) ≔ closest_split_pair(𝑃,split,𝛿) 10 𝑆 11 if 𝛿 < 𝛿 then return (𝛿 ,pairSplit) 𝑆 𝑆 12 else return (𝛿, best)\n6/17 Runtime • Sort the points by 𝑥: 𝑂(𝑛 log 𝑛) once 𝑛 • 𝑇 (𝑛) = 2𝑇 ( ) + 𝑂(? ) 2 • If we want 𝑂(𝑛 log 𝑛) we need seaching the gutter to be 𝑂(𝑛)\n7/17 Can we limit our search in the gutter? Facts: • Only interested in pairs where one is on each side • Only interested in pairs that are closer than 𝛿 • No two on the same side can be closer than 𝛿 Each point on the right only needs to check a max of three points on the left.\n8/17 If only... If we had a two lists, left and right) sorted by 𝑦, this would be easy an easy 𝑂(𝑛) merge-like process. But sorting could be 𝑂(𝑛 log 𝑛)! We need to keep it 𝑂(𝑛). But…if all the points were sorted by 𝑦, we could sort them into left and right in linear time.\n9/17 Now: 1 define closest_with_sorted2(𝑃 ,𝑃 ): 𝑥 𝑦 2 if 𝑛 is 2 or 3, return dumb_version(𝑃 ) 𝑥 mid ≔ ⌊𝑛⌋ 3 2 (𝛿 ,pairLeft) ≔ closest_with_sorted(𝑃 ,𝑃 ) 4 𝐿 𝑥[1..mid] 𝑦 (𝛿 ,pairRight) ≔ closest_with_sorted(𝑃 ,𝑃 ) 5 𝑅 𝑥[(mid+1)..𝑛] 𝑦 6 if 𝛿 < 𝛿 then 𝛿 ≔ 𝛿 , best ≔ pairLeft 𝐿 𝑅 𝐿 7 else 𝛿 ≔ 𝛿 , best ≔ pairRight 𝑅 split ≔ (𝑃 .𝑥 + 𝑃 .𝑥)/2 8 𝑥[mid] 𝑥[mid+1] (𝛿 ,pairSplit) ≔ closest_split_pair(𝑃 ,split,𝛿,𝑃 ) 9 𝑆 𝑥 𝑦 10 if 𝛿 < 𝛿 then return (𝛿 ,pairSplit) 𝑆 𝑆 11 else return (𝛿, best)\n10/17 Searching the gutter 1 define closest_split_pair(𝑃 ,split,𝛿,𝑃 ) 𝑥 𝑦 2 𝐿 ≔ new List, 𝑅 ≔ new List 3 for pair in 𝑃 : 𝑦 4 Put pair in the right list or ignore if outside gutter 5 if either list is empty return None 6 for pair in 𝐿: 𝐿 7 Do a merge-like comparison with two below and two above in 𝑅 8 Note closest pair and best distance 9 return best distance and closest pair\n11/17 Runtime 𝑛 • 𝑇 (𝑛) = 2𝑇 ( ) + 𝑂(𝑛) 2 • Runtime: 𝑂(𝑛)\n12/17 The Maximum Subarray Sum Problem Given an array 𝐴[1…𝑛] of numbers, find a subarray with the max sum. Or find 1 ≤ 𝑖 ≤ 𝑗 ≤ 𝑛 that maximizes the sum: 𝑗 ∑ 𝐴[𝑘] 𝑘=𝑖 Brute force? 𝑂(𝑛6) Grenander? 𝑂(𝑛2) Shamos Divide-and-Conquer? 𝑂(𝑛 log 𝑛)\n13/17 Divide and Conquer for Max Subarray Joining two? • max_left ≔ max(𝐴.max_left, 𝐴.total + 𝐵.max_left) • max_right ≔ max(𝐵.max_right, 𝐵.total + 𝐴.max_right) • total ≔ 𝐴.total + 𝐵.total • max ≔ max(max_left, max_right, 𝐴.max, 𝐵.max, 𝐴.max_right + 𝐵.max_left) Array of one? • All of the above are equal: value in the array.\n14/17 Divide and Conquer for Max Subarray 1 define analyze(𝐴): 𝑛 ≔ len(𝐴) 2 3 if 𝑛 == 1: 4 return (ml : 𝐴[1], mr : 𝐴[1], total : 𝐴[1], mx : 𝐴[1]) 𝑐 ≔ ⌊𝑛⌋ 5 2 1 define max_subsum(𝐴): 6 𝐿 ≔ 𝐴[1..𝑐], 𝑅 ≔ 𝐴[(𝑐 + 1)..𝑛] (_, _, _, mx) ≔ analyze(A) 2 (lml, lmr, ltotal, lmx ≔ analyze(L) 7 3 return mx (rml, rmr, rtotal, rmx ≔ analyze(R) 8 9 return (ml : max(lml, ltotal + rml), mr : max(rmr, rtotal + lmr), 10 total : ltotal + rtotal, 11 mx : max(lml, rml, lmx, rmx, lrm + rlm)) 12\n15/17 Worked\n16/17 Divide and Conquer for Max Subarray 𝑛 𝑇 (𝑛) = 2𝑇 ( ) + 𝑂(1) 2 𝑂(𝑛)\nQuestions? Slides by Aaron Hillegass"},
{"file": "dsa_slides/Notes_BellmanFordFloydWarshall.pdf", "content": "CS 3510 Algorithms 2/06/2024 Lecture 8: Bellman-Ford and Floyd-Warshall Lecturer: Abrahim Ladha Scribe(s): Richard Zhang 1 Single Source Shortest Path (with Negative Edges) Previously, we were dealing with finding the shortest path from a single source for a graph withnonnegativeedgeweights. WeusedDijkstra’salgorithmforthis,whichworkedbecause the paths to any node v would go through nodes closer to the source than v. This allows the algorithm to mark the node on the top of the priority queue as visited and not consider any more paths to this node. However, with negative edges, this useful property is broken! Consider the following graph: 100 C D 100 -1000 2 A B Figure 1: Graph with negative edge weights. Dijkstra’s will pop node B off the priority queue and mark its shortest distance to be 2 since the path from A to C seems to be much larger. However, the path with the least total weight would actually be A → C → D → B with a total weight of −800 (which is much lower than 2). Dijkstra’s has not accounted for shorter paths found in the future. Another issue arises with negative edges. Consider the graph below: C 1 1 A -1 -1 B D Figure 2: Graph with negative cycle 8: Bellman-Ford and Floyd-Warshall-1\nWhat would be the shortest path from A to B? The path from A to C to B with length 2 can be reduced to 0 by going through node D. However, we can go through node D multiple times and further decrease the path length possibly to −∞! C and D in this graph form a negative cycle since the sum of edges in the cycle is negative. There can be positive edge weights in a negative cycle, but the total sum of all the cycle edges is negative. Negative cycles make the shortest path problem ill-defined because of this, so we need a way to somehow detect this when designing a shortest path algorithm that accounts for negative edges. 1.1 Bellman-Ford The Bellman-Ford algorithm can help us with finding the shortest path from a single source with negative edge weights. The Bellman-Ford algorithm utilizes the same sort of distance update operation/relaxation that Dijkstra’s uses: dist(v) := min(dist(v),dist(u)+l(u,v)) u dist(u) l(u,v) dist(v) s v Figure 3: Distance update visualization Dijkstra has the privilege to stop performing this update operation for a node v when it is popped from the priority queue since a shorter path would to v would never happen in the future. However, with negative edges, Bellman-Ford may need to apply this update operation may more times to account for future shorter paths. By applying this update operation multiple times for all the vertices, Bellman-Ford can find the shortest paths (al- though without Dijkstra’s shortcuts). How does Bellman-Ford know when to stop updating the distance array? Let’s first assume that there are no negative cycles. If we were to apply this update operation once for all the edges in the graph, the nodes that are one edge away from the source are guaranteed to be updated. Apply another round of this, and the nodes one or two edges away are guaranteed to be updated. Essentially, the ith round would have found the shortest paths with at most i edges. We can keep this going until we reach the maximum number of edges that a shortest path in a graph can have, which is |V|−1 (visiting all |V| vertices). This is because a path with more edges would visit a vertex more than once, an indication of an unnecessary cycle that only increases the path length. With these insights, we can implement the algorithm as follows: 8: Bellman-Ford and Floyd-Warshall-2\ndef bellmanFord(G, l, s): for all v in V: dist(u) = infinity dist(s) = 0 for i=1...(|V| - 1) for each e=(u,v) in E dist(v) = min{dist(v), dist(u) + l(u, v)} Figure 4: Bellman-Ford pseudo-code Thereisnolongerapriorityqueue,andthisalgorithmappliesthesamedistanceupdateover and over again until the maximum possible number of updates are reached. The runtime wouldbeO(|V||E|)sinceweloopingthroughalltheedges|V|−1times. Thisissignificantly slower than Dijsktra’s runtime of O((|V|+|E|)logV) with a binary heap. 1.2 Example Let’s run Bellman-Ford on the following graph: 10 S A 11 -20 12 B C Figure 5: Bellman-Ford example graph The current distance values for each round is presented in the table below: Round dist(S) dist(A) dist(B) dist(C) 0 0 ∞ ∞ ∞ 1 0 10 11 ∞ 2 0 10 11 23 3 0 3 11 23 Figure 6: Bellman-Ford example distance values There are 3 rounds since |V|−1 = 3. At round 0, only the starting node S will have a distance value with the rest being infinity. As the rounds progress, notice how the distance values are each of the vertices are decreasing. Round 1 includes the shortest path distances with at most one edge, so the nodes A and B would have their distance values updated. 8: Bellman-Ford and Floyd-Warshall-3\nRound 2 includes shortest path distances with at most two edges, so C would have its distance value updated. The key observation here is the update on dist(A) in Round 3: dist(A) := min(dist(A),dist(C)+l(C,A)) This would evaluate to min(10,23−20) = 3, demonstrating how the distance value for a vertex can change multiple times (unlike Dijsktra’s). Since Round 3 considers the shortest paths with at most 3 edges, the path S → B → C → A is accounted for. 1.3 Negative Cycle Detection Bellman-Ford can also be used to detect if a negative cycle exists in a graph. With one more round, the shortest paths with |V| edges would be considered. However, this would mean that a vertex must have been visited at least twice in this shortest path if it used |V| edges. This means that there is a cycle that starts and ends at this vertex. If this cycle was a nonnegative one, then the shortest path value would not change since the cycle would only increase the total weight or keep it the same. However, if the cycle was negative, the shortest path distance would decrease. With these insights, we can do the following to detect a negative cycle: 1. Run one more round of Bellman-Ford. 2. If a distance value has decreased for some vertex, then there must be a negative cycle. 2 Shortest Paths in Directed Acyclic Graphs Without the presence of cycles, finding the shortest path in a directed acyclic graph (DAG) becomes much easier. Even with negative edges, the shortest path for a node is entirely based on the shortest paths from its incoming nodes. With the shortest paths calculated and fixed for the incoming nodes, the shortest path to the current node would be the minimum of the shortest paths of the incoming nodes plus their respective incoming edges. With the guaranteed topological order from the Top-Sort algorithm, we can compute the shortest paths from the bottom-up with the same update operation used in Dijkstra’s and Bellman-Ford. Here is the algorithm: def shortestPathInDag(G, l, s): for all v in V: dist(u) = infinity dist(s) = 0 Topsort G for u in V in topsort order for edge (u,v) in E: dist(v) = min(dist(v), dist(u) + l(u,v)) Figure 7: Shortest Path in DAG pseudo-code 8: Bellman-Ford and Floyd-Warshall-4\nLet’s run this algorithm on the following DAG: 7 2 3 1 S A B C 7 Figure 8: Shortest Path in DAG example graph For each current node in the outer loop, here are the following current distance values after the relaxation operations are applied in the inner loop: Current Node dist(S) dist(A) dist(B) dist(C) Before outer loop 0 ∞ ∞ ∞ S 0 2 7 ∞ A 0 2 5 9 B 0 2 5 6 C 0 2 5 6 Figure 9: Shortest Path in DAG example values Going in topological order ensures that all the incoming vertices of the current vertex were already processed. All the incoming edges to the vertex will be considered before the vertex is reached in the outer loop, which ensures that all possible shortest paths are considered and no possible path shorter is missed. The runtime of this algorithm would be O(|V|+|E|) since only the outgoing edges of each current vertex are processed once for the relaxation operation. This runtime is much more efficient than Dijkstra’s. 3 All Pairs Shortest Path Unlike the Single Source Shortest Path problem, the All Pairs Shortest Path problem looks fortheshortestpathsforallpossiblepairsofvertices. Insteadofonlyconsideringonevertex tobethestartingvertex, thisproblemnowhaseveryvertexinthegraphbeapossiblestart. One native solution is to run Dijsktra’s for each vertex in the graph and concatenate the distance arrays together. This would result in a runtime of O(|V|·(|V|+|E|)log|V|) = O((|V|2 +|V||E|)log|V|), which does not seem terrible. However, with a relatively dense graph, where |E| ∼ |V|2, the runtime would be O(|V|3log|V|), which does not look as good. Of course, an algorithm for this problem would need to be Ω(|V|2) since there are |V|2 possible pairs that need to be iterated on. 8: Bellman-Ford and Floyd-Warshall-5\n3.1 Floyd-Warshall We can achieve a faster runtime than running Dijkstra’s |V| times with the Floyd Warshall algorithm, which has a runtime of O(|V|3). Floyd-Warshall works by defining the following subproblem: dist(i,j,k) = shortest path from v to v only considering v ...v i j 1 k The nodes being considered are the possible vertices that could be in between v and v i j in the shortest path from v to v . Floyd-Warshall computes the shortest paths efficiently i j by reusing the shortest paths of possibly other pairs with the consideration of a smaller set of vertices. The algorithm inductively builds up the larger subproblems from smaller subproblems until all vertices are considered. We can store the answer to these smaller subproblems in an array for fast lookups. Onebasecasewouldbewhereweareconsideringtheshortestpathsfromanyvertexv toit- i self without considering any vertices in between. In this case, the shortest path distance for thesecaseswouldbe0,sodist(i,i,0) = 0forv ∈ V. Anotherbasecaseiswheretwovertices i are connected by an edge. Without any intermediate vertices being considered, the shortest path would have the distance value be the edge weight itself, so dist(i,j,0) = l(v ,v ) for i j all (v ,v ) ∈ E i j Supposebytheinductionhypothesisthatwehavecomputedalltheanswersfordist(i,j,k− 1) considering the vertices v ...v . Now let’s also consider the node v . 1 k−1 k v k dist(i,k,k−1) dist(k,j,k−1) dist(i,j,k−1) v v i j Figure 10: Inductive case in Floyd-Warshall There are two cases. One case is where the shortest path still does not include v , so the k shortest path would be what it was before v was being considered (dist(i,j,k−1)). The k other case is where v is in the shortest path between v and v when now considering v . k i j k The shortest path from v to v can be appended with the shortest path from v to v , i k k j resulting the shortest path from v to v now considering vertices v ...v . We can simply i j 1 k take the minimum of these two cases to get the answer to the larger subproblem. With these ideas, here is the pseudo code: 8: Bellman-Ford and Floyd-Warshall-6\ndef floydWarshall(G, l): # Base cases for i in range(|V|): dist(i, i) = 0 for i in range(|V|): for j in range(|V|): dist(i, j) = l(i, j) if defined, infinity otherwise # Inductive cases for k in range(|V|) for i in range(|V|) for j in range(|V|) dist(i,j) = min(dist(i,j), dist(i,k) + dist(k, j)) Figure 11: Floyd-Warshall pseudo-code Notice how a 2D array is being used in the implementation even though our dist sub- problem has three inputs. This is because only the dist values from the previous round (where v ...v are being considered) were needed, so there is no need to store all the 1 k−1 distance values before k−1. The runtime for this algorithm would be O(|V|3) due to the triple nested for loop. 3.2 Example Let’s run Floyd-Warshall on the graph below: 7 C D 2 2 9 E 5 1 B A Figure 12: Floyd-Warshall example graph Below are the distance array values when initialized with the base case values: 8: Bellman-Ford and Floyd-Warshall-7\ndist A B C D E A 0 5 ∞ 9 1 B 5 0 2 ∞ ∞ C ∞ 2 0 7 ∞ D 9 ∞ 7 0 2 E 1 ∞ ∞ 2 0 Figure 13: Floyd-Warshall base case distance values When A is being considered, we now consider shortest paths that have the node A in between. The distance array values become updated to the following: dist A B C D E A 0 5 ∞ 9 1 B 5 0 2 14 6 C ∞ 2 0 7 ∞ D 9 14 7 0 2 E 1 6 ∞ 2 0 Figure 14: Floyd-Warshall first round distance values When B is also being considered, we now consider shortest paths that have node A and/or node B in between (i.e. E to A to B to C). The distance array values become the following: dist A B C D E A 0 5 7 9 1 B 5 0 2 14 6 C 7 2 0 7 8 D 9 14 7 0 2 E 1 6 8 2 0 Figure 15: Floyd-Warshall second round distance values The distance table will continue to update until all the vertices have been considered, resulting in the shortest path distances between all pairs of nodes. 4 Dynamic Programming Sneak Peek Bellman-Ford and Floyd-Warshall employ a technique called dynamic programming. Dynamic programming breaks the problems down into smaller, overlapping subproblems. The answers to these subproblems are cached in some data structure and are used to solve bigger problems. You will learn more about dynamic programming in the next unit, so stay tuned! 8: Bellman-Ford and Floyd-Warshall-8"},
{"file": "dsa_slides/14_LinearProgramming (1).pdf", "content": "CS 3510 Algorithms: Linear Programming Aaron Hillegass Georgia Tech\n2/21 Linear Programming Problems • A set of variables to find assignments for: 𝑥 , …, 𝑥 1 𝑛 • A linear function to maximize: 𝑓(𝑥 , …, 𝑥 ) = 𝑐 𝑥 + … + 𝑐 𝑥 1 𝑛 1 1 𝑛 𝑛 • A set of linear inequalities of the form: 𝑎 ⃗ ⋅ 𝑥⃗ ≤ 𝑏 𝑖 Any vector ( 𝑥 ) that satisfies the inequalities is said to be feasible. 0 We are looking for the optimal solution ( 𝑥∗ ) that is feasible and maximizes 𝑓.\n3/21 Example: Cheapest Healthy Diet A kg of each food has nutritional content and a price: Food Calories Calcium Magnesium Vitamin C Price Milk 3000 32 720 10 12.00 Liver 1000 12 1200 20 2.60 Orange 150 2 100 100 9.50 Beef 1400 9 210 0 11.50 Spinach 220 40 2000 60 11.50 A human has daily requirements: 2000 calories, 22 of calcium, 120 of magnesium, 15 of vitamin C. What is the cheapest healthy diet?\n4/21 Example: Cheapest Healthy Diet Let 𝑥 , …, 𝑥 be the amount of each food in the daily diet. 1 5 We are minimizing 𝑓(𝑥)⃗ = 𝑐 𝑥 + 𝑐 𝑥 + 𝑐 𝑥 + 𝑐 𝑥 + 𝑐 𝑥 1 1 2 2 3 3 4 4 5 5 There is a contraint for each dietary requirement: 𝑎 𝑥 + 𝑎 𝑥 + 𝑎 𝑥 + 𝑎 𝑥 + 𝑎 𝑥 ≥ 𝑏 ) 𝑖,1 1 𝑖,2 2 𝑖,3 3 𝑖,4 4 𝑖,5 5 𝑖 And you can’t have negative amounts of food: 𝑥 ≥ 0 for all 𝑖 𝑖\n5/21 An example we can draw Item 1 sells for $1, Item 2 sells for $6. Maximize: 1.0𝑥 + 6.0𝑥 1 2 Each day, factory can only produce: • 400 or less total items • No more than 200 of Item 1 • No more than 300 of Item 2 Constraints: • 𝑥 ≤ 200 1 • 𝑥 ≤ 300 2 • 𝑥 + 𝑥 ≤ 400 1 2 • 𝑥 ≥ 0, 𝑥 ≥ 0 1 2\n6/21 Unsolveable\n7/21 We Love Matrices 1.0 Maximize: 𝑓(𝑥)⃗ = 1.0𝑥 + 6.0𝑥 𝑐 ⃗ = ( ) = ∇𝑓 1 2 6.0 Constraints: Maximize: 𝑐𝑇 ⃗ 𝑥⃗ • 𝑔 (𝑥)⃗ = 1.0𝑥 + 0.0𝑥 ≤ 200 1.0 0.0 ∇𝑔 1 1 2 1 𝑨 = ( ) = ( ) 0.0 1.0 ∇𝑔 2 • 𝑔 (𝑥)⃗ = 0.0𝑥 + 1.0𝑥 ≤ 300 1.0 1.0 ∇𝑔 2 1 2 3 200.0 • 𝑔 (𝑥)⃗ = 1.0𝑥 + 1.0𝑥 ≤ 400 ⃗ 𝑏 = ( ) 3 1 2 300.0 400.0 • 𝑥 ≥ 0, 𝑥 ≥ 0 1 2 Constraints: Variables as a vector : ⃗ • 𝑨𝑥⃗ ≤ 𝑏 𝑥 𝑥⃗ = ( 1) • 𝑥⃗ ≥ 0 𝑥 2\n8/21 Think Gradients Maximize: 𝑓(𝑥)⃗ = 1.0𝑥 + 6.0𝑥 1 2 Constraints: • 𝑔 (𝑥)⃗ = 1.0𝑥 + 0.0𝑥 ≤ 200 1 1 2 • 𝑔 (𝑥)⃗ = 0.0𝑥 + 1.0𝑥 ≤ 300 2 1 2 • 𝑔 (𝑥)⃗ = 1.0𝑥 + 1.0𝑥 ≤ 400 3 1 2\n9/21 The Simplex Method • Start at any corner of the feasible region • Walk an edge uphill until you hit another corner • Repeat until none of the edges you are on go uphill\n10/21 With three variables • Level sets of 𝑔 and 𝑓 are planes • Inequalities define half-spaces • Feasible region is convex 3D polytope • If bounded and non-empty, an optimal solution definitely exists. • And at least one corner is an optimal solution.\n11/21 Slack Variables How do we define “the corners”? • 1.0𝑥 + 0.0𝑥 + 𝑠 = 200 1 2 1 • 0.0𝑥 + 1.0𝑥 + 𝑠 = 300 1 2 2 • 1.0𝑥 + 1.0𝑥 + 𝑠 = 400 1 2 3 • 𝑥 , 𝑥 , 𝑠 , 𝑠 , 𝑠 ≥ 0 1 2 1 2 3 At each corner, how many are zero? How many real variables are we solving for?\n12/21 Implementing Simplex ⃗ Often 𝑥⃗ = 0 is feasible. An easy place to start: Only slack variables are non-zero. Variables that are non-zero are “basic”. At each corner, we pick an “entering” variable (no longer needs to be zero) and an “exiting” variable (one that will be locked down as zero). Then we maximize 𝑓 by solving for values for all “in” variables. There are two methods for implementing the simplex algorithmF: • Tableau – row operations, makes perfect sense if you are solving by hand. • Revised – vector operations, makes it easy to implement on a computer.\n13/21 Computation Complexity Let 𝑛 be the number of variables. Worst case is exponential in 𝑛. But, in practice, it is surprisingly efficient: polynomial in 𝑛.\n14/21 Quick Example • Maximize: 2𝑥 + 3𝑥 + 5𝑥 1 2 3 • Subject to: • 𝑥 + 2𝑥 + 𝑥 ≤ 4 1 2 3 • 𝑥 , 𝑥 , 𝑥 ≥ 0 1 2 3\n15/21 Linear Programming is Kinda Easy • Linear Integer Programming seems easier, but…\n16/21 Tableau Example • Maximize: 𝑧 = 3𝑥 + 𝑥 + 2𝑥 • 𝑥 enters, 𝑠 exits: 1 2 3 1 6 𝑥 𝑥 𝑠 • Subject to: • 𝑥 = 9 − 2 − 3 − 6 1 4 2 4 • 𝑥 + 𝑥 + 3𝑥 ≤ 30 • Rewrite other equations: 1 2 3 𝑥 𝑥 𝑠 • 2𝑥 + 2𝑥 + 5𝑥 ≤ 24 • 𝑧 = 27 + 2 + 3 − 3 6 1 2 3 4 2 4 3𝑥 5𝑥 𝑠 • 4𝑥 + 𝑥 + 2𝑥 ≤ 36 • 𝑠 = 21 − 2 − 3 + 6 1 2 3 4 4 2 4 3𝑥 𝑠 • 𝑥 , 𝑥 , 𝑥 ≥ 0 • 𝑠 = 6 − 2 − 4𝑥 + 6 1 2 3 5 2 3 2 • Rewrite with slack variables: • Which variable should we let be non- • 𝑠 = 30 − 𝑥 − 𝑥 − 3𝑥 zero now? 4 1 2 3 • 𝑠 = 24 − 2𝑥 − 2𝑥 − 5𝑥 • Which equality limits 𝑥 ? 5 1 2 3 3 3 3𝑥 𝑠 𝑠 • 𝑠 = 36 − 4𝑥 − 𝑥 − 2𝑥 • 𝑥 = − 2 − 5 + 6 6 1 2 3 3 2 8 4 8 ⃗ • Start search at 𝑥⃗ = 0 • Rewrite other equations and repeat: 33 𝑥 𝑠 5𝑠 • Increasing which variable will most • 𝑥 = − 2 + 5 − 6 1 4 16 8 16 69 3𝑥 5𝑠 𝑠 increase 𝑧? • 𝑠 = + 2 + 5 + 6 4 4 16 8 16 111 𝑥 𝑠 11𝑠 • Which constraint limits increase of 𝑥 ? • 𝑧 = + 2 − 5 − 6 1 4 16 8 16\n17/21 Simplex? • We are walking corners: 8𝑥 2𝑠 𝑠 • 𝑥 = 4 − 3 − 5 + 6 2 3 3 3 • Started (0, 0, 0, 30, 24, 36) → 𝑧 = 0 • Moved to (9, 0, 0, 21, 6, 0) → 𝑧 = 27 • Rewrite other equations: 33 3 69 𝑥 𝑠 2𝑠 • Moved to ( , 0, , , 0, 0) → 𝑧 = 27.75 • 𝑧 = 28 − 3 − 5 − 6 4 2 4 6 6 3 • What variable do we want to allow to be 𝑥 𝑠 𝑠 • 𝑥 = 8 + 3 + 5 − 6 1 6 6 3 non-zero? 𝑥 𝑠 • 𝑥 = 18 − 3 + 5 • 𝑧 = 111 + 𝑥 2 − 𝑠 5 − 11𝑠 6 4 2 2 4 16 8 16 • Which equation limits 𝑥 ? 2 • No way to increase 𝑧. Final 33 𝑥 𝑠 5𝑠 • 𝑥 = − 2 + 5 − 6 1 4 16 8 16 solution: 3 3𝑥 𝑠 𝑠 • 𝑥 = − 2 − 5 + 6 3 2 8 4 8 69 3𝑥 5𝑠 𝑠 • (8, 4, 0, 18, 0, 0) → 𝑧 = 28 • 𝑠 = + 2 + 5 + 6 4 4 16 8 16\n18/21 Primal vs. Dual Primal Problem Dual Problem Maximize 𝑐𝑇 ⃗ 𝑥⃗ Minimize 𝑏𝑇⃗ 𝑦⃗ Subject to 𝑨𝑥⃗ ≤ 𝑏 ⃗ and 𝑥 ≥ 0 Subject to 𝑨𝑇 𝑦⃗ ≥ 𝑐 ⃗ and 𝑦⃗ ≥ 0 From our example: Also our example: 0 8 𝑐𝑇 ⃗ 𝑥⃗ = [3, 1, 2]( ) = 28 𝑏𝑇⃗ 𝑦⃗ = [30, 24, 36]( ( 1) ) = 28 4 6 0 2 ( ) 3 max 𝑐𝑇 ⃗ 𝑥⃗ = min 𝑏𝑇⃗ 𝑦⃗\n19/21 Proof of Weak Duality Show that: max 𝑐𝑇 ⃗ 𝑥⃗ ≤ min 𝑏𝑇⃗ 𝑦⃗ max 𝑐𝑇 ⃗ 𝑥⃗ = 𝑥𝑇 ⃗ 𝑐 ⃗ ≤ 𝑥𝑇 ⃗ 𝑨𝑇 𝑦⃗ = (𝑨𝑥)⃗ 𝑇 𝑦⃗ ≤ 𝑏𝑇⃗ 𝑦⃗\n20/21 Simplex Solves Primal and Dual Minimize 30𝑦 + 24𝑦 + 36𝑦 Reminder: 1 2 3 𝑥 𝑠 2𝑠 • 𝑧 = 28 − 3 − 5 − 6 Subject to: 6 6 3 • 𝑠 is basic, so 𝑦 = 0 • 𝑦 + 2𝑦 + 4𝑦 ≥ 3 4 1 1 2 3 1 1 • Coefficent of 𝑠 = − , so 𝑦 = • 𝑦 + 2𝑦 + 𝑦 ≥ 1 5 6 2 6 1 2 3 2 2 • Coefficent of 𝑠 = − , so 𝑦 = • 3𝑦 + 5𝑦 + 2𝑦 ≥ 2 6 3 3 3 1 2 3 1 2 • 𝑦 , 𝑦 , 𝑦 ≥ 0 30(0) + 24( ) + 36( ) = 28 1 2 3 6 3\nQuestions? Slides by Aaron Hillegass Based on lectures by Abrahim Ladha"},
{"file": "dsa_slides/09_ChainMatrixKnapsack.pdf", "content": "CS 3510 Algorithms: Chain Matrix Multiplication and Knapsack on Integers Aaron Hillegass Georgia Tech\n2/35 Parenthesizations There are 5 different parenthesizations: A(B(CD)), (AB)(CD), A((BC)D), ((AB)C)D, (A(BC))D What is the fastest way?\n3/35 How many parenthesizations? For 𝑛 terms, let 𝐶 be the number of parenthesizations. 𝑛−1 Example: 𝐶 = 2 because: (𝐴𝐵)𝐶 , 𝐴(𝐵𝐶). 2 If we define 𝐶 = 1, then 0 𝑛 𝐶 = ∑ 𝐶 𝐶 𝑛 𝑘−1 𝑛−𝑘 𝑘=1 Why?\n4/35 Catalan Numbers 𝐶 are known as “The Catalan Numbers”: 𝑖 1, 1, 2, 5, 14, 42, 132, 429, 1430, 4862, 16796, 58786, … Easy formulas: (2𝑛)! 2𝑛 2𝑛 𝐶 = = ( ) − ( ) 𝑛 (𝑛+1)!𝑛! 𝑛 𝑛+1 How fast does it grow? 4𝑛 𝐶 ≈ 𝑛 3 √ 𝑛2 𝜋\n5/35 Parenthesization as tree (𝐴 (𝐴 𝐴 ))(𝐴 𝐴 ) 1 2 3 4 5\n6/35 A good approximation of cost? m colums A swor n k columns B swor m k columns = X C swor n 𝑛𝑚𝑘 Pessimistic? So what?\n7/35 Are some better than others? 𝐴((𝐵𝐶)𝐷) = (20 × 1 × 10) + (20 × 10 × 100) + (50 × 20 × 100) = 120, 200 operations (𝐴𝐵)(𝐶𝐷) = (50 × 20 × 1) + (1 × 10 × 100) + (50 × 1 × 100) = 7, 000 operations\n8/35 Notation A sequence of 𝑛 matrices [𝐴 , 𝐴 , …, 𝐴 ] 1 2 𝑛 A sequence of 𝑛 + 1 dimensions [𝑑 , 𝑑 , …, 𝑑 ] 0 1 𝑛 Where 𝐴 will be 𝑑 × 𝑑 . 𝑖 𝑖−1 𝑖\n9/35 Recursion: Best split point? • Cost of (𝐴 )(𝐴 𝐴 𝐴 𝐴 ) = 𝑑 𝑑 𝑑 + cost(𝐴 𝐴 𝐴 𝐴 ) 1 2 3 4 5 0 1 5 2 3 4 5 • Cost of (𝐴 𝐴 )(𝐴 𝐴 𝐴 ) = cost(𝐴 𝐴 ) + 𝑑 𝑑 𝑑 + cost(𝐴 𝐴 𝐴 ) 1 2 3 4 5 1 2 0 2 5 3 4 5 • Cost of (𝐴 𝐴 𝐴 )(𝐴 𝐴 ) = cost(𝐴 𝐴 𝐴 ) + 𝑑 𝑑 𝑑 + cost(𝐴 𝐴 ) 1 2 3 4 5 1 2 3 0 3 5 4 5 • Cost of (𝐴 𝐴 𝐴 𝐴 )(𝐴 ) = cost(𝐴 𝐴 𝐴 𝐴 ) + 𝑑 𝑑 𝑑 1 2 3 4 5 1 2 3 4 0 4 5\n10/35 Recurrence dp[𝑖, 𝑗] = min of • cost(𝐴 )(𝐴 …𝐴 ) = 𝑑 𝑑 𝑑 + cost(𝐴 𝐴 …𝐴 ) 𝑖 𝑖+1 𝑗 𝑖−1 𝑖 𝑗 𝑖+1 𝑖+2 𝑗 • cost(𝐴 𝐴 )(𝐴 …𝐴 ) = cost(𝐴 𝐴 ) + 𝑑 𝑑 𝑑 + cost(𝐴 …𝐴 ) 𝑖 𝑖+1 𝑖+2 𝑗 𝑖 𝑖+1 𝑖−1 𝑖+1 𝑗 𝑖+2 𝑗 • … • cost(𝐴 …𝐴 )(𝐴 ) = cost(𝐴 ..𝐴 ) + 𝑑 𝑑 𝑑 𝑖 𝑗−1 𝑗 𝑖 𝑗−1 𝑖−1 𝑗−1 𝑗\n11/35 Dynamic Programming Last dimension of last matrix Let dp be a 1..𝑛 × 1..𝑛 array. 1 2 3 4 5 5 3 2 10 4 For 𝑖 ≤ 𝑗, dp[i,j] is min cost of product 1 9 0 ? ? ? ? 𝐴 …𝐴 𝑖 𝑗 2 5 0 ? ? ? Base case: dp[i,i] := 0 3 3 0 ? ? Solution: dp[1,n] 4 2 0 ? [𝐴 , 𝐴 , 𝐴 , 𝐴 , 𝐴 ] 9×5 5×3 3×2 2×10 10×4 5 10 0 First dimension of frst matrix\n12/35 Recurrence: try all splits For any dp[i,j], try all 𝑖 ≤ 𝑘 < 𝑗 : Split:(𝐴 …𝐴 )(𝐴 ..𝐴 ) 𝑖 𝑘 𝑘+1 𝑗 𝑓(𝑘) = dp[𝑖, 𝑘] + 𝑑 𝑑 𝑑 + dp[𝑘 + 1, 𝑗] 𝑖−1 𝑘 𝑗 dp[𝑖, 𝑗] ≔ min 𝑓(𝑘) 𝑖≤𝑘<𝑗\n13/35 The no-choice cases 1 2 3 4 5 5 3 2 10 4 dp[0,2] = 𝑑 𝑑 𝑑 = (9)(5)(3) = 135 0 1 2 9 0 135 ? ? ? 1 dp[1,3] = 𝑑 𝑑 𝑑 = (5)(3)(2) = 30 1 2 3 5 0 30 ? ? 2 dp[2,4] = 𝑑 𝑑 𝑑 = (3)(2)(10) = 60 2 3 4 3 0 60 ? 3 dp[3,5] = 𝑑 𝑑 𝑑 = (2)(10)(4) = 80 3 4 5 2 0 80 4 10 0 5\n14/35 The 2 possiblities cases 1 2 3 4 5 5 3 2 10 4 0 135 120 ? ? 9 1 0 30 130 ? 5 2 0 60 104 3 k=1: A (A A ): (9)(5)(2) + 30 = 120 3 1 2 3 0 80 2 Cost of (A A A ) =dp[1,3] = min 4 1 2 3 0 10 5 k=2: (A A )A : 135 + (9)(3)(2) = 189 1 2 3\n15/35 The 3 possiblities cases 1 2 3 4 5 5 3 2 10 4 0 135 120 300 ? 9 1 0 30 130 150 5 2 0 60 104 3 k=1: A (A A A ): (9)(5)(10) + 130 = 580 3 1 2 3 4 0 80 2 4 Cost of (A A A A ) =dp[0,4] = min k=2: (A A )(A A ): 135 + (9)(3)(10) +60 = 465 1 2 3 4 1 2 3 4 0 10 5 k=3: (A A A )A : 120 + (9)(2)(10) = 300 1 2 3 4\n16/35 The final case: 4 possibilities 1 2 3 4 5 5 3 2 10 4 0 135 120 300 272 9 1 0 30 130 150 5 2 k=1: A (A A A A ): (9)(5)(4) + 150 = 330 0 60 104 3 3 1 2 3 4 5 k=2: (A A )(A A A ): 135 + (9)(3)(4) +104 = 347 0 80 2 4 1 2 3 4 5 Cost of (A A A A A ) =dp[0,5] = min 1 2 3 4 5 k=3: (A A A )(A A ): 120 + (9)(2)(4) + 80 = 272 1 2 3 4 5 0 10 5 k=4: (A A A A )A : 300 + (9)(10)(4) = 660 1 2 3 4 5\nBut what is the best parenthization?\n18/35 Backtracking 1 2 3 4 5 5 3 2 10 4 1 9 0 135 120 300 272 (𝐴 (𝐴 , 𝐴 ))(𝐴 , 𝐴 ) 9×5 5×3 3×2 2×10 10×4 2 5 0 30 130 150 (5 × 3 × 2) + (9 × 5 × 2) + (2 × 10 × 4) + (9 × 2 × 4) 30 + 90 + 80 + 72 = 272 3 3 0 60 104 4 2 0 80 5 10 0\n𝑘 Just remember best for each cell.\n20/35 Backtracking (𝐴 (𝐴 𝐴 ))(𝐴 𝐴 ) 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 5 3 2 10 4 5 3 2 10 4 1 9 0 135 120 300 272 9 0 1 1 3 3 1 2 5 0 30 130 150 5 0 2 3 3 2 3 3 0 60 104 3 0 3 3 3 4 2 0 80 2 0 4 4 5 10 0 10 0 5\n21/35 Gathering data 1 define chain(𝑛, 𝑑 : array[0..𝑛]): dp ≔ array[1..𝑛, 1..𝑛] of zeros 2 best_k ≔ array[1..𝑛,1..𝑛] 3 4 for seqlen ∈ 2..𝑛 : 5 for i ∈ 1..(𝑛 − seqlen + 1): j ≔ i + seqlen − 1 6 bv ≔ MAX 7 8 for 𝑘 ∈ [𝑖..𝑗 − 1]: 𝑣 ≔ dp[𝑖,𝑘] + 𝑑[𝑖 − 1]𝑑[𝑘]𝑑[𝑗] + dp[𝑘 + 1,𝑗] 9 10 if 𝑣 < bv: 11 bv = 𝑣; bk = 𝑘 dp[𝑖,𝑗] ≔ bv 12 best_k[𝑖, 𝑗] ≔ bk 13\n22/35 Showing results print(\"Min ops: \" + dp[1, n]) // Min ops: 272 print(split_string(best_k, 1, n) // ((A1)(A2.A3))(A4.A5) 1 define split_string(best_k, 𝑖, 𝑗): 2 if 𝑖 == 𝑗: 3 return “A” + i 4 if 𝑖 + 1 == 𝑗: 5 return “A” + i + “.A” + j 𝑘 = best_k[𝑖, 𝑗] 6 7 return “(“ + split_string(best_k, 𝑖, 𝑘) + “)(“ + split_string(best_k, 𝑘 + 1, 𝑗)\n23/35 Complexity Memory grows slower than the runtime: • Memory: 𝑂(𝑛2) • Runtime: 𝑂(𝑛3)\n𝑊 Your knapsack holds kg. 1…𝑛 𝑤 There are items with weight and 𝑖 𝑣 valued . 𝑖 Which do you steal?\n25/35 Brute force There are 2𝑛 possible combinations. Compute the value of each. Take the argmax.\n26/35 Greedy Algorithm 𝑣 • Sort items by decreasing “value density”: 𝑖 𝑤 𝑖 • Start at highest value density and keep packing until full. • When does this definitely get you the highest possible value? Knapsack holds 9 kg Weight: 7kg Weight: 3kg Weight: 3kg Weight: 3kg Value: $700 Value: $250 Value: $250 Value: $250\n27/35 Possibilities • Fractional Knapsack: The treasure is pellets/liquid so you can take fractions • 0/1 Knapsack: You take the whole item or none.\nDynamic Programming Solution?\n29/35 Dynamic Programming Solution? Last item added to options Works when weights are integers. 0 1 2 3 4 Let dp[r, i] represent the max value for 2,2.00 1,0.75 1,1.05 2,2.50 weight up to 𝑟, picking from items 1, …, 𝑖. 0 0 0 0 0 0 𝑖 goes from 0 through 𝑛. 1 0 ? ? ? ? 𝑟 goes from 0 through 𝑊. Base cases: 2 0 ? ? ? ? • dp[0, i] = 0 3 0 ? ? ? ? • dp[r, 0] = 0 Solution: dp[W, n] 4 0 ? ? ? ? Recurrence? Weight in kg. W = 5 0 ? ? ? ?\n30/35 Recurrence? Last item added to options 0 1 2 3 4 • 𝑤 > 𝑟? Can’t steal item 𝑖: 𝑖 2,2.00 1,0.75 1,1.05 2,2.50 dp[𝑟, 𝑖] ≔ dp[𝑟, 𝑖 − 1] 0 0 0 0 0 0 If 𝑤 ≤ 𝑟, you might steal it. If it is the most 𝑖 profitable move. 1 0 0 0.75 1.05 1.05 • Steal is best? 2 0 2.0 2.0 2.0 2.5 dp[𝑟, 𝑖] ≔ 𝑣[𝑖] + dp[𝑟 − 𝑤[𝑖], 𝑖 − 1] 3 0 2.0 2.75 3.05 3.55 • No steal is best? 4 0 2.0 2.75 3.80 4.50 dp[𝑟, 𝑖] ≔ dp[𝑟, 𝑖 − 1] Weight in kg. W = 5 0 2.0 2.75 3.80 5.55\n31/35 Knapsack Pseudocode 1 define knapsack(𝑊 : int, 𝑤 : int[1..𝑛], 𝑣 : float[1..𝑛]): dp ≔ new array(0..𝑊, 0..𝑛) 2 3 for 𝑖 ∈ 0..𝑛: dp[0, 𝑖] ≔ 0.0 4 for 𝑟 ∈ 0..𝑊: dp[𝑟, 0] ≔ 0.0 5 for 𝑟 ∈ 1..𝑊: 6 for 𝑖 ∈ 1..𝑛: 7 if 𝑤[𝑖] > 𝑟: dp[𝑟, 𝑖] ≔ dp[𝑟, 𝑖 − 1] 8 9 else: dp[𝑟, 𝑖] ≔ max(dp[𝑟, 𝑖 − 1], 𝑣[𝑖] + dp[𝑟 − 𝑤[𝑖], 𝑖 − 1]) 10 11 return dp[𝑊, 𝑛]\n32/35 Backtracking Last item added to options 0 1 2 3 4 items ≔ empty list 1 2,2.00 1,0.75 1,1.05 2,2.50 (𝑟, 𝑖) = (𝑊, 𝑛) 2 3 while i > 0: 0 0 0 0 0 0 4 if dp[𝑟, 𝑖] == dp[𝑟, 𝑖 − 1]: 1 0 0 0.75 1.05 1.05 𝑖 ≔ 𝑖 − 1 5 6 else: 2 0 2.0 2.0 2.0 2.5 items.append(𝑖) 7 8 𝑖 ≔ 𝑖 − 1 3 0 2.0 2.75 3.05 3.55 𝑟 ≔ 𝑟 − 𝑤[𝑖] 9 4 0 2.0 2.75 3.80 4.50 10 return “items” Weight in kg. W = 5 0 2.0 2.75 3.80 5.55\n33/35 Complexity? Memory: 𝑂(𝑊 𝑛) Compute: 𝑂(𝑊 𝑛) But only works if weights are integers! What do we do if they are floats?\n34/35 Weights are floats? Use branch and bound. Pick a subset of items 1 to 𝑗 < 𝑛. Use fractional knapsack for upper bound of value that fits in in remaining space from items 𝑗 + 1 to 𝑛. It is NP-Hard.\nQuestions? Slides by Aaron Hillegass"},
{"file": "dsa_slides/NotesLinearProgramming.pdf", "content": "CS 3510 Algorithms 4/11/2024 Lecture 18: Linear Programming Lecturer: Abrahim Ladha Scribe(s): Saigautam Bonam Linear programming is a class of problems in which you have a linear set of constraints, multiple variables, and an objective function. The goal is to find a solution to the variables that maximizes the objective function while satisfying all constraints. Suppose you sell items. Let’s say item 1 sells for 1 dollar, item 2 for 6 dollars (item 2 is a luxury item). Your factory can currently produce only 400 items a day. The factory also cannot make more than 200 of item 1 and 300 of item 2 due to sanctions. We may present this problem as the following linear program: Objective Function: max{x +6x }, subject to the following constraints: 1 2 x ≤ 200 1 x ≤ 300 2 x +x ≤ 400 1 2 x ,x ≥ 0 1 2 We can also represent these attributes as the following matrices:     1 0 200 (cid:20) (cid:21) 1 A = 0 1b = 300c = 6 1 1 400 where you want to maximize cTx subject to Ax ≤ b and x ≥ 0. A represents the LHS of the constraints, and is a m×n matrix where m is the number of constraints, and n is the number of variables. b is a m×1 vector represents the RHS of the constraints, while c is a n×1 vector represents the coefficients of the objective function. We can plot the constraint inequalities and find the region in which all are satisfied below. 400 300 200 100 100 200 300 400 18: Linear Programming-1\nThis is called the feasible region. The optimal solution would be when the line corre- sponding to x +6x intersects the shaded region with the highest value. That would be at 1 2 (x ,x ) = (100,300). This makes sense intuitively; since the coefficient is higher for item 2, 1 2 we would want to satisfy the 400-item constraint by decreasing quantity of item 1 instead of item 2. The optimal solution of a linear program (LP) is only unachievable when the constraints are either infeasible or unbounded. For an infeasible example, the constraints x ≤ −1 1 and x ≥ 1 will not be able to be satisfied. For an unbounded example, consider the only 1 constraint is x ,x ≥ 0 and the objective function as max{x +x }. There is no bounded 1 2 1 2 solution. We can write the standard form for an LP as follows using the A,b,c matrices defined earlier:   x 1 1. We wish to find solution x =   x 2  ... x n 2. The objective function is maxcTx. 3. The objective function is subject to constraints Ax ≤ b and x ≥ 0. 1 Three Classic Examples Surprisingly, many problems can be represented by LPs. ShortestPath: SupposeinagraphGwewanttocomputetheshortests−tpath. Letd be v the shortest path from s to v. Note that d = 0. We may add constraints d ≤ d +w(u,v) s v u for pairs of vertices u,v in G that share an edge. We can write the LP as follows: Objective Function : maxd t subject to d ≤ d +w(u,v) ∀(u,v) ∈ E v u and d = 0 s Why do we maximize instead of minimize in the objective function? d = 0 is the mini- t mum solution, but the rest of the constraints have d = min {d +w(u,t)} need to t u,(u,t)∈E u enforcethatd isactuallyapath! d isthelargestvaluelessthanorequaltoallthepathsset. t t Max Flow: Given a flow network G where edges have capacities c(u,v) ≥ 0, we want to maximize the flow from s to t. Here’s the LP: 18: Linear Programming-2\n(cid:88) (cid:88) Obj. Fn (sum of flows from s) : max{ f(s,v)− f(v,s)} v∈V v∈V subj. to constraints for all edges f(u,v) ≤ c(u,v) f(u,v) ≥ 0 (cid:88) (cid:88) f(v,u) = f(u,v) (flow conservation) v∈V v∈V The second term of the objective function is usually not required, as the incoming flow to s is usually 0 in most graphs we’ve seen, as s is a source. However, it’s still necessary to write it in case it’s not a well-formed graph. The constraints regarding the flow f(u,v) ensure thatthemaxflowthroughanedgeisthecapacityoftheedge, andthatthereisnonnegative flow. Note that a constraint like x = y is not in standard form, but can be transformed into one with constraints x−y ≤ 0 and y−x ≤ 0 Knapsack. We’re given items (v ,w ) where we want to maximize the value of the item i i while ensuring the weight is below some constraint. Knapsack is already formatted as an LP! Consider the variables x ...x denoting a 1 if we choose an item and 0 and if we don’t. 1 i (cid:88) Obj. Fn : max( v x ) i i (cid:88) subj. to constraints w x < W i i and 0 ≤ x ≤ 1 i Note this is technically not an LP, as you cannot take a fractional amount of items. It is an ILP. 2 Simplex How do we solve LPs? Let’s go over an algorithm to solve them called Simplex. Think of it as a traversal over the polytope of the feasible region. It is necessary that one of the corners of our polytope must be the optimal solution for our objective function. This is because our objective function is linear. Think about it like this: if you’re on an edge and looking in two directions, there’s one direction that will lead to a greater result and another that will lead to a smaller result. Choose the direction that leads to a larger result, and that will stop only when the edge stops. Here’s the Simplex algorithm that revolves around the same idea; choose the direction that maximizes the objective function: begin corner v = (0, 0) while True: if any corner v’ neighbor of v is more optimal: v = v’ else break 18: Linear Programming-3\nTo more easily determine v′, we can change the origin to the current vertex v repeatedly by using a modified coordinate system. Let’s look at this algorithm in action. Consider the following LP: Obj. Fn : max{2x +5x } 1 2 subj. to constraints 2x −x ≤ 4 (a) 1 2 x +2x ≤ 9 (b) 1 2 −x +x ≤ 3 (c) 1 2 x ≥ 0 (d) 1 x ≥ 0 (e) 2 Finding the intersection of all these constraints gives us this polytope: Notes/intersection.png Let’s start with the corner (0,0). Looking at the two adjacent corners, these are (0,3) and (2,0). Plugging these into our objective function, we can see that (0,3) gives us a higher value, which makes sense given the coefficient for x is larger than the coefficient for x . We 2 1 move to vertex (0, 3). From there, we can see the next corner is (1, 4), which results in a larger value. We move to (1, 4). However, from (1, 4), we can see that the next coordinate, which is (3.4, 2.8), actually results in a smaller value in our objective function. As a result, we stop at (1, 4), which is our solution to the LP. This variant of LP allows solutions to be real numbers, which guarantees by linearity that the solutions can be found by moving through the edges of the feasible region. This makes LP, on average, a polynomial time problem to solve. However, a variant of LP is Integer Linear Programming (ILP), which corresponds to Knapsack! ILP is an NP-complete decision variant of LP where the solutions must be integers. Let’s prove it’s NP-Hard. We can reduce from Vertex Cover instead of Knapsack. Recall the definition of Vertex Cover: given input G,g, it returns if there exists a set S ⊂ V of size g such that every edge has an endpoint in S. For the reduction: for each edge (u,v) in G, add variables x ,x which can take on values 0 or 1 and the constraint x +x ≥ 1, u v u v which symbolizes choosing one vertex. The objective function is to maximize the sum of the variables! If we can get at least g as a result from the ILP, we return True for Vertex Cover. Otherwise, we return False. 18: Linear Programming-4"},
{"file": "dsa_slides/03_QuickSortSelect.pdf", "content": "CS 3510 Algorithms: Quicksort, Quickselect, and Max Subarray Sum Aaron Hillegass Georgia Tech\n2/23 The one that stumped me 𝑂(𝑛𝑎) < (log 𝑛)log𝑛 = 𝑛loglog𝑛 < 𝑂(𝑏𝑛) for any 𝑎 or 𝑏 How do we know they are equal? 𝑎log𝑏 = 𝑏log𝑎 𝑂(𝑛𝑎) < 𝑛loglog𝑛 because 𝑎 is a constant and log log 𝑛 grows. To show 𝑛loglog𝑛 ≤ 𝑏𝑛 , take log of both sides: (log log 𝑛)(log 𝑛) ≤ 𝑛 log 𝑏 Ignore the constant log 𝑏: (loglog𝑛)(log𝑛) (log𝑛)(log𝑛) (2log𝑛) lim < lim = lim 𝑛→∞ 𝑛 𝑛→∞ 𝑛 𝑛→∞ 𝑛 Apply l’Hopital’s rule: 2 lim = 0 𝑛→∞ 𝑛 So 𝑏𝑛 grows faster than 𝑛loglog𝑛 .\n3/23 Quicksort Invented in 1959 by Tony Hoare Divide-and-Conquer Usually a little quicker than Mergesort or Heapsort Sorts in-place – no additional memory requirements\n4/23 Quicksort Algorithm • Partition • Pick a “pivot” from the data. • Move through the array swapping as necessary to move things less than the pivot to one end and greater than the pivot to the other. Note:𝑂(𝑛) • Swap the pivot into the border between “Smaller Zone” and “Bigger Zone” • Return the new location of the pivot • Continue • Partition everything to the left of the pivot • Partition everything to the right of the pivot\n5/23 Partition 1 define partition(𝐴, lo, hi): pivot ≔ 𝐴[hi] 2 𝑖 ≔ lo 3 4 for 𝑗 in 𝑖 to hi − 1: 5 if 𝐴[𝑗] ≤ pivot: 6 swap 𝐴[𝑖] with 𝐴[𝑗] 𝑖 ≔ 𝑖 + 1 7 8 swap 𝐴[𝑖] with 𝐴[hi] 9 return 𝑖\n6/23 Quicksort 1 define qsort(𝐴, lo, hi): 2 if hi ≤ lo: 3 return 𝑝 ≔ partition(𝐴, lo, hi) 4 qsort(𝐴, lo, 𝑝 − 1) 5 qsort(𝐴, 𝑝 + 1, hi) 6\n7/23 Best Case Analysis You pick the median every time! 𝑛−1 𝑛 𝑇 (𝑛) = 2𝑇 ( ) + 𝑂(𝑛) < 2𝑇 ( ) + 𝑂(𝑛) 2 2 𝑇 (𝑛) ∈ 𝑂(𝑛 log 𝑛)\n8/23 Worst Case Analysis You pick the min/max every time! 𝑇 (𝑛) = 𝑇 (𝑛 − 1) + 𝑂(𝑛) 𝑇 (𝑛) ∈ 𝑂(𝑛2)\n9/23 Average Case 𝑇(𝑛) = 1 ∑ 𝑛−1 [𝑇(𝑖) + 𝑇(𝑛 − 𝑖)] + 𝑂(𝑛) = 2 ∑ 𝑛−1 𝑇(𝑖) + 𝑛 𝑛 𝑖=0 𝑛 𝑖=0 𝑛𝑇(𝑛) = 2 ∑ 𝑛−1 𝑇(𝑖) + 𝑛2 𝑖=0 (𝑛 − 1)𝑇(𝑛 − 1) = 2 ∑ 𝑛−2 𝑇(𝑖) + (𝑛 − 1)2 𝑖=0 𝑛𝑇(𝑛) − (𝑛 − 1)𝑇(𝑛 − 1) = 2𝑇(𝑛 − 1) + 2𝑛 − 1 𝑛𝑇(𝑛) = (𝑛 + 1)𝑇(𝑛 − 1) + 2𝑛 Divide by 𝑛(𝑛 + 1) 𝑇(𝑛) = 𝑇(𝑛−1) + 2 evaluated at 𝑛 − 1: 𝑇(𝑛−1) = 𝑇(𝑛−2) + 2 𝑛+1 𝑛 𝑛+1 𝑛 𝑛−1 𝑛 𝑇(𝑛) = 𝑇(𝑛−2) + 2 + 2 𝑛+1 𝑛−1 𝑛 𝑛+1 𝑇(𝑛) = 𝑇(1) + 2 ∑ 𝑛+1 1 𝑛+1 2 𝑖=3 𝑖\n10/23 Harmonic Numbers 𝑛 1 𝐻 = ∑ 𝑛 𝑖=1 𝑖 𝐻 ∈ 𝑂(log 𝑛) 𝑛 𝑇(𝑛) 𝑇(1) 𝑛+1 1 = + 2 ∑ 𝑛+1 2 𝑖=3 𝑖 𝑇(𝑛) 𝑇(1) = + 2𝑂(log 𝑛) 𝑛 2 𝑇 (𝑛) = 𝑂(𝑛 log 𝑛) Quicksort ∈ 𝑂(𝑛 log 𝑛)\n𝑘 How could you find the -th smallest item in an array?\n12/23 Quickselect 1 define qselect(𝐴, lo, hi, 𝑘): 2 if hi == lo: 3 return 𝐴[lo] 𝑝 ≔ partition(𝐴, lo, hi) 4 worst pivots: 𝑂(𝑛2) 5 if 𝑘 == 𝑝: return 𝐴[𝑝] best pivots: 𝑂(𝑛) 6 if 𝑘 < 𝑝: 7 return qselect(𝐴, lo, 𝑝 − 1, 𝑘) 8 if 𝑘 > 𝑝: 9 return qselect(𝐴, 𝑝 + 1, high, 𝑘)\nPicking a good pivot is a big deal. Can we eliminate the worst cases?\n14/23 Median of Medians Guarantees that pivot is between 30th and 70th percentile.\n15/23 Interlocking Proofs We will prove three things. For all 𝐴: • Median of Medians runs in 𝑂(𝑛) if we use Quickselect to find the medians. • Quickselect runs in 𝑂(𝑛) if use Median of Medians to find pivots. • Quicksort runs in 𝑂(𝑛 log 𝑛) if we use Median of Medians to find pivots. Let the time to run Quickselect be given by 𝑇(𝑛). Doing a partition is 𝑂(𝑛) The time for MoM to find the median of the medians using Quickselect would be 𝑇(𝑛) 5 We know that the worst case using MoM is that the scan only cuts the search space by 30%. 𝑛 7𝑛 𝑇 (𝑛) ≤ 𝑇 ( ) + 𝑇 ( ) + 𝑂(𝑛) 5 10\n16/23 Interlocking Proofs 𝑛 7𝑛 𝑇 (𝑛) ≤ 𝑇 ( ) + 𝑇 ( ) + 𝑂(𝑛) 5 10 Can we find a function 𝑇 that satisfies this? Let’s try a linear function: 𝑇(𝑛) = 𝑐𝑛 𝑐𝑛 ≤ 𝑎𝑛 + 𝑐(𝑛) + 𝑐(( 7 )𝑛) = (1 + 𝑐( 9 ))𝑛 for some 𝑎 > 0 5 10 10 𝑐 ≤ 𝑎 + 9 𝑐 10 𝑐 ≤ 10𝑎 𝑇(𝑛) ≤ 10𝑎𝑛 𝑇(𝑛) is linear. That is, Quickselect works in 𝑂(𝑛).\n17/23 Interlocking Proofs Quickselect find medians 𝑂(𝑛). Median of Medians for an array of 𝑛, requires ( 𝑛 medians of 5) + (1 median of 𝑛 ). 5 5 It is 𝑂(𝑛). Worst case Quicksort using MoM: 𝑇(𝑛) = 𝑇(7𝑛) + 𝑇(3𝑛) + 𝑂(𝑛) 10 10 Quicksort using Median of Medians is 𝑂(𝑛 log 𝑛)\n18/23 Good Pivots in Quicksort First or last: Terrible on lists that are already sorted Random: Usually 𝑂(𝑛 log 𝑛) Median of Three: Almost always 𝑂(𝑛 log 𝑛) Median of Medians: Guaranteed 𝑂(𝑛 log 𝑛) In practice: Median of Three\n19/23 The Maximum Subarray Sum Problem Given an array 𝐴[1…𝑛] of numbers, find a subarray with the max sum. Or find 1 ≤ 𝑖 ≤ 𝑗 ≤ 𝑛 that maximizes the sum: 𝑗 ∑ 𝐴[𝑘] 𝑘=𝑖 Brute force? 𝑂(𝑛6) Grenander? 𝑂(𝑛2) Shamos Divide-and-Conquer? 𝑂(𝑛 log 𝑛)\n20/23 Divide and Conquer for Max Subarray Joining two? • max_left ≔ max(𝐴.max_left, 𝐴.total + 𝐵.max_left) • max_right ≔ max(𝐵.max_right, 𝐵.total + 𝐴.max_right) • total ≔ 𝐴.total + 𝐵.total • max ≔ max(max_left, max_right, 𝐴.max, 𝐵.max, 𝐴.max_right + 𝐵.max_left) Array of one? • All of the above are equal: value in the array.\n21/23 Divide and Conquer for Max Subarray 1 define analyze(𝐴): 𝑛 ≔ len(𝐴) 2 3 if 𝑛 == 1: 4 return (ml : 𝐴[1], mr : 𝐴[1], total : 𝐴[1], mx : 𝐴[1]) 𝑐 ≔ ⌊𝑛⌋ 5 2 1 define max_subsum(𝐴): 6 𝐿 ≔ 𝐴[1..𝑐], 𝑅 ≔ 𝐴[(𝑐 + 1)..𝑛] (_, _, _, mx) ≔ analyze(A) 2 (lml, lmr, ltotal, lmx ≔ analyze(L) 7 3 return mx (rml, rmr, rtotal, rmx ≔ analyze(R) 8 9 return (ml : max(lml, ltotal + rml), mr : max(rmr, rtotal + lmr), 10 total : ltotal + rtotal, 11 mx : max(lml, rml, lmx, rmx, lrm + rlm)) 12\n22/23 Divide and Conquer for Max Subarray 𝑛 𝑇 (𝑛) = 2𝑇 ( ) + 𝑂(1) 2 𝑂(𝑛)\nQuestions? Slides by Aaron Hillegass"},
{"file": "dsa_slides/NotesSeqDP.pdf", "content": "CS 3510 Algorithms 2/22/2024 Lecture 10: Longest Sequences Lecturer: Abrahim Ladha Scribe(s): Adam Zamlynny 1 Longest Common Substring Take the two strings “EL GATO” and “GATER”. They both share the letter E which is a common substring, but they also contain the string “GAT” which is the longest common substring. Given two strings, we want to find their longest common substring. Brute-force here, while not that bad can be improved by DP. We do this using two degrees of freedom where the first is the index of the first string, and the second is the index of the second string. We will solve instead the longest common suffix problem, and then transform out solu- tion into solving the longest common substring. Let a ,a be any strings with a y,a y having the longest common suffix y. What is the 1 2 1 2 longest common substring a yb ,a yb . We might be able to append letters here if y is the 1 1 2 2 suffix and yb will remain the longest common suffix. Thinking about this, we come to the 1 following. If b = b , then yb = yb is the longest common suffix of a yb ,a yb . If b ̸= b , 1 2 1 2 1 1 2 2 1 2 then y is the longest common substring of a yb ,a yb , but a yb ,a yb have no common 1 1 2 2 1 1 2 2 suffix, so their longest common suffix is of length zero. Think about what the last step is and how that will effect your answer. If you can inductively come up with an answer from previous answers you’ll save a lot of time. Let’s define the elements of our table. We’ll have a two dimensional table T, indexed by the indices i and j. Index i is the index into the first string and j the index into the second string. (cid:26) (cid:27) T[i−1,j −1]+1, if x = y T[i,j] = i j 0, if x ̸= y i j Now we can write the code to fill in the table. Here the maximum suffix over all prefixes is the maximum of the substrings, so instead of finding that at the end, we’re going to do it as we fill in the table. 10: Longest Sequences-1\ndef lcsubstring(x, y): initialize dp as a table of size |x| + 1 by |y| + 1 as 0s max = 0 maxpos = (0, 0) for i in 1..(|x| + 1) for j in 1..(|y| + 1) if x[i - 1] = y[j - 1] dp[i, j] = dp[i - 1, j - 1] + 1 else dp[i, j] = 0 if max < dp[i, j] max = dp[i, j] maxpos = i, j Figure 1: Longest common substring algorithm. Considering it in terms of suffices is easier than solving this problem in terms of sub- strings. The table stores the suffix lengths, and the longest suffix of a prefix is of course the longest substring. We’ll fill in the table. E L G A T O 0 0 0 0 0 0 0 G 0 0 0 1 0 0 0 A 0 0 0 0 2 0 0 T 0 0 0 0 0 3 0 E 0 1 0 0 0 0 0 R 0 0 0 0 0 0 0 Figure 2: Longest common substring example table. When filling in the table, we only look if the two letters in the strings are equal and if they are we add one to the element to the left and up. Runtime: n×m table with O(1) work for each entry, so O(nm) overall runtime and space complexity, where n = |x|, and m = |y|. 2 Longest Common Subsequence Taketheexamplex = a,b,c,b,d,a,bandy = b,d,c,a,b,a. Subsequencesarenotcontiguous, soforexamplethesubsequencesb,c,b,aandb,d,a,barebothvalid. Againlet’sthinkabout 10: Longest Sequences-2\nthe last letter that we add since this will be eaiser for the dp approach: if we add one character, how does that change our state and our answer. Let X = x ,...,x , Y = y ,...,y , Z = z ,...z where Z is the longest common sub- 1 m 1 n 1 k sequence. Then if x = y , then z = x = y and z ,...,z is the longest common m n k m n 1 k−1 subsequence. If x ̸= y , then Z ̸= X or Z ̸= Y and Z is still the longest common m n k m k n subsequence. Let’s define our recurrence T[i,j] is the longest common subsequence of x ,...,x and 1 i y ,...,y . If either of the i or j is zero, we want zero because there could not be a subse- 1 j quence. If the two letters are equal we want to add that character so we take the longest common subsequence of both strings without that character and add one for that charac- ter. Otherwise, the longest common subsequence is the maximum of the longest common subsequences with one less character.   0, if i = 0 or j = 0   T[i,j] = T[i−1,j −1]+1, if x = y i j  max(T[i−1,j],T[i,j −1]), if x ̸= y  i j You take the max, since if the letters are different, it may increase a previous longest subsequence in one, but not both. Consider this example, where d ̸= e but d does increase one subsequence. ......a.....b...c....|d ..a...b...c...d....|e def lcsubsequence(x, y): initialize dp as a table of size |x| + 1 by |y| + 1 as 0s initialize bt as a table of size |x| + 1 by |y| + 1 as 0s for i in 1..(|x| + 1) for j in 1..(|y| + 1) if x[i] = y[j] dp[i, j] = dp[i - 1, j - 1] + 1 bt[i, j] = ↖ else if dp[i, j - 1] < dp[j, i - 1] dp[i, j] = dp[i - 1, j] bt[i, j] = ← else dp[i, j] = dp[i, j - 1] bt[i, j] = ↑ Figure 3: Longest common subsequence algorithm. 10: Longest Sequences-3\nB D C A B A 0 0 0 0 0 0 0 A 0 0 0 0 1 1 1 B 0 1 1 1 1 2 2 C 0 1 1 2 2 2 2 B 0 1 1 2 2 2 2 D 0 1 2 2 2 2 2 A 0 1 2 2 3 3 3 B 0 1 2 2 3 4 4 Figure 4: Longest common subsequence example dp table. If you’re trying to follow the notes, refer to both this table and the table below which shows how this table was made. B D C A B A · ← ← ← ← ← ← A ↑ ← ← ← ↖ ← ↖ B ↑ ↖ ← ← ← ↖ ← C ↑ ↑ ← ↖ ← ← ← B ↑ ↖ ← ← ← ← ← D ↑ ↑ ↖ ← ← ← ← A ↑ ↑ ← ← ↖ ← ↖ B ↑ ↖ ← ← ↑ ↖ ↖ Figure 5: Longest common subsequence example backtracking table. The arrows in the above table correspond to the backtracking which you could take to get an answer. Note that the arrows the algorithms produce give a strict solution. Runtime: n×m table with O(1) work for each entry, so O(nm) overall runtime and space complexity, where n = |x|, and m = |y|. 3 Longest Palindromic Subsequence Suppose you have as input one string x = a ,...,a and we want to find the longest palin- 1 n dromic subsequence. Notice quickly that this is lcsubsequence(a ,...,a , a ,...,a ). 1 n n 1 We can construct the dp array dp[n][n] with dp[i][j] = largest palindromic subsequence from a ...a . We have the base cases where empty strings and single characters are palin- i j dromic, so ∀i dp[i][i] = 1. If a ,...,a has the fact that a = a , then it could be the ends of a palindrome, but it i j i j depends on a (a ...a )a , so we obtain the recurrence T[i,j] below: i i+1 j−1 j 10: Longest Sequences-4\n(cid:26) (cid:27) 2+T[i+1,j −1], if x = x T[i,j] = i j max(T[i+1,j],T[i,j −1]), if x ̸= x i j def lpalindromesubsequence(x1...x_n): initialize dp as a table of size n by n as 0s for i in 1...(n) dp[i][i] = 1 for s in range 1...(n) for i in range n-s j = i + s if x[i] = x[j] dp[i][j] = 2 + dp[i+1][j-1] else dp[i][j] = max(dp[i+1][j], dp[i][j-1]) Figure 6: Longest palindromic subsequence algorithm. Runtime: n×n table with O(1) work for each entry, so O(n2) overall runtime and space complexity, where n = |x|. 10: Longest Sequences-5"},
{"file": "dsa_slides/01a_Notes.pdf", "content": "CS 3510 Algorithms 1/09/2023 Lecture 1: Introduction, Fibonacci, and Big-O Lecturer: Abrahim Ladha Scribe(s): Saigautam Bonam Why are algorithms so important? Primarily it is a useful, interesting, and foundational theory. It can also help you make a lot of money. Analgorithmisaprocesstocomputesomething. Theruntimeofanalgorithmisthenumber of steps it takes as a function of the input size. 1 Fibonacci Consider the problem of finding the nth Fibonacci number. These are 0,1,1,2,3,5,8,... and so on, beginning from F . There is a well-known recurrence with a base case of F = 0 0 0 and F = 1, and F = F +F . This recurrence gives us an obvious algorithm: 1 n n−1 n−2 def f1(n): if n == 0, 1: return n else: return f1(n - 1) + f1(n - 2) How many steps does this algorithm take? Let T(n) be the number of steps the algorithm takes on input n. Note that the time is dependent on its recursive calls, so T(n) = T(n−1)+T(n−2)+3 Note that 3 is just a constant that describes the amount of extra work outside of the recur- sive calls. We’ll see soon that the actual number doesn’t matter. We have that T(n−1) > T(n−2), then T(n) > 2T(n−2)+3. If we use the same idea, we will get T(n) > 4T(n−4)+3 and so on. This ultimately gets us to the following inequality: T(n) > 2n/2 This means T(n) grows exponentially with respect to n This is very bad and slow!. Practi- cally, I ran this for 43 days to compute F on a server and someone turned off the server 70 before it finished. Slow algorithms don’t simply mean you wait longer for the answer, but you may experience the heat death of the universe before you get an answer. You may also notice by the recurrence we can see that T(n) > F , but the Fibonacci’s also grow n exponentially. 1: Introduction, Fibonacci, and Big-O-1\nF n F F n−1 n−2 F F F F n−2 n−3 n−3 n−4 ... ... ... Note in the recursion tree, we actually recompute a massive amount of what is needed. As a human, this is not the way we compute Fibonacci numbers! We do so iteratively, writing down and re-using previous answers. Our pen-and-paper process of computing the Fibonacci’s is motivation for our next algorithm: def f2(n): if n == 0: return 0 arr = [0] * (n + 1) arr[0] = 0 arr[1] = 1 for i in range(2, n + 1): arr[i] = arr[i - 1] + arr[i - 2] return arr[n] We loop n times, and at each loop iteration we perform one addition. For now let’s overes- timate this addition takes n time (based on the number of bits of each number), and so we get that an upper bound for this algorithm is T(n) < n2. This shows the algorithm is faster than the previous one. Let’s examine another approach to Fibonacci. Consider the following: (cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21) 0 1 0 1 = 1 1 1 1 (cid:20) (cid:21)2(cid:20) (cid:21) (cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21) 0 1 0 0 1 1 1 = = 1 1 1 1 1 1 2 (cid:20) (cid:21)n(cid:20) (cid:21) (cid:20) (cid:21) 0 1 0 F = n 1 1 1 F n+1 Multiplication by by the matrix computes Fibonacci numbers! def f3(n): A = [[0 1][1 1]] v = [0 1] compute An compute Anv return F n 1: Introduction, Fibonacci, and Big-O-2\nT(n) = n−1 matrix multiplications, one vector multiplication and some constant work. T(n) = (n−1)MMs+(1)VM +c Let’s not worry about the costs of a 2×2 matrix multiplication yet, but the number, just for now. Can we reduce the number of matrix multiplications? By repeatedly squaring like A = A·A we only need to perform log(n) matrix multiplications. We will go into this algorithm for exponentiation in lecture three. T(n) = (logn)MMs+(1)VM +c You may recall diagonalization of a matrix. If a matrix A is diagonalizable, there exists D,P where A = PDP−1, where (cid:20) (cid:21) λ 0 D = 1 0 λ 2 We only care about this since An = (PDP−1)n = (PDP−1)(PDP−1)(PDP−1)... = PD(P−1P)D(P−1P)D... = PDnP−1 That’s helpful to us because (cid:20) λn 0 (cid:21) Dn = 1 0 λn 2 Although we don’t know it yet, computing PDnP−1 is much more efficient than computing An. def f4(n): D = ... P = ... P−1 = ... v = ... compute Dn F = PDnP−1v n return F n What is our runtime? we have two matrix multiplications, a vector multiplication, and two integer exponentiations to power n. T(n) = (2)MMs+(1)VM +(2)exps+c We don’t have the tools yet to know this is faster. But the moral here is that a deeper knowledge of mathematical tools can only improve run time. In fact, using this, we can derive a closed formula for F . Rather than plug in n, then compute the matrix product, n compute the matrix produce in terms of some variable n and then simplify it. Computing An = PDnP−1 algebraically, we get the closed form 1: Introduction, Fibonacci, and Big-O-3\n(cid:32) √ (cid:33)n (cid:32) √ (cid:33)n 1 1+ 5 1 1− 5 F = √ − √ n 5 2 5 2 Note that analyzing the runtime becomes more difficult. I want you to think about what it costs to compute powers of roots. 2 Big-O Let us discuss how to better analyze runtime. What is a “basic computer step”? Is it an instruction? We consider the runtime in the size of the input, denoted as n. It could be an array of size n or numbers with n bits. Computing the exact number of steps can be unnecessarily complicated. We use big O notation to get to the heart of the matter. For f,g : N → R, we say f(n) is O(g(n)) if there is a constant c such that f(n) ≤ c(g(n)) for large enough n. Note we care about asymptotics, and y may be less than f on small n. Most texts say f(n) = O(g(n)), but I like to say “is”. This big-O is a property of f rather than its equivalence. f(n) We say f(n) is o(g(n)) (little-oh) if lim = 0. Although this is a formal definition, n→∞ g(n) you may think of f(n) is o(g(n)) to mean that f grows strictly less than g asymptotically. f is never o(f). We say f(n) is Ω(g(n)) if there exists a constant c such that f(n) ≥ c(g(n)) for large enough n. We say that f(n) is Θ(g(n)) if f(n) = Ω(g(n)) and f(n) = O(g(n)). They are matching upper and lower bounds. f(n) We say that f(n) is ω(g(n)) if lim diverges. n→∞ g(n) Althoughyoushouldusetheformaldefinitionsofallofthese, itmaybehelpfultoremember them like this: O → f ≤ g Ω → f ≥ g o → f < g ω → f > g Θ → f = g Recall the following common bounds you might have seen already: O(1),O(logn),O(n),O(nlogn),O(n2),O(2n),O(n!) 1: Introduction, Fibonacci, and Big-O-4\nand more. Note that most algorithms will be at least Ω(n), as it usually takes at least n time to process the input (for example, an array of n integers). Any algorithm that doesn’t look at the whole input may be faster (for example, binary search). 1: Introduction, Fibonacci, and Big-O-5"},
{"file": "dsa_slides/NotesRandomness.pdf", "content": "CS 3510 Algorithms 04/18/2024 Lecture 21: Randomness Lecturer: Abrahim Ladha Scribe(s): Diksha Holla Randomness is a funny thing. The topics of today’s lecture is algorithms which might be really hard to solve without some creativity 1 Driving First algorithm, remembering how to drive. There I was going 17 mph on a 40 mph road in the suburbs when I forgot which foot goes on which pedal. I was approaching a car stopped at a light and had to think fast. I had the choice between the following two algorithms: algorithm 1: algorithm 2: look in the glove box for the manual and just guess. find the page on which pedal does what. Algorithm 1 requires what, logn? n steps? In a constrained scenario I can’t afford that. Choosing alg 1 may seem like the only idea but in the time constraint there is failure (and death) with probability 1. The second alg seems ridiculous but it only takes constant time! It has a failure rate of only 1/2. I just guessed which pedal was gas and which was brake and stopped just in time. The power of randomness! 2 Primality Lets talk about a more realistic problem, primality testing. On input n, determine if n is √ prime or not. There are algorithms that tests all factors up to n. Can we do better? Turns out with the power of randomness we can! Before seeing the next algorithm, try to come up with your own deterministic primality testing algorithm. RecallFermatslittletheorem: prime⇒ap−1 ≡ 1(modp)wecanusethistotestprimality of a general n. def f(n): a = math.random(z_1, ... n-z) if a^(n-1) = 1 (mod n) return true else return false 21: Randomness-1\nThe arithmetic can be done faster using repeated squaring but this at least takes poly- time. If n is prime it always returns true, but if n is not prime it doesn’t always return false. There do exist numbers which may pass this test and not be prime. Carmichael numbers. They are rare but they do exist. Only if less than 10,000. Odds you hit one are negligible but never zero. Okay randomness did enable this faster primality testing algorithm. was there not a fast deterministic primality testing algorithm? It took humanity until 2004!! to find one. [AKS04] uses derandomization techniques to give a deterministic polytime primality testing algorithm. we have known about primality for millennia, and took us this long to find this algorithm. It’s the case that AKS04 is too advanced to present today, but just know it exists and was an achievement. What about algorithms for comparing pi? π = 3.14... This is actually surprisingly hard. Everyone just uses math.pi or something , and assume its stored to sufficient precision. But if you had to compute it, how would you? Recall tan(π) = 1, so compute π = 4tan−1(1) = 4(1−1/3+1/5−1/7+1/9−...) by 4 rules of the Taylor series, we can compute 4tan−1(1). The problem with this is each term takes approx. the same or more operations to compute, but the amount we get to π gets smaller and smaller. For G digits of precision, we need like a billion operations :( Theres a better way. Just guess! Throw a fistfull of darts at the board, whats the probability that it hits? Its proportional to the area of the circle. The probability a dart hits is the area! P[dart hits circle] = darts hit/ total darts ≈ πr2 ≈ π/4 4r2 so we can guess π by throwing darts! lets optimize a bit πr2/4 P[darts hit circle] = darts hit/ total darts = ≈ π/4 r2 21: Randomness-2\na constant speed up to compute the same value. How do you determine if the dart hits the circle though? if d = (x, y), then x2+y2 = r2 is the boundary. For the unit circle and sphere, if x2 +y2 ≤ 1, then its a hit, x2 +y2 > 1 is a miss. as we throw more and more darts, the error rate of out algorithm from approx. π will converge. It will do so slowly, yet faster than compiling taylor series that’s for sure. Okay I admit its not faster than computing the taylor series, and its less accurate, but it generalizes in ways the taylor series solution doesn’t. Here’s a classical geometry way. let circles 1, 2 intersect at points p1, p2. Then we have true triangles of lengths r1, r1, γ r2, r2, x where γ is the distance between p1 and p2. therearecirclearcsaspiepieces. Thisgivesusawaytocomputethesliver, thealmond. steps: 1. find p1, p2 from the radii and centers 2. find distance between p1, p2, called x 3. let t1 = area(r1, r1, x) 4. let t2 = area(r2, r2, x) 5. use c1r1, c2r2 to find θ andθ 1 2 6. sector 1 = θ 2r2 1 1 21: Randomness-3\n7. sector 2 = θ 2r2 2 2 8. return s1 - t1 ts2 - t2 Or, using the method of Monte-Carlo, just guess the area! a little math must be done to determine the bounding box, but that’s not too bad and an overestimate won’t hurt.This method works for this problem really since the question only asks for the digits of precision. One final algorithm. Given a graph G, 2-color the graph so at least half the edges have endpoints with different colors. Guess each node to be either red or blue. Then each edge has a 1/2 chance to be different or same endpoint. If ¿ E/2 edges have different endpoints, halt. else, recolor. Because endpoints of each are chosen randomly, and independently, the chance our first colors is valid is very high. The probability we have successive coloring’s without the property should converge exponentially. 21: Randomness-4"},
{"file": "dsa_slides/npcomplete.pdf", "content": "CS 3510 Algorithms 3/12/2024 Lecture 13: NP-completeness Lecturer: Abrahim Ladha Scribe(s): Himanshu Goyal 1 Introduction This lecture is like my sales pitch to the field of complexity theory. An algorithm is a constructive solution to a problem. For some problem, an algorithm is an upper bound. A problem has complexity. An algorithm has run time. Given a problem, there may exist any number of upper and lower bounds. Those lower bounds can involve deep mathematical relationships between problems (or class of problems). Why are we studying complexity in an algorithm course? Sometimes, those deep mathe- matical relationships are themselves, just algorithms! Example: Let us consider the following problem, discusses its complexity (upper and lower bounds) • Input: Unsorted list of integers • Output: the list of indices as if it was sorted Hereisonepotentialsolution: Wehaveanobviousupperboundonsorting. InO(nlogn) time, sort the array, keeping track of indices and then return indices. Can we do faster? Is there some o(nlogn) algorithm? Suppose to the contrary, there exists an o(nlogn) algorithm, (suppose its O(n) even). Then we given an o(nlogn) sorting algorithm Usingtheindices, youcansortthearrayinlineartime. Sothisgivesao(nlogn)+O(n) = o(nlogn) sorting algorithm, a contradiction. Therefore, the complexity of this problem is Ω(nlogn). This way of solving problems is commonly referred as reduction and is very popular in complexity theory. Itsallaboutthiskindofthinking. Donotthinkaboutsolvingtheproblem. Do not solve the problem. Instead think about how this problem relates to a known problem. All algorithms in this unit will only be reductions, i.e. an algorithm to convert one problem instance into instance of another problem. This is the true moral of this unit. 2 Background A decision problem is a problem which can be phrased as a Yes/No question. For instance, PATH = {⟨G,s,t⟩| there ∃ a path from s to t in G}. Here, it is not asking for the path. It is asking only, if there exists a path. A search problem is one which would actually 13: NP-completeness-1\noutput path. Every search problem obviously has a corresponding decision problem. In our formalisation, we only consider decision problems. Define P to be the class of decision problems decidable in polynomial time. If A ∈ P, ∃ an algorithm on input x, to correctly say yes if x ∈ A and no if x ̸∈ A, ∃ some k > 0 such that this algorithm runs in O(nk) time. Reason 1: The class P captures our intuitive notion of what it means for an algorithm be be “efficient”. If there exists a polynomial time algorithm for a problem, either the problem is ridiculously trivial, or we have some deep mathematical characterization, an understanding of the problem. Think about how all our fast algorithms did things based on properties of the problem they were solving. Most problems have terrible brute force algorithms, most problems seems to be in EXP. Reason 2: P is closed under operations which do not violate our intuition about ef- ficiency. The sum, product, composition of polynomials are all polynomials. If f, g are polynomial time algorithms they are “efficient”. Running f, then running g should also maintain this intuition about “efficiency”. This algorithm has run time f +g, also a poly- nomial. Reason 3: Althoughbythetimehierarchytheorems,theredoexistproblemsofΩ(n99), none of these appear natural or useful. The highest polynomial time algorithm I have personally seen is for the LLL1 algorithm, what it solves is unimportant to us, but its existence was surprising. It solves a very hard problem in O(n8) time, thought to be super polynomial. TheoriginalO(n8)timealgorithmisactuallyunusableinpracticeandispurely a theoretical result. However, now since its poly time, it can be handed off to the engineers. There are many hardcore pruning algorithms which have unclear analysis, but make the LLL algorithm practically instant on any input I could test. Recall why big-O hides those addition and multiplication constants. Its not our job, its someone elses. A poly time algorithm for a problem implies non-trivial intuition about the problem statement. If you can be offered this little insight, you can perhaps expand it to take more. This is why its hard to find practical high degree poly time algorithms. If it gets too high, then its not poly time. Many of the algorithms we studied ran in times O(n),O(nlogn),O(n2) and not much more. A final reason is that most models of computation can simulate each other up to a polynomial degree. Within P, there should be caution. Our concern for this will be around and just outside of P, so we will continue to only use and care about the word-RAM model. 2.1 Examples within P • PATH={⟨G,s,t⟩|there∃apathformstotinG}. BFS/DFScansolvethisproblem in linear time. • RELPRIME ∈ P i.e. RELPRIME = {⟨x,y⟩| gcd(x,y) = 1} since GCD takes polynomial time (cubic in fact) 1https://en.wikipedia.org/wiki/LenstraLenstraLovsz_lattice_basis_reduction_algorithm 13: NP-completeness-2\n3 Complexity Classes NP = Non-deterministic polynomial time. With out getting too muddy into the definition of non-determinism. Lets give an equivalent deterministic definition. A ∈ NP if solutions to problems in A are verifiable in polynomial time. A ∈ P =⇒ ∃ a polynomial time algorithm. The algorithm may take on {⟨G,s,t⟩ and determines ∃ a path or not. A ∈ NP =⇒ ∃ a poly time verifier V which takes on input {⟨G,s,t⟩ the problem instance, and also a witness/certificate/solution say ⟨v ,v ....v ⟩ and determines yes/no, if 1 2 k ⟨v ,v ....v ⟩ is a path in G from v = s to v = t. 1 2 k 1 k If P is the class of decision problems solvable in polynomial time, NP is the class of problems verifiable in polynomial time. The problems in P have solvers, or algorithms, the problems in NP have verifiers, autograders, for them. The problems that a verifier is verifying may not be solvable in polynomial time. But a solution to a problem can be easily checked. There is no discussion on how this solution is obtained. It is given to you by god. To show a problem is in NP, just show how you could write an efficient verifier. It takes as input the problem, but also the answer, and simply checks it. COMPOSITES ∈ NP where COMPOSITES = {n | n is not prime}, why? the problem instance would be some ⟨n⟩, the witness would be the factors p ,...,p . Now, we can com- 1 k ? pute N = p ·p ....·p in polytime. 1 2 k Let us prove one direction of a famously hard problem. Let A ∈ P. We show =⇒ A ∈ NP. Since this is ∀A ∈ P, then we conclude P ⊆ NP. If A ∈ P, ∃ a poly time algorithm f to decide A. We give a poly time verifier V. V on input (Problem p, witness = “”) simply returns f(p). Essentially your verifier just solves the problem! Since f is polytime, so is V obviously. Another way to think: a witness can only speed up computation. Having the answers can only help, so P ⊆ NP. We believe its strict but cannot prove it. We have some, many, thousands of problems in NP, we do not believe to be in P. But we can not prove it. The history of complexity theory is a history of failure. LetEXP=problemssolvableinexponentialtime. Weknow,andwecanproveP ⊊ EXP. There are problems in EXP provably not in P. We also know we can brute to solve the problems in NP, so we see P ⊆ NP ⊆ EXP. Since P ⊊ EXP, one of the P ⊊ NP, NP ⊊ EXP ? must be true. So if you could show NP = EXP, this would imply, P ̸= NP for free. P = NP is then equivalent to question “ Are there any decision problem requiring exponential time to solve which also require super polynomial time to verify?” Let L = SPACE(logn) and PSPACE = (cid:83)∞ SPACE(nk) be defined as polynomial k=0 and logarithmic space respectively. By the space hierarchy theorem, we know L ⊊ PSPACE but we similarly have the chain L ⊆ P ⊆ NP ⊆ PSPACE. If you could prove L = P and NP = PSPACE then you would get P ̸= NP for free! If you could show P = PSPACE , you would get P = NP for free. 13: NP-completeness-3\nIf P ̸= NP, our world looks like Notes/NP_4.pdf From now on, we won’t care about outside NP. Let us zoom in. 13: NP-completeness-4\nNotes/NP_3.pdf 4 NP-Completeness NP-Complete problems are the hardest problems in NP. 13: NP-completeness-5\nNotes/NP_2.pdf Why do we care about NP-Completeness. It depends who you are. To me, it means you can give up working on a problem, or apply NP-complete specific technqiues, such as SMT solvers. How do we prove a problem is NP-Complete: First we define a poly time reduction for two problems, A and B, we say A ≤ B. (A is poly time reducible to B) if exists f which p is computable in polynomial time such that x ∈ A ⇐⇒ f(x) ∈ B Note this implies x ∈/ A ⇐⇒ f(x) ∈/ B. f maps right answers to right answers and wrong answers to wrong answers. 13: NP-completeness-6\nA B f f Intuitively, you should think A ≤ B means “B is harder than A”. B is an upper bound p for A or A is an lower bound for B. If A ≤ B and B ∈ P =⇒ A ∈ P. If A ≤ B then ∃ f such that x ∈ A ⇐⇒ f(x) ∈ B, p p with f computable in poly time. If B ∈ P , the ∃ algorithm for B(x) which runs in poly time, we give a poly time algo for A to prove A ∈ P. algo for A(x): compute f(x) if f(x) in B return true else return false Here we accept x if the algorithm for B accepts f(x). We reject x if the algorithm for B rejects f(x). So this decides A by looking at B. Since f, algo for B are computable in poly time, so this decides A in poly time. To prove B is NP-Complete you show: • B ∈ NP • B is NP-hard: If A is some NP-Complete problem, prove A ≤ B by giving a poly p time reduction form A to B. Cook and Levin2 independently proved ∀A ∈ NP that A ≤ SAT. We will take about p SAT next time, but for us all that matters is that there exists an NP-Complete Problem. To show some B is NP-Complete, pick a candidate NP-Complete problem (like SAT) and show SAT ≤ B. By transitivity, this shows ∀A ∈ NP, A ≤ SAT ≤ B =⇒ ∀A ∈ NP, p p p A ≤ B or that “B is harder than anything in NP”. Combining this with B ∈ NP, you get p “B is the hardest problem in NP” truely one of many. 2https://en.wikipedia.org/wiki/Cook–Levin theorem 13: NP-completeness-7\nWe will have a vast collection of NP-Complete problems by our polynomial time reduc- tions, they all are as hard as each other up to a polynomial factor. A fast algorithm for one is a fast algorithm for all. Prove: SAT (or any other NP-complete problem) ∈ P =⇒ NP = P. We already know P ⊆ NP, so we prove just NP ⊆ P Proof: Let A ∈ NP, we know A≤ SAT. We proved A≤ B and B ∈ P =⇒ A ∈ P. So p p A ∈ P =⇒ NP ⊆ P =⇒ P = NP. 13: NP-completeness-8\nNotes/np_attachment.pdf 13: NP-completeness-9\nNotes/NP_1.pdf 13: NP-completeness-10"},
{"file": "dsa_slides/NotesDynamicProgramming.pdf", "content": "CS 3510 Algorithms 2/20/2024 Lecture 9: Dynamic Programming Lecturer: Abrahim Ladha Scribe(s): Saigautam Bonam, Tejas Pradeep 1 Dynamic Programming Dynamic Programming is often done in two ways either top down (with a recurrence and memoization) or bottom up (with iteration). For this class, we will emphasize bottom up. In either case, we solve small subproblems and store them. This will often cause a space time tradeoff. The difference between Divide-and-Conquer algorithms and Dynamic Programming algorithms is that the subproblems in Dynamic Programming overlap, so it’s inefficient to recompute all of them. Here’s our first DP example: def fib(n): if n <= 1: return 1 else: return fib(n - 2) + fib(n - 1) Figure 1: Fibonacci with little space, but large time complexity. AsanexampleconsidertheFibonaccialgorithmabove. Thealgorithmtakesexponential time with respect to the input. However we can make this faster by using some space and storing intermediate results. Consider using an array and using the former two elements to calculate the current element like in the following algorithm. def fib2(n): if n = 0 return 0 initialize dp as table of size n + 1 with 0s dp[0] = 0 dp[1] = 1 for i in 2..(n+1) dp[i] = dp[i - 1] + dp[i - 2] return dp[n] Figure 2: Fibonacci with some space, but small time complexity. 9: Dynamic Programming-1\nEssentially every dynamic programming solution involves a memory structure, giving a base case on the memory structure, and filling up that memory structure using a recurrence (in this case dp[i] = dp[i−1]+dp[i−2]). 2 Example 1 Problem: Suppose you have n stair steps. You can leap one, two, or three steps at a time. What is the number of combinations, or the number of ways, that you could reach step n? Solution: Create an array T[0...n] where T[i] represents the number of ways to reach step i. Our base cases are T[0] = 1,T[1] = 1 and T[2] = 2. To reach step n, there was some last step. You either jumped 1, 2, or 3 steps, and you jumped from 1, 2, or 3 steps away. That means that the number of ways to go from 0 to n is the sum of the number of ways to go from 0 to n−1, 0 to n−2, and 0 to n−3. This gives us our recurrence of T[i] = T[i−1]+T[i−2]+T[i−3] Implementation: (Pseudocode) def numways(k): initialize dp as table of size n + 1 with 0s dp[0] = 0 dp[1] = 1 dp[2] = 2 for i in 3..(n+1): dp[i] = dp[i - 1] + dp[i - 2] + dp[i - 3] return dp[k] Figure 3: Numways algorithm. The results are as follows: indices 0 1 2 3 4 5 6 7 8 9 results 1 1 2 4 7 13 24 44 81 149 Runtime: The runtime of a dynamic progrmaming algorithm is the size of the table multiplied by the work done to calculate each cell. In this case that is O(n)×O(1) = O(n) 3 Example 2 Suppose you had two operations: 9: Dynamic Programming-2\n• add one • multiply by two How many operations does it take to get from 0 to some k. The results are as follows. indices 0 1 2 3 4 5 6 7 8 9 results 0 1 2 3 3 4 4 5 4 5 It’s not monotonic, and it’s actually a little hard. Note that for 8 it’s a power of two so it takes fewer operations than either of its neighbors. Trivially you could find this in exponential time by trying all possibilities. Let’s write our recurrence. dp[0] = 0 because for 0, you need zero operations. dp[1] = 1 because for 1, you add one to zero. Now we’ll try to multiply by two as much as possible so we’ll add 1 if the element is odd and multiply by two if it is even. We get the following overall recurrence. (cid:26) (cid:27) d(i−1)+1, if i is odd d(i) = (1) d(i/2)+1, if i is even This might require a proof, but it’s simple enough you can likely see why it is what it is. Let’s actually write out the algorithm. def minop(k): initialize dp as table of size k + 1 with 0s dp[0] = 0 dp[1] = 1 for i in 2..(k + 1) dp[i] = dp[i - 1] + 1 if i is even dp[i] = min(dp[i], dp[i / 2] + 1) return dp[k] Figure 4: Minop algorithm. Minimum is a fairly common tool in dynamic programming problems. Usually you choose between two possible recurrences, and the minimum results in your answer for that element in the table. You could try some greedy approach, but it gets hard to prove if that is optimal. 4 Example 3 Problem: House robber. Given an array houses with values H = [h ...h ], you want to 1 n robthem,butyoucan’trobtwoadjacenthousesoralarmswillgooff. Whatisthemaximum amount you can steal? Here’s an example: [2,7,9,3,1] would yield 2+9+1 = 12. 9: Dynamic Programming-3\nSolution: Let’s define our array T[0...n] with T[i] =maximum we can steal from houses 0...i. The base cases are that T[0] = H[0],T[1] = max(H[0],H[1]). Now let’s develop the recurrence. At the next house visited, we can choose to rob or not rob a house. We take the maximum of both scenarios. If we rob the house, than we could not have robbed the previous house, and if we didn’t rob the house, then we could have robbed the previous house. This gives us the recurrence T[i] = max(T[i−2]+H[i],T[i−1]) def houserobber(H): initialize dp as table of size n with 0s dp[0] = H[0] dp[1] = max(H[0], H[1]) for i in 2..n dp[i] = max(dp[i - 2] + H[i], dp[i - 1]) return dp[n - 1] Figure 5: House Robber algorithm For example, given H = [2,7,9,3,1], T = [2,7,11,11,12]. The runtime is O(n) since we have a table of size n with O(1) work at each step. 5 Example 4 Problem: Given n,m, what is the number of paths through an n×m grid from (1, 1) to (n,m) if you can only go down and right? For example, given a 3×3 matrix, there are 6 paths. This can be solved without dynamic programming, just some combinatorics, but let’s do it with DP. Solution: Let’s define array T[1...n][1...m] where T[i][j] =the number of paths from (1, 1) to (i,j). The base cases are that T[1...n][1] and T[1][1...n] equal 1. This is essentially the first row and the first column of the matrix. Now working on the recurrence, for non-base case cells, the number of ways to reach (i,j) is the sum of the number of ways to reach the cell above it and the cell to its left. This is because you can only come to (i,j) from (i−1,j) or (i,j −1). So, the recurrence relation is: dp[i][j] = dp[i−1][j]+dp[i][j −1] Finally, dp[n][m] will give the number of paths from (1,1) to (n,m). 9: Dynamic Programming-4\ndef count_paths(n, m): initialize dp as a table of size (n+1)*(m+1) with 0s for i in 0...n: dp[i][0] = 1 for j in 0...m: dp[0][j] = 1 for i in 1...n: for j in 1..m: dp[i][j] = dp[i-1][j] + dp[i][j-1] return dp[n][m] Figure 6: House Robber algorithm The Time and Space complexity is O(n∗m) 6 Example 5 Problem Similar question as example 4, Find the number of paths to reach cell (n, m). Additionally, the grid has bombs, denoted by an input bombs such that bombs[i][j] = True denoted cell (i,j) has a bomb. Find number of paths from (0,0) to (n,m) such that no bombs are traversed. Unlike example 4, this problem cannot be solved with combinatorics and requires dy- namic programming. Let’s define array T[1...n][1...m] where T[i][j] =the number of paths from (1, 1) to (i,j). Base Cases: 1. If the starting cell (1,1) has a bomb, then there are 0 paths. (cid:40) 0 if bombs[1][1] = True dp[1][1] = 1 otherwise 2. For the first row and first column: (cid:40) 0 if bombs[i][1] = True dp[i][1] = dp[i−1][1] otherwise (cid:40) 0 if bombs[1][j] = True dp[1][j] = dp[1][j −1] otherwise 9: Dynamic Programming-5\nRecurrence Relation: For all cells not on the first row or first column: (cid:40) 0 if bombs[i][j] = True dp[i][j] = dp[i−1][j]+dp[i][j −1] otherwise Thismeansthatifacellhasabomb, nopathscangothroughit. Otherwise, thenumber of paths to a cell is the sum of the paths to the cell above it and the cell to its left. The solution will be in dp[n][m], which gives the number of paths from (1,1) to (n,m) without traversing any bombs. def count_paths_with_bombs(n, m, bombs): initialize dp as a table of size (n+1)*(m+1) with 0s if bombs[0][0]: return 0 dp[0][0] = 1 for i in 1...n: if not bombs[i][0]: dp[i][0] = dp[i-1][0] for j in 1...m: if not bombs[0][j]: dp[0][j] = dp[0][j-1] for i in 1...n: for j in 1...m: if not bombs[i][j]: dp[i][j] = dp[i-1][j] + dp[i][j-1] return dp[n][m] Figure 7: House Robber algorithm The overall time complexity is O(n×m). This is because we’re essentially visiting each cell of the grid exactly once. The space complexity is also O(n×m) due to the “dp” table. 9: Dynamic Programming-6"},
{"file": "dsa_slides/16_Approximations.pdf", "content": "CS 3510 Algorithms: Approximation Algorithms Aaron Hillegass Georgia Tech\n2/27 When Optimal Requires Exponential Time Give up on optimal. We want to find a polynomial time algorithm that gives a good solution that is guaranteed to be within a certain factor of the optimal solution. (Greedy algorithms are a pretty common approach.) The guarantees are the hard part.\n3/27 Load Balancing for soonest completion • You have 𝑚 machines: {𝑀 , 𝑀 , …, 𝑀 }. 1 2 𝑚 • You have 𝑛 jobs requiring different amounts of time: {𝑡 , 𝑡 , …, 𝑡 }. 1 2 𝑛 • Assign jobs to machines: Let 𝐴(𝑖) be the set of all jobs assigned to machine 𝑀 . 𝑖 • The total time for all jobs assigned to machine 𝑀 is 𝑇 = ∑ 𝑡 . 𝑖 𝑖 𝑗 𝑗∈𝐴(𝑖) • Minimize the maximum total time across all machines. • (This problem is NP-hard.)\nWhat would be a reasonable greedy algorithm?\n5/27 Greedy Algorithm for Load Balancing • Sort the jobs in descending order of their processing times • Step through sorted list of jobs: • Figure out which machine currently has the smallest total processing time. • Assign the job to that machine. • If optimal assignment 𝐴∗ has a maximum total time of 𝑇 ∗ . 𝑇 ∗ ≥ 1 ∑ 𝑡 • 𝑚 𝑖∈1…𝑛 𝑖 • 𝑇 ∗ ≥ max 𝑡 𝑖∈1…𝑛 𝑖 What is the worst that our approximation can do?\n6/27 Proof Greedy Algorithm is a 2-approximation We need to show that our solution 𝑇 ≤ 2𝑇 ∗ . Reminder: 𝑇 ∗ ≥ 1 ∑ 𝑡 and 𝑇 ∗ ≥ max 𝑡 𝑚 𝑖 𝑖 𝑖 𝑖 When we added last job to each macine, it had the smallest total 1 processing time. So, the every machine had less than ( ) ∑ 𝑡 . 𝑚 𝑖 𝑖 The last job added to each machine had time 𝑡 ≤ max 𝑡 𝑖 𝑖 𝑖 𝑇 ≤ [ 1 ∑ 𝑡 ] + [max 𝑡 ] ≤ 2 max(( 1 ) ∑ 𝑡 , max 𝑡 ) ≤ 2𝑇 ∗ 𝑗 𝑚 𝑖 𝑖 𝑖 𝑖 𝑚 𝑖 𝑖 𝑖 𝑖\nDid we ever use the fact that we sorted the jobs in descending order of their processing times? Can we get a tighter upper bound?\n3 8/27 Sorting to show a -approximation 2 If 𝑛 ≤ 𝑚, our solution 𝑇 = 𝑇 ∗ (each machine gets one task) else: • 𝑇 ∗ ≥ 2𝑡 𝑚+1 • Let 𝑡 be the last job assigned to a machine that defines 𝑇 . By the 𝑗 assumption, 𝑗 ≥ 𝑚 + 1, thus 𝑡 ≤ 𝑡 ≤ 1 𝑇 ∗ 𝑗 𝑚+1 2 𝑇 ≤ [ 1 ∑ 𝑡 ] + 𝑡 ≤ [ 1 ∑ 𝑡 ] + 1 𝑇 ∗ ≤ 3 𝑇 ∗ 𝑚 𝑖 𝑖 𝑗 𝑚 𝑖 𝑖 2 2\n9/27 Find Central Locations • You have 𝑛 sites: 𝑆 = {𝑠 , …𝑠 } 1 𝑛 • You have distance function 𝑑: • 𝑑(𝑎, 𝑏) = 𝑑(𝑏, 𝑎) ≥ 0 • 𝑑(𝑎, 𝑎) = 0 • 𝑑(𝑎, 𝑏) ≤ 𝑑(𝑎, 𝑐) + 𝑑(𝑐, 𝑏) Given an integer 𝑘, find 𝐶 ⊂ 𝑆 such that • |𝐶| = 𝑘 and • the required radius 𝑟(𝐶) = max 𝑑(𝑠, 𝐶) is minimized.\nWhat’s a reasonable greedy algorithm for this problem?\n11/27 Greedy Algorithm for Finding Central Locations • Pick any site 𝑠 , put it in 𝐶. 0 • While |𝐶| < 𝑘: • Find the site 𝑠 with max 𝑑(𝑠, 𝐶) • Add 𝑠 to 𝐶\n12/27 Proof: Greedy Algorithm is a 2-Approximation To prove: If 𝐶∗ is an optimal solution, 𝑟(𝐶) ≤ 2𝑟(𝐶∗) Proof: Define 𝑟∗ to be the radius of 𝐶∗ . Obviously true if 𝑘 ≥ |𝑆|. Assume that we get a set 𝐶 from the greedy algorithm, but 𝑟(𝐶) > 2𝑟∗ . That is, there is a site 𝑠 such that 𝑑(𝑠, 𝐶) > 2𝑟∗ . Imagine a single step of the algorithm: You are adding a site 𝑐′ to 𝐶′ . 𝑑(𝑐′, 𝐶′) ≥ 𝑑(𝑠, 𝐶′) ≥ 𝑑(𝑠, 𝐶) > 2𝑟∗ So every 𝑐′ more than 2𝑟∗ away from every point in 𝐶. And 𝑠 is more than 2𝑟∗ away from every point in 𝐶. There is no way to cover 𝑘 + 1 points more than 2𝑟∗ from each other with 𝑘 circles of radius 𝑟∗\n∗ 𝐶 The proof works even if is not restricted to the given sites.\n14/27 Weighted Set Cover There is a set 𝑈 and a set of 𝑛 subsets 𝑆 , …, 𝑆 . 1 𝑛 Each subset 𝑆 has a non-negative weight 𝑤 . 𝑖 𝑖 Problem: Find a cover 𝐶 of 𝑈 such that the total cost: ∑ 𝑤 𝑖 𝑆 ∈𝐶 𝑖 is minimized. What is a reasonable greedy algorithm?\n15/27 Greedy Algorithm for Weighted Set Cover • Let 𝑅 = 𝑈, Let 𝐶 be the empty set. 1.1 1.1 • While |𝑅| > 0: 1 1 𝑤 • Find the subset 𝑆 with min 𝑖 𝑖 |𝑆 ∩𝑅| 𝑖 • Add 𝑆 to 𝐶 𝑖 • Remove elements of 𝑆 from 𝑅 𝑖 1 1 Note we can talk about the cost of picking up each element 𝑠: 𝑤 For 𝑠 in 𝑆 , let 𝑐 = 𝑖 𝑖 𝑠 |𝑆 ∩𝑅| 𝑖\n16/27 Harmonic Numbers 𝑛 1 𝐻(𝑛) = ∑ 𝑖=1 𝑖 1 Think of it as an approximation of the area under the curve 𝑦 = from 𝑥 𝑥 = 1 to 𝑥 = 𝑛. ln(𝑛) < 𝐻(𝑛) < ln(𝑛) + 1, so 𝐻(𝑛) ∈ Θ(ln(𝑛))\n𝐻 𝑐 17/27 Relating to 𝑠 For any set 𝑆 , ∑ 𝑐 ≤ 𝐻(|𝑆 |)𝑤 𝑘 𝑠 𝑘 𝑘 𝑠∈𝑆 𝑘 Proof: Let 𝑑 = |𝑆 |. 𝑘 Make a list of all elements in 𝑆 in order by which they are added to 𝑅. 𝑘 𝑠 be an element added at some step 𝑗. At the start of that step 𝑠 , 𝑠 , …, 𝑠 ∈ 𝑅 𝑗 𝑗 𝑗+1 𝑑 𝑤 𝑤 |𝑆 ∩ 𝑅| ≥ 𝑑 − 𝑗 + 1 so 𝑘 ≤ 𝑘 𝑘 |𝑆 ∩𝑅 | 𝑑−𝑗+1 𝑘 Some may have been covered earlier in some 𝑆 with 𝑖 < 𝑗, so 𝑖 𝑤 𝑤 𝑤 𝑤 𝑐 ≤ 𝑖 ≤ 𝑗 ≤ 𝑘 ≤ 𝑘 𝑠 |𝑆 ∩𝑅 | |𝑆 ∩𝑅 | |𝑆 ∩𝑅 | 𝑑−𝑗+1 𝑗 𝑖 𝑗 𝑘 Add up the costs of everything: 𝑑 𝑑 𝑤 𝑤 𝑤 𝑤 ∑ 𝑐 ≤ ∑ 𝑐 ≤ ∑ 𝑘 = 𝑘 + 𝑘 + … + 𝑘 = 𝐻(𝑑)𝑤 𝑠∈𝑆 𝑠 𝑗=1 𝑠 𝑗 𝑗=1 𝑑−𝑗+1 𝑑 𝑑−1 1 𝑘 𝑘\n18/27 The Punch Line Let 𝑑∗ = max |𝑆 | 𝑖 𝑖 Let 𝐶∗ be the optimal solution and let 𝑤∗ be the cost of it. The cost of the greedy algorithm is at most 𝐻(𝑑∗)𝑤∗ . 𝑤∗ = ∑ 𝑤 ≥ ∑ 1 ∑ 𝑐 ≥ 1 ∑ 𝑐 = 1 ∑ 𝑤 𝑆 ∈𝐶∗ 𝑖 𝑆 ∈𝐶∗ 𝐻(𝑑∗) 𝑠∈𝑆 𝑠 𝐻(𝑑∗) 𝑠∈𝑈 𝑠 𝐻(𝑑∗) 𝑆 ∈𝐶 𝑘 𝑖 𝑖 𝑖 𝑘\nPricing Model: Think about how each step contributes to the overall cost.\n20/27 Weighted Vertex Cover Given a graph 𝐺 = (𝑉 , 𝐸) and a weight 𝑤 for each vertex 𝑖, 𝑖 Find a 𝐶 ⊂ 𝑉 such that every edge touches a vertex in 𝐶. Minimize the total weight: ∑ 𝑤 𝑖 𝑖∈𝐶 What is an algorithm that will get you a good result in polynomial time?\n21/27 Thinking Prices Every vertex 𝑖 has a weight 𝑤 . Every edge 𝑒 will have a price 𝑝 . 𝑖 𝑒 We say a price is fair if 𝑤 ≥ ∑ 𝑝 𝑖 𝑖,𝑗 (𝑖,𝑗)∈𝐸 For any cover 𝑆 and a set of nonnegative and fair prices: ∑ 𝑝 ≤ ∑ 𝑤 𝑒 𝑖 𝑒∈𝐸 𝑖∈𝑆 Proof: ∑ ∑ 𝑝 ≤ ∑ 𝑤 𝑒 𝑖 𝑖∈𝑆 𝑒=(𝑖,𝑗) 𝑖∈𝑆 Some edges could be covered by multiple vertices, but each edge is counted at least once: ∑ 𝑝 ≤ ∑ ∑ 𝑝 𝑒 𝑒 𝑒∈𝐸 𝑖∈𝑆 𝑒=(𝑖,𝑗) ∑ 𝑝 ≤ ∑ 𝑤 𝑒 𝑖 𝑒∈𝐸 𝑖∈𝑆\n22/27 Algorithm We say a vertex 𝑖 is paid for if ∑ 𝑝 = 𝑤 𝑒 𝑖 𝑒=(𝑖,𝑗) • Set 𝑝 = 0 for all edges. 𝑒 • While there is an edge where neither endpoint is paid for: • Choose such an edge 𝑒. • Increase 𝑝 until one of its endpoints is paid for 𝑒 • Return all the vertices that are paid for\n23/27 Example a:4 a:4 0 0 3 0 b:3 0 d:3 b:3 0 d:3 0 0 0 0 c:5 c:5 a:4 a:4 3 1 3 1 0 0 b:3 d:3 b:3 d:3 0 0 2 0 c:5 c:5\n24/27 Prices and Cover from Algorithm The cover 𝑆 and the prices 𝑝 generated by the algorithm satisfy: 𝑒 ∑ 𝑤 ≤ 2 ∑ 𝑝 𝑖 𝑒 𝑖∈𝑆 𝑒∈𝐸 Proof: Every vertex in 𝑆 is paid for: ∑ 𝑤 = ∑ ∑ 𝑝 𝑖 𝑒 𝑖∈𝑆 𝑖∈𝑆 𝑒=(𝑖,𝑗) Each edge can be counted at most twice: ∑ 𝑤 = ∑ ∑ 𝑝 ≤ 2 ∑ 𝑝 𝑖 𝑒 𝑒 𝑖∈𝑆 𝑖∈𝑆 𝑒=(𝑖,𝑗) 𝑒∈𝐸\n25/27 Punch Line 𝑆 is a cover and its cost is at most twice the minimum cost of any cover. It is a cover because every edge has at least one end that is paid for and in 𝑆. Let 𝑆∗ be an optimal cover, we know ∑ 𝑝 ≤ 𝑤(𝑆∗). 𝑒 𝑒∈𝐸 𝑤(𝑆) ≤ 2 ∑ 𝑝 ≤ 2 ∑ 𝑤 𝑒 𝑖 𝑒∈𝐸 𝑖∈𝑆∗\n26/27 That's The End • This was a lot for one summer: • Divide and conquer • Graph algorithms • Dynamic programming • Complexity theory and NP-Completeness • MaxFlow and MinCut, Linear Programming, Cryptography • Approximation Algorithms • Next: CS 4510: Automata and Complexity • You did great. Thank you! • Keep in touch.\nQuestions? Slides by Aaron Hillegass Based on lectures by Abrahim Ladha"},
{"file": "dsa_slides/10_NPComplete.pdf", "content": "CS 3510 Algorithms: Complexity and NP-Completeness Aaron Hillegass Georgia Tech\n2/21 Problems Thus far: Problems were given, we found solutions. Today: Working with problems themselves. Goal: To be able to say “There is no way I can find an efficient algorithm to solve this.” Problem: We have defined lower-bounds for only a tiny number of problems. We have a not-very-satisfying workaround\n3/21 Example: Comparison Sorting Problem: Given a list of numbers, put them in ascending order. Solutions: Quicksort, Mergesort, Heapsort Upper-bound? Quicksort gives us 𝑂(𝑛 log 𝑛) Lower-bound? Proof gives us Ω(𝑛 log 𝑛)\n4/21 NP-Complete No one has ever found a polynomial-time solution to any NP-complete problem. If we found a polynomial-time solution to any NP-complete problem, we could solve all NP-complete problems in polynomial time. We believe, but can’t prove, they are Ω(2𝑓(𝑛)) for some 𝑓. Examples: Knapsack problem with non-integer weights, graph coloring, Traveling Saleman Problem, Boolean Satisfiability Problem, Vertex Cover Problem\n5/21 Working with NP-Complete Problems The game today is “Show that your problem is NP-Complete”. What then? • Give up. • Put up with an approximate solution. • Use an SMT solver and hope your cases are easy ones. • Do only small problems, use a lot of hardware, get your constants down.\n6/21 The Worst From “Computers and Intractability: A Guide to the Theory of NP-Completeness” (1979) by Michael Garey and David S. Johnson\n7/21 The Dream\n8/21 Reality\n𝑃 9/21 Decision Problems and Problems Decision problems return True or False. sorting Are 𝑎 and 𝑏 relatively prime? Let us work in set theory: {< 𝑎, 𝑏 > | gcd(𝑎, 𝑏) = 1} Decision Problems Knapsack becomes a decision problem: Capacity is 𝑊, items have weights 𝑤 …𝑤 , and values 𝑣 …𝑣 . Is there a subset of 1 𝑛 1 𝑛 halting items whose total weight is less than or equal to 𝑊 and whose total value is greater than or equal to 𝑉 ? P relprime 𝑃 is the set of all decision problems that can be solved in polynomial time.\nNP 10/21 Verifiable and For the decision version of Knapsack, if the answer is NP True, how could you verify it? Knapsack? We say the data that lets you verify is a “witness”. Usually the witness is just the solution of the non-decision form P of the problem. relprime NP is the set of all decision problems can be verified in polynomial time. 𝑃 ⊂ NP\nYour boss: “I don’t need an array sorted. But I have an array and I want to know the indices of where each element would be if I sorted it.” Can you find an algorithm faster than 𝑂(𝑛 log 𝑛) ?\n12/21 Reductions A reduction is an algorithm that transforms one problem into another. In this setting we will be interested in polynomial time reductions of decision problems. If there exists a polynomial-time reduction of problem 𝐴 to problem 𝐵, we will say “𝐴 ≤ 𝐵”. 𝑝 If we know 𝐴 is a hard problem and 𝐴 ≤ 𝐵, then 𝐵 is also hard. 𝑝\n13/21 NP-Hard NP-Hard NP-hard is a class of problems. NP A problem 𝐻 is NP-hard if every problem in NP can be reduced to 𝐻 in polynomial time. We know: 𝑃 ≠ NP-hard P\n14/21 NP-Complete! NP-Hard 𝐻 is NP-Complete if it is in the intersection of NP and NP-Hard: • A decision problem whose witness can be NP NP-Complete confirmed in polynomial time • Any problem in NP can be reduced (in polynomial time) to 𝐻 P The Cook-Levin Theorem gave us the first: Boolean Satisfiability is NP-Complete.\n15/21 Boolean Satisfiability • Boolean variables: 𝑥 , 𝑥 , …, 𝑥 1 2 𝑛 • Literals: 𝑥 , ¬𝑥 , 𝑥 , ¬𝑥 , …, 𝑥 , ¬𝑥 1 1 2 2 𝑛 𝑛 • Clause: Literals or-ed (𝑥 ∨ ¬𝑥 ∨ ¬𝑥 ∨ 𝑥 ) 1 2 3 5 • CNF Expression: Clauses and-ed (𝑥 ∨ ¬𝑥 ) ∧ (¬𝑥 ∨ 𝑥 ) 1 2 3 5 Given a CNF expression, is there an assignment that satisfies it? Cook-Levin Theorem proves that any problem in NP can be reduced to Boolean Satisfiability in polynomial time. Now we have one problem in NP! How do we get others?\n16/21 Using reductions! SAT 3-SAT Imagine you have a problem 𝐻 in NP. CLIQUE SUBSET-SUM EXACT-COVER If Boolean Satisfiability can be reduced (in VERTEX-COVER KNAPSACK polynomial time) to 𝐻, then 𝐻 is NP-Complete. HAM-CYCLE TSP\n17/21 Reductions of Decision Problems These are decision problems. {A=True} {B=True} Let reduction from problem 𝐴 to problem 𝐵 be 𝑓 a f(a) Let 𝑎 be an instance of the problem 𝐴. Then 𝑓(𝑎) is an instance of the problem 𝐵. a f(a) Then 𝑎 is True in 𝐴 if and only if 𝑓(𝑎) is True in 𝐵. {A=False} {B=False}\n18/21 3-SAT 3-Sat: a special case of the Boolean Satisfiability problem. Clauses can only contain three literals: (𝑥 ∨ ¬𝑥 ∨ ¬𝑥 ) ∧ (¬𝑥 ∨ 𝑥 ∨ ¬𝑥 ) 1 2 3 1 4 5 Can we reduce Boolean Satisfiability to 3-Sat?\n19/21 3-SAT If you have a long clause with 𝑛 literals: (𝑟 ∨ … ∨ 𝑟 ) 1 𝑛 Create a new variable 𝑧 and break the long clause into two: (𝑟 ∨ … ∨ 𝑟 ∨ 𝑧) ∧ (¬𝑧 ∨ 𝑟 ∨ 𝑟 ) 1 𝑛−2 𝑛−1 𝑛 How long is it now?\n20/21 CIRCUIT-SAT x 1 Given a boolean circuit with 𝑛 inputs and a single output, is there an input that makes the x output True? 2 Can we reduce 3-SAT to Circuit-SAT? x 3\nQuestions? Slides by Aaron Hillegass Based on lectures by Abrahim Ladha"},
{"file": "dsa_slides/NotesLPDuality.pdf", "content": "CS 3510 Algorithms 4/16/2024 Lecture 19: LP Duality Lecturer: Abrahim Ladha Scribe(s): Richard Zhang Lasttime, wediscussedwhatlinearprogramming(LP)wasandhowmanydiverseproblems it can represent. Today, we do a far more in depth example and detail the Simplex method, as well as the duality theorems. 1 LP and Simplex In-Depth Recall LP standard form. Given a m×n matrix A , a m×1 vector b, and an n×1 vector c, we find a solution x of size n×1 such that maximize cTx x subject to Ax ≤ b, x ≥ 0 Consider the following linear program: maximize 2x +3x +5x x 1 2 3 subject to x +2x +x ≤ 4, 1 2 3 x ≥ 0, 1 x ≥ 0, 2 x ≥ 0 3 Each constraint adds a geometric figure to our n space (where n is the number of vari- ables). The set of feasible solution to an LP forms a polyhedron in n-space. The constraint x ,x ,x ≥ 0 force us into the positive octant. This forms a triangle based pyramid (pic- 1 2 3 tured below)! 19: LP Duality-1\nNotes/lp_graph.png In order to maximize cTx, we can treat it as a plane where it equals some value z that we control (cTx = z). We take its plane and intersect it with this polyhedron. On some fixed plane with a fixed value z, the points will have the same objective function value, so we seek a maximal place to shift the plane given its slope. This shift is performed by changing the value of z and finding the maximum z that intersects with our polyhedron. It is a guarantee that such a maximum exists at an extreme point (or equal to all points on a line). The simplex algorithm simply iterates over the polyhedron’s external points, increasing the objective function each time until it cannot be increased any more. Here we see the max is at (0,0,4) with a value of 20. It is a feat we are searching for a real number solution but only need to check finitely many extreme points. Let’s now do a more complex example. Suppose the following LP in standard form was given: maximize 3x +x +2x x 1 2 3 subject to x +x +3x ≤ 30, 1 2 3 2x +2x +5x ≤ 24, 1 2 3 4x +x +2x ≤ 36, 1 2 3 x ,x ,x ≥ 0 1 2 3 19: LP Duality-2\nFirst, we put this LP into ”slack form” to convert inequalities to equalities. z = 3x +x +2x 1 2 3 x = 30−x −x −3x 4 1 2 3 x = 24−2x −2x −5x 5 1 2 3 x = 36−4x −x −2x 6 1 2 3 Althoughnotshown,allthevariablesareconstrainedtobenonnegative(x ,x ,x ,x ,x ,x ≥ 1 2 3 4 5 6 0). If we have the constraints in the form Ax ≤ b, then we can express such constaints to be Ax+s = b with s ≥ 0. The slack form would therefore be in the form s = b−Ax and (cid:2) (cid:3)T is shown above. In the example, s = x x x . z would be the value of the objective 4 5 6 function which we are trying to maximize. It will become clear later why the objective function is expressed this way. Denote the variables on the left hand side as basic variables and the variables on the right hand side as nonbasic variables. We need to start somewhere, so we start off with a basic feasible solution that sets all the nonbasic variables to be 0. To satisfying the constraints, the basic variables would be set accordingly, giving us an initial basic solution of (x ,x ,x ,x ,x ,x ) = (0,0,0,30,24,36). 1 2 3 4 5 6 The current objective function value is 0. As we run simplex, we will ”pivot” to increase the current objective function value. This will rewrite our LP by swapping one basic variable for one nonbasic variable to maximize the objective function. Think of the basic variables as those which are loose. An equality constraint is tight if its nonbasic variables get its basic variable to be equal to 0. Wechoosethevariableintherightsideofz whosecoefficientislargestandisnonbasic. This is the entering variable. Then, choose the ”tightest” constraint which we will elaborate morelater. Thebasicvariableassociatedwiththatconstraintwillbetheleaving variable. In this example, x is the entering variable since its coefficient in the objective function is 1 the highest and is a nonbasic variable. As we try to increase the value x and increase the 1 objective function value, we find that x will be the first basic variable that will become 6 zero, specifically when x = 9. This makes the corresponding constraint for x the tightest 1 6 constraint. Another way of seeing it is that each of the constraints provide an upper bound on the value of x . The upper bounds would be 30,12,9 and the constraint with the lowest 1 upper bound (constraint with basic variable x ) is chosen. We then solve for x on this 6 1 constraint and obtain x x x 2 3 6 x = 9− − − 1 4 2 4 Finally, we rewrite the LP by substituting this (basically doing Gaussian elimination) for 19: LP Duality-3\nany occurrence of x on the right side of the LP. What we get is this: 1 x x 3x 2 3 6 z = 27+ + − 4 2 4 x x x 2 3 6 x = 9− − − 1 4 2 4 3x 5x x 2 3 6 x = 21− − + 4 4 2 4 3x x 2 6 x = 6− −4x + 5 3 2 2 Notice that x is now a basic variable and x is now a nonbasic variable. x has ”en- 1 6 1 tered” the group of basic variables, while x left that group. We then create a new 6 solution that again make the current nonbasic variables 0. The current solution is now (x ,x ,x ,x ,x ,x ) = (9,0,0,21,6,0) and has an objective function value of 27. 1 2 3 4 5 6 Nowlet’sdoanother”pivot”operation. Whatisthenextvariablewechooseastheentering variable? It would not be x since it has a negative coefficient in the objective function. x 6 3 has the largest coefficient in the objective function and is nonbasic, so it will become the entering variable. The constraint with basic variable x limits the increase of x the most, 5 3 so x is the leaving variable and we substitute the following: 5 3 3x x x 2 5 6 x = − − + 3 2 8 4 8 The new LP would now be the following after the substitution: 111 x x 11x 2 5 6 z = + − − 4 16 8 16 33 x x 5x 2 5 6 x = − + − 1 4 16 8 16 3 3x x x 2 5 6 x = − − + 3 2 8 4 8 69 3x 5x x 2 5 6 x = + + − 4 4 16 8 16 x is now a basic variable and x is now a nonbasic variable. Again, after setting the cur- 3 5 rent nonbasic variables to be all zero, we have a current solution of (x ,x ,x ,x ,x ,x ) = 1 2 3 4 5 6 (33,0, 3, 69,0,0) and a current objective function value of z = 111. 4 2 4 4 We can only increase x to increase the objective function value. The upper bounds would 2 be 132,4,∞. The constraint with x as the basic variable has an upper bound of ∞ for 4 x since x increases as x increases. The constraint for the basic variable x constraints 2 4 2 3 x the most, so x will enter and x will leave. Performing the same substitution as the 2 2 3 19: LP Duality-4\nprevious ”pivot” operations, we get the following LP: x x 2x 3 5 6 z = 28− − − 6 6 3 x x x 3 5 6 x = 8+ + − 1 6 6 3 8x 2x x 3 5 6 x = 4− − + 2 3 3 3 x x 3 5 x = 18− + 4 2 2 Sinceallthecoefficientsinz arenowallnegative,wecannotcontinuepivotingandincreasing the objective function value. We have reached the optimal solution and maximum objective function value, which is (x ,x ,x ,x ,x ,x ) = (8,4,0,18,0,0) and z = 28. Observe the 1 2 3 4 5 6 values of the original slack variables x ,x ,x . These values determine how much slack, or 4 5 6 difference, between the light and right hand sides of their corresponding constraints. We have now reached the end of the simplex algorithm for this example. Simplex at a high level converts an LP into slack form and keeps pivoting to increase the objective function value until it cannot increase it anymore. The last solution found becomes the final solution returned. 2 Duality How do we know simplex returns the optimum? Recall how a max-flow min-cut solution was the optimum. If a flow for a network was equal to the min-cut of that same network, that flow was the maximum flow. Finding the max flow was a maximization problem and finding the min cut was a minimization problem, but they both have the same optimal objective function value. Similarly, take any LP in standard form and call it the Primal. It hasan”evil”LPcalledaDualwhichminimizesbuthasthesameoptimalobjectivefunction value. Primal Dual maximize cTx minimize bTy x y subject to Ax ≤ b, subject to ATy ≥ c, x ≥ 0 y ≥ 0 Note that the Dual of the Dual is the Primal. They are like brothers. If the Primal has m constraints, the Dual will have m variables. If the Dual has n constraints, the Primal will have n variables. We will prove that their optimal objective values are the same with the Weak Duality Theorems. Theorem (Weak Duality Theorem). Given a primal LP (A,b,c) and its dual, let x be a feasible solution for the primal LP and let y be a feasible solution the dual LP. Then, we have cTx ≤ bTy 19: LP Duality-5\nThe Weak Duality Theorem can show that the max-flow ≤ min-cut in a flow network. Proof: Let n be the number of variables in the Primal and m be the number of variables in the Dual. n n (cid:34) m (cid:35) m (cid:34) n (cid:35) m (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) cTx = c x ≤ (a y )x = (a x )y ≤ b y = bTy j j ij i j ij j j i j j=1 j=1 i=1 j=1 i=1 i=1 Two important results come from the Weak Duality Theorem. 1. If cTx = bTy, then x,y are the optimal solutions for their corresponding LP’s. If a solution x for the Primal has an objective function value equal to bTy, then this must be the highest. Otherwise, a solution with a higher objective function value would be greater than bTy and thus contradict the Weak Duality Theorem. A similar explanation can be made for a solution y for the Dual. 2. IfthePrimalhasasolutionthatisunbounded,thentheDualhasnofeasiblesolutions. IftheDualhasasolutionthatisunbounded,thenthePrimalhasnofeasiblesolutions. Say that the Primal was unbounded, but the Dual had a feasible solution. This feasible solution would have a finite objective function value. However, the Weak DualityTheoremstatesthatthisDualfeasiblesolutionshouldhaveagreaterobjective function value than any solution of the Primal, which is impossible since the possible Primal objective function values are unbounded! Therefore, no feasible solution exists for the Dual. A similar explanation can be made for the second case. Why is simplex optimal? It actually solves both the primal and the dual simultaneously, just like how the max-flow algorithm also finds the min cut. Recall our solution to the example problem was (x ,x ,x ) = (8,4,0) with z = 28. The 1 2 3 final z equation representing the objective function was in the form v′+ (cid:80) c′x . With n as j j the number of original variables in the Primal, each entry in y for the Dual would then be the following: (cid:40) −c′ if x is nonbasic y = n+i n+i i 0 otherwise Applying this to the final z equation in the example (z = 28− x3 − x5 − 2x6), we get that 6 6 3 y = 0,y = 1,y = 2. y = 0 since x = x and x become a basic variable at the end. 1 2 6 3 3 1 1+3 4 4 Plugging these values into bTy gives us 28 which is the same as the maximum objective function value for the Primal. 19: LP Duality-6\n3 A quick way for certifying optimality We can certify that a solution from simplex is the best by applying Gaussian elimination and the rules of algebra. Let’s look at the following LP: maximize x +6x x 1 2 subject to x ≤ 100 (a), 1 x ≤ 300 (b), 2 x +x ≤ 400 (c) 1 2 Simplex returns x = 100,x = 300 with the objective function value as 1900. We can 1 2 use Gaussian elimination by multiplying inequality (b) by 5 and adding it to inequality (c) (5b+c = x +6x ≤ 1500+400 = 1900. With these constraints, we have shown that the 1 2 objective function value cannot exceed 1900. Since the simplex solution has a objective function value of 1900, we know that this solution is the optimum. 4 Quick Remark about the Runtime of Simplex The runtime for the average case is polynomial. However, with some inputs, simplex may run for (cid:0)m+n(cid:1) iterations. Since each iteration runs in O(mn) time, the worst case runtime n would be exponential. 19: LP Duality-7"},
{"file": "dsa_slides/satisfiability.pdf", "content": "CS 3510 Algorithms 3/14/2023 Lecture 14: Satisfiability Lecturer: Abrahim Ladha Scribe(s): Jiaxuan Chen 1 Introduction Let’s review what we did last time. We began our discussion on NP-completeness. To prove some problem B is NP-complete, you should: 1. Prove B ∈ NP by showing it is verifiable in polynomial time. 2. Prove B is NP-hard. That is, A ≤ B. p • Choose some known A which is NP-complete. • Givesomereductionf computableinpolynomialtimesuchthat,foreveryx ∈ A: x ∈ A(is good) ⇐⇒ f(x) ∈ B(is good), x ∈/ A(is bad) =⇒ f(x) ∈/ B(is bad). InordertoproveaproblemisNP-complete,thisdependsonsomeotherknownNP-complete problemexisting. CookandLevinindependentlydidthis. TheyprovedSATisNP-complete without a predecessor. That is, ∀A ∈ NP,A ≤ SAT. Note that, this is true for every prob- p lem in NP. But, what is SAT? A variable is one of x ,x ,...,x . 1 2 n A literal is a variable or its negation x or ¬x . i i A clause is an OR of several literals. A formula in CNF form is an AND of several clauses. For example: (x )∧(¬x ) 1 1 14: Satisfiability-1\nis unsatisfiable. Another example might be satisfiable: (x∨y∨z)∧(x∨z∨w)∧... SAT SAT is extremely universal. Most constraint problems can be made to look like SAT. Each clause is a constraint: every constraint must be satisfied, but they can be satisfied in a number of ways. Let’s say you have to feed everyone. You want either a burger, a gyro or a cheeseburger. My buddy only wants a cheeseburger. Each of us is a constraint. We have variables like you order a burger (b) or gyro (g). Our SAT formula is like: (b∨g∨c)∧c The formula (x ∨¬y )∧(x ∨y )∧···∧(x ∨¬y ) is satisfiable only when x = y ,x = 1 1 2 2 n n 1 1 2 y ,...,x = y . A SAT formula for string equality. 2 n n To be clear, an assignment is a selection of variables x ∈ {0,1}. An assignment satisfies a i given boolean constraint, an assignment satisfies I. SAT Definition: Φ ∈ SAT such that Φ is a formula in CNF form and is satisfiable. Recall Cook and Levin proved L ∈ NP =⇒ L ≤ SAT. So if SAT ∈ P =⇒ NP ⊆ P =⇒ p P = NP. SAT is like an elected representative of the entire class of NP. This is also why we don’t believe there exists a polynomial time algorithm for SAT. kSAT definition: ∃Φ such that Φ is a formula in CNF, satisfiable, each clause has at most k literals. 3SAT Weprovethat3SAT isNP-completebyreduction. First,weshow3SAT ∈ NP. Ourwitness is simply the assignment of variables for the problem instance solution. All these compu- tations can be done in polynomial time. For all Φ(C ,...,C ), check if Φ(C ,...,C ) = 1 1 m 1 m or not. NowweproveSAT ≤ 3SAT. ForageneralSAT formula,weconvertittoa3SAT instance p such that Φ is satisfiable (∈ SAT) if and only if F(Φ) is satisfiable (∈ 3SAT). We describe our reduction F as follows: For an input Φ of every SAT formula has some max clause size k. If k ≤ 3 then Φ is both in SAT and 3SAT. Now suppose Φ has max clause size k > 3. We convert a clause of size k > 3 to a pair of clauses, one of size k−1 and the other of size 3. We add a variable z as follows: (x ∨x ∨···∨x ∨x ) ⇐⇒ (x ∨x ∨···∨x ∨z)∧(x ∨x ∨¬z) 1 2 k−1 k 1 2 k−2 k−1 k 14: Satisfiability-2\nWhere each x is a literal. Note, if the k clause is true, at least one of its literals is true, i so there is a selection of z to make the two clauses true. If the k clause is always false, the two clauses are also always false for any selection of z. Note, it is important this conversion does not change the satisfiability of Φ. Repeat this process, adding dummy variables, until Φ only has clauses of size 3. • Note: Since this does not alter satisfiability, Φ ∈ SAT if and only if F(Φ) ∈ 3SAT. reduction F occurs in polynomial time. • This reduction F occurs in polynomial time. • We conclude: SAT ≤ 3SAT and so, 3SAT is NP-complete. p NotethatsinceCook-LevinshowedusSAT ∈ NP,3SAT ≤ SAT,andwefound3SAT ∈ NP, p 3SAT ≤ 3SAT. Thisimplies3SATisNP-completewithouthavingtorepeattheentireSAT p proof. A simple reduction suffices. It is possible to repeat this reduction for 4SAT, 5SAT, ..., kSAT for any k ≥ 3. What about 2SAT? Actually, 2SAT ∈ P, so if 3SAT ≤ 2SAT, SAT ∈ P and NP = P. p Surely, we don’t believe should happen. Recall (p ⇒ q) ⇔ (¬p∨q). So every 2SAT clause of size two is an implication. (a∨b) ⇔ (¬a ⇒ b), (¬a∨b) ⇔ (a ⇒ b), (a∨¬b) ⇔ (¬a ⇒ ¬b), (¬a∨¬b) ⇔ (a ⇒ ¬b). Create a graph two vertices for each literal, two edges for each clause. If (a∨b) a clause, add edge ¬a → b, ¬b → a. Recall implication is transitive, and a formula is unsatisfiable if and only if ∀x,(x ⇒ ¬x) or (¬x ⇒ x) so ∃ a path in our graph from x to ¬x and from ¬x to x. (x∨y)∧(¬x∨y)∧(¬x∨¬y) x → ¬y ¬y → x ¬x → ¬y ¬y → ¬x x → y y → ¬x If x = 0 ⇒ y = 1, if y = 1 ⇒ x = 0. CircuitSAT Let circuitSAT be defined as the set: circuitSAT = {C | C a boolean circuit with AND/OR/NOT gates and a way to bring output to 1} We prove that circuitSAT is NP-complete. 14: Satisfiability-3\n• First, we show that circuitSAT ∈ NP. The verifier V takes as input ⟨C⟩ and a witness of n bits, and runs ⟨C⟩ on the inputs. The size of the input is obviously polynomial (increasing depth or more gates). • Now, we show that 3SAT ≤ circuitSAT. Let Φ be a 3SAT formula. We create p a boolean circuit with variables x ,...,x and additional input wires for negated 1 k literals. We add one root gate on the next layer. For each clause, add a sub-circuit for the appropriate three. Then, add an ”AND” gate to AND the clauses together. • If Φ ∈ 3SAT, this circuit C = F(Φ) ∈ circuitSAT. • If Φ ∈/ 3SAT, this circuit is also unsatisfiable. • Construction of this circuit obviously takes polynomial time. We conclude that 3SAT ≤ circuitSAT so circuitSAT is NP-complete. p 14: Satisfiability-4"},
{"file": "dsa_slides/99_Cryptography.pdf", "content": "CS 3510 Algorithms: Cryptography and RSA Aaron Hillegass Georgia Tech\n2/39 The Problem\n3/39 One Time Pad Encryption • Alice wants to send Bob a 𝑘-bit message 𝑚. • Alice and Bob both have the same file of 𝑘 random bits 𝑟. • Encryption: Alice sends 𝑚 = 𝑚 ⊕ 𝑟 𝑒 • Decryption: Bob reads 𝑚 = 𝑚 ⊕ 𝑟 𝑒 • Why? 𝑎 ⊕ 𝑏 ⊕ 𝑏 = 𝑎\n4/39 One Time Pad Encryption is totally secure However: • Alice and Bob must have the same file of 𝑘 random bits 𝑟. • The adversary must not have that file. • The random bits must never be used twice: (𝑚 ⊕ 𝑟) ⊕ (𝑚 ⊕ 𝑟) = 𝑚 ⊕ 𝑚 1 2 1 2 (What is an easy way to increase the entropy of 𝑚 ⊕ 𝑚 ?) 1 2\n5/39 Symmetric Encryption (like AES) • Alice and Bob both know a secret key 𝑘 (think 128 or 256 bits). • To send a message 𝑚, Alice encrypts it using 𝑘 • To decrypt the message, Bob decrypts it using 𝑘 • Really, really efficient (AES is often implemented in hardware) • Salted with a random initialization vector so identical messages appear different.\n6/39 Defining Secure A system is secure if the best strategy available to the adversary is to repeatedly guess the key. With a 128 bit key, there are 2128 possibilities. Is it breakable? • How many milliseconds does a single CPU take to check a guess? • How many CPUs do you have? • How many years do you have/\nBut…securely sharing the secret key is really inconvenient.\n8/39 Asymmetric (or “Public Key”) Encryption • Bob has secret key 𝑘 . 𝑠 • He sends public key 𝑘 to Alice. 𝑝 • Alice encrypts 𝑚 using 𝑘 to 𝑚 . 𝑝 𝑒 • Bob decrypts 𝑚 it using 𝑘 . 𝑒 𝑠 • Adversary can’t decrypt. • Algorithms: Rivest–Shamir–Adleman algorithm (RSA) or elliptical curves\n9/39 Doing public key encryption using OpenSSL # Create a 4096-bit private key openssl genrsa -out skey.pem 4096 # Extract a public key openssl rsa -in skey.pem -out pubkey.pem -outform PEM -pubout # Mail the public key to whoever will be decrypting your message # Encrypt the message secret.txt using the private key openssl pkeyutl -encrypt -inkey skey.pem -pubin -in m.txt -out m.enc\nHow does RSA work?\n𝑛 11/39 Ring of Integers Modulo • Denoted ℤ/𝑛ℤ • 𝑎 ≡ 𝑏(mod 𝑛) ⇔ 𝑛 divides 𝑎 − 𝑏 • Members? {0, 1, …, 𝑛 − 1} • Addition is defined and has a zero • Multiplication is defined and has a one • Quiz: What is 3 × 4 in ℤ/5ℤ?\nℤ/𝑛ℤ 12/39 Exponents in 𝑥𝑦 mod 𝑁 1 define modexp(𝑥, 𝑦, 𝑁): 2 if 𝑦 == 0, return 1 𝑦 𝑧 ≔ modexp(𝑥, , 𝑁) 3 2 4 if 𝑦 is even: 5 return 𝑧2 mod 𝑁 6 else: 7 return 𝑥 ∗ 𝑧2 mod 𝑁 𝑂((log(𝑁))2 log(𝑦))\nℤ/𝑛ℤ 13/39 Inverses in Given 𝑎 ∈ ℤ/𝑛ℤ, define the inverse 𝑎−1 to be the member such that 𝑎𝑎−1 = 1 Quiz: What is the inverse of 4 in ℤ/9ℤ? What numbers have an inverse?\n𝑎 ≠ 0 ℤ/𝑛ℤ has an inverse in if and only 𝑎 𝑛 if and are relatively prime. gcd(𝑎, 𝑛) = 1 That is .\n15/39 Euclid’s Algorithm for GCD function gcd(a,b): if b == 0: return a else: return gcd(b, a mod b) Correctness: if 𝑑 | 𝑎 and 𝑑 | 𝑏, then 𝑑 | 𝑎 − 𝑏 Termination: Assume (without loss of generality) that 𝑎 > 𝑏. Note that 0 ≤ 𝑎 mod 𝑏 < 𝑏 < 𝑎; recursion will terminate.\n16/39 Complexity of Euclid’s Algorithm for GCD function gcd(a,b): if b == 0: return a else: return gcd(b, a mod b) • If 𝑎 and 𝑏 are 𝑛 bits, mod operation is 𝑂(𝑛) 𝑎 • When 𝑎 > 𝑏, 𝑎 mod 𝑏 ≤ . Recursion goes 𝑂(𝑛) deep. 2 • Complexity is 𝑂(𝑛2) • Fast!\n17/39 Fermat’s Little Theorem If 𝑝 is prime and 0 < 𝑎 < 𝑝, then 𝑎𝑝−1 = 1(mod 𝑝) Proof: Define set 𝑆 = ℤ/𝑝ℤ − 0 = {1, 2, …, 𝑝 − 1} Define set 𝑎𝑆 = {𝑎1, 𝑎2, …, 𝑎(𝑝 − 1)} ⊂ ℤ/𝑝ℤ Want to show: 𝑎𝑆 = 𝑆. 1. 0 ∉ 𝑎𝑆 2. Each element of 𝑎𝑆 is different\n0 ∉ 𝑎𝑆 18/39 Proving: Suppose 0 ∈ 𝑎𝑆, Then ∃𝑖 such that 1 ≤ 𝑖 ≤ 𝑝 − 1 and 𝑎𝑖 = 0 𝑝 is prime, so ∃𝑎−1 . Then we note 𝑖 = 𝑎−1𝑎𝑖 = 𝑎−10 = 0\n𝑎𝑆 19/39 Proving: Each element of is different Suppose ∃𝑖, 𝑗 ∈ 𝑆 such that 𝑖 ≠ 𝑗 and 𝑎𝑖 = 𝑎𝑗, Then we note 𝑖 = 𝑎−1𝑎𝑖 = 𝑎−1𝑎𝑗 = 𝑗 Shown: 𝑎𝑆 = 𝑆\n20/39 Continuing Proof Consider Π𝑆 = Π𝑎𝑆 Π𝑆 = 1 × 2 × … × (𝑝 − 1) ≡ (𝑝 − 1)! 𝑝 Π𝑎𝑆 = 1𝑎 × 2𝑎 × … × (𝑝 − 1)𝑎 ≡ (𝑝 − 1)!𝑎𝑝−1 𝑝 (𝑝 − 1)! ≡ (𝑝 − 1)!𝑎𝑝−1 𝑝 (𝑝 − 1)! is 1 × 2 × … × 𝑝 − 1. Each term is non-zero. So each has an inverse. We can cancel them out. 𝑎𝑝−1 ≡ 1 𝑝\n21/39 Euler Phi Define 𝜑(𝑁) to be the number of numbers less than 𝑁 that are relatively prime to 𝑁: 𝜑(𝑁) = | {𝑎 | 0 < 𝑎 < 𝑁 and gcd(𝑎, 𝑁) = 1} | 𝑝 is prime? 𝜑(𝑝) =? 𝑝 and 𝑞 are prime? 𝜑(𝑝𝑞) =?\n𝜑(5 × 7) = 4 × 6 22/39 Example: There are 6 multiples of 5: 5, 10, 15, 20, 25, 30. There are 4 multiples of 7: 7, 14, 21, 28. There are 34 numbers less than 35. 34 - 6 - 4 = 24 𝜑(𝑝𝑞) = (𝑝𝑞 − 1) − (𝑝 − 1) − (𝑞 − 1) = 𝑝𝑞 − 𝑝 − 𝑞 + 1 = (𝑝 − 1)(𝑞 − 1)\n𝑝 𝑞 𝜑(𝑝𝑞) = (𝑝 − 1)(𝑞 − 1) and are prime?\n24/39 Euler’s Theorem A generalization of Fermat’s Little Theorem: If 𝑎 and 𝑛 are relatively prime, 𝑎𝜑(𝑛) = 1(mod 𝑛) (FLT? 𝑛 is prime, so 𝜑(𝑛) = 𝑛 − 1)\n25/39 Hardness Assumption • If 𝑝 and 𝑞 are big primes, computing 𝑝𝑞 is easy. • If 𝑁 is the product of two big primes, factoring 𝑁 into 𝑝𝑞 is hard. Not proven! But we have never found a polynomial time algorithm. (Except: Shor’s Algorithm factors 𝑏-bit numbers in 𝑂(𝑏3) on quantum computers.)\n26/39 RSA • Bob generates large primes 𝑝 and 𝑞. • Define 𝑁 = 𝑝𝑞. • Bob calculates 𝜑(𝑁) = (𝑝 − 1)(𝑞 − 1). • Bob creates 𝑒, 𝑑 where 𝑒 = 𝑑−1(mod 𝜑(𝑁)). • (Note 𝑒 and 𝜑(𝑁) must be relatively prime.) • Public key (𝑁, 𝑒) is sent to Alice. • Bob keeps private key: (𝑁, 𝑑).\n27/39 To send message • Alice has message 𝑚. Computes 𝑚𝑒(mod 𝑁), which is sent to Bob. 𝑑 • Bob calculates (𝑚𝑒) (mod 𝑁) which is 𝑚.\n28/39 The adversary? • Has 𝑁, 𝑒, 𝑚𝑒 . • Needs 𝑑 to read message. • To find 𝑑, must compute 𝜑(𝑁). • To find 𝜑(𝑁), must factor 𝑁 into 𝑝 and 𝑞 – which we think is very hard.\n𝑑 (𝑚 𝑒 ) = 𝑚(mod 𝑁 ) 29/39 Prove: We know: 𝑒𝑑 = 1(mod 𝜑(𝑁)) so ∃𝑘 such that 𝑒𝑑 = 𝑘(𝜑(𝑁)) + 1 𝑑 𝜑(𝑁) (𝑚𝑒) = 𝑚𝑒𝑑 = (𝑚𝑘) 𝑚 Note: 𝑚𝑘 and 𝑁 are almost certainly relatively prime 𝜑(𝑁) By Euler’s Theorem: (𝑚𝑘) = 1(mod 𝑁) 𝜑(𝑁) (𝑚𝑘) 𝑚 = 1𝑚(mod 𝑁) 𝑑 Voila! (𝑚𝑒) = 𝑚(mod 𝑁)\n30/39 RSA in practice RSA is really slow. We encrypt the message with a random secret key using AES. We encrypt just the secret key using RSA.\n31/39 Finding multiplicative inverses Remember: “Bob creates 𝑒, 𝑑 where 𝑒 = 𝑑−1(mod 𝜑(𝑁))” • Pick a random candidate for 𝑒 • Relatively prime to 𝜑(𝑁)? If not repeat. • Use Euler’s Extended Algorithm to find 𝑑\n32/39 Extended Euclidean Algorithm For any integers, 𝑎 and 𝑏, ∃𝑥, 𝑦 ∈ ℤ such that 𝑎𝑥 + 𝑏𝑦 = gcd(𝑎, 𝑏) We can find 𝑥, 𝑦, and the gcd using the Extended Euclidean Algorithm. In our case: 𝑒𝑥 + 𝜑(𝑁)𝑦 = 1 𝑒𝑥 = 1(mod 𝜑(𝑁)) So 𝑑 = 𝑥 in ℤ/𝜑(𝑁)ℤ\n33/39 Code function extended_gcd(a, b) (old_r, r) := (a, b) (old_s, s) := (1, 0) (old_t, t) := (0, 1) while r ≠ 0 do quotient := old_r div r (old_r, r) := (r, old_r − quotient × r) (old_s, s) := (s, old_s − quotient × s) (old_t, t) := (t, old_t − quotient × t) output \"x and y are\", (old_s, old_t) output \"greatest common divisor:\", old_r\n34/39 Implementation: Finding Big Primes Generate a big random number, check to see if it is prime. If not, repeat. How can you be pretty sure it is prime? Use Fermat’s Little Theorem: • Checking 𝑝? Come up with an 𝑎 that is big, but smaller than 𝑝. • Does 𝑎𝑝−1 = 1(mod 𝑝)? 𝑝 is probably prime. • Repeat for several values of 𝑎 Carmichael Numbers: Numbers that are not prime but satisfy 𝑎𝑝−1 = 1(mod 𝑝) for all 𝑎 relatively prime to 𝑝. Incredibly rare: 1 in 50 trillion. Also: the Seive of Eratosthenes and Miller–Rabin.\n𝜋 35/39 Estimating with randomness To find 𝜋: • Create billion (𝑥, 𝑦) pairs where they are both between 0 and 1. • Count how many are inside the unit circle. Multiply by 4. Divide by a billion.\n𝜋 36/39 Estimating 𝜋 Remember tan( ) = 1: 4 • 𝜋 = 4 tan−1(1) Approximate tan−1(1) using a Taylor series. • tan−1(1) = 1 − 1 + 1 − 1 + 1 − 1 + … 3 5 7 9 11 • Called “Leibniz’s formula for 𝜋”\n37/39 Cryptographic Hash Functions • A hash function takes any number of bits and outputs 𝑛 bits (where 𝑛 is usually 256 to 512 bits – too big to guess, small enough to store or transmit or use as a key in a dictionary. • If you have the output, it is very, very difficult to come up with an input that generates it. • Uses: passwords, digital signatures, file integrity, etc. • MD5, SHA-1, SHA-2, SHA-3, BLAKE2, • Slow hash functions: bcrypt\n38/39 Zero Knowledge Proofs The 3-coloring problem is NP-complete. How could you prove that you had a 3-coloring without revealing the coloring itself? All NP-Complete problems can be reduced to 3-COLOR.\nQuestions? Slides by Aaron Hillegass Based on lectures by Abrahim Ladha"},
{"file": "dsa_slides/Notes3colorMario.pdf", "content": "CS 3510 Algorithms 11/01/2023 Lecture 18: 3-Coloring and Mario Lecturer: Abrahim Ladha Scribe(s): Jiaxuan Chen 3-Coloring We did an intro lecture on complexity. We did a lecture on NP-complete problems with boolean formulas. We did another on NP-complete problems with graphs. Then we did a lecture on NP-complete problems with constraint-like problems like subset-sum and knap- sack. In this lecture we will do puzzles and (single-player) games. YoumayhavesomeintuitiononhowthebestalgorithmsyoucanthinkofforSATarereally checking all assignments in 2n time. This may feel like sudoku, for some hard puzzles. If you’ve seen that most similar puzzles are NP-complete. Note, that most two-player games are not NP-complete. 3COL = {⟨G⟩ | G admits a 3 coloring} A graph is 3-colorable if you can color the vertices using three colors such that no adjacent vertices share the same color. Red Red ? Yellow Blue Yellow Blue K is not 3-colorable. K is 3-colorable. 4 3 3COL is NP-complete, while it may be tempting to try to reduce from a graph problem such as Clique, we reduce from 3SAT. For any 3CNF formula, we shall construct a graph that admits a 3-coloring if and only if the formula was satisfiable. First we show 3COL ∈ NP. Our verifier on input problem G and witness a coloring of the variables checks in polytime if there are 3 colors and if the coloring has the property that no two adjacent vertices have the same color. 18: 3-Coloring and Mario-1\nNow we transform Φ into a graph, we want our graph to replicate the 3CNF structure, one vertex for each x and ¬x , i = 1...n. And each clause C is satisfied. We use three colors; i i i tangerine for true, fuchsia for false, and I choose sapphire for “secret third color”. We will use color S only to control colors T and F. To have the first property, that each of x , ¬x i i is T for each x , we build the following graph. i S x ¬x x ¬x x ¬x x ¬x 1 1 2 2 3 3 n n Convince yourself x , ¬x cannot both be color T or both F. None can be color S. This i i simulates that every variable has an assignment, and that x = 1 ⇐⇒ ¬x = 0. i i This is not the entire graph, but it does force an assignment to exist. Now we force the rest of our graph such that every single clause is satisfied. We will build an OR gadget so that we can tie the colors of the clause together. Later we check if the AND clauses fit the color T. so if x = x = x = F this graph has no coloring of the output node except F. i k j 18: 3-Coloring and Mario-2\nIf one of x ,x ,x is true, then there exists a coloring with the output node to be true. i k j Doesn’t matter that there are colors that don’t have the output node as true. As long as such a coloring exists. We can force the coloring we want. Remember the OG triangle: We map the output node of each OR gadget to the F and S colored ones of the original triangle. This forces the 3coloring of the graph to have all output nodes for some color, essentially AND’ing them together. We now prove our reduction. It may have been obvious since we had to explain it. If Φ is 3SAT, there is a satisfying assignment of n variables. Color the corresponding x i vertices accordingly and the rest of the colors essentially collapse. Any false coloring will not propagate, hence ⟨G⟩ ∈ 3COL since there exists such coloring. If Φ is not 3SAT, there is at least one unsatisfied clause. No true for the OR gadget corre- spondstothisclause, everypossibleoutputnodeofthatgadgetmustbethefalsecolor. The structure means no other can be gadget true color. If the gadgets were gadget true color and gadget true for all clauses were false, it was not 3SAT, it would have been satisfiable. So ⟨G⟩ ̸∈ 3COL. QED: G is constructible in polytime, few for loops over Φ. We see 3SAT ≤ 3COL since 3SAT is NP-complete and 3COL is NP-complete. As NP is p closed under ≤ , 3COL must also be NP-complete. p List of NP-complete games: • Mario (probably not in NP) • Battleship • Sudoku on a n×n grid with n blocks • Inverse (NP-hard, probably not in NP) 18: 3-Coloring and Mario-3\n• Minesweeper • Mastermind • Freecell Solitaire • Tetris NP-completeness of Mario Let’s prove Mario is NP-complete. This comes from a great paper series by Erik Demaine. I have recently been told of a good video explanation of this reduction as well. Seems a Mario SAT NP-hard or something on YouTube. We prove 3SAT ≤ MARIO. We construct p our level (that is, we prove 3SAT ≤ MARIO). p Basically,eachvariablegadgetcanonlychooseoneofx or¬x . Thenwe’llfeedamushroom i i into each clause gadget, hence can only go from each one clause to the next if we have a mushroom. 18: 3-Coloring and Mario-4\nVariable Gadget He can only choose one, but he needs mushrooms. Needs at least one mushroom to get through the fire plant. If Φ is 3SAT, then the level is satisfiable, Mario takes the choices according to the exact assignment of the variables. So we see L ∈ MARIO. If Φ is not 3SAT, some clause is always unsatisfied. Mario will get stuck in this clause gadget forever and die with no mushroom, so we see L ∈/ MARIO. Thus, Φ is 3SAT ⇔ L ∈ MARIO. This transformation takes polynomial time so we observe 3SAT ≤ MARIO and that Mario is NP-hard. p Why is this formulation of Mario not in NP-complete? Why is Mario not in NP? Turns out this formulation of any NP-hard is not known to be NP-complete. While it is seductive to consider a witness for a verifier as a sequence of button pushes, the verifier runs in polytime only iff the witness is in P is polynomially sized, which we don’t know is true. 18: 3-Coloring and Mario-5"},
{"file": "dsa_slides/06_MSTs (1).pdf", "content": "CS 3510 Algorithms: Finding Minimum Spanning Trees with Greedy Algorithms Aaron Hillegass Georgia Tech\n2/26 Minimum Spanning Subgraph? You have a connected undirected weighted graph 𝐺 with a finite number of vertices. You want to find a connected subgraph 𝑇 that contains all vertices of 𝐺 and has minimum total edge weight.\n3/26 Trees Given a connected undirected graph 𝐺, the following are equivalent: 1. 𝐺 is a tree 2. 𝐺 has |𝑉 | − 1 edges 3. 𝐺 has no cycles Why is the minimum spanning subgraph always a tree?\n4/26 Example\n5/26 Example\n6/26 Kruskal's Algorithm 1 1 define kruskals(𝐺): edges ≔ sorted_by_cost(𝐺.𝐸) 2 3 forest ≔ 𝐺.𝑉 , no edges 4 while len(forest) > 1: (𝑢, 𝑣) ≔ pop_head(edges) 5 6 if 𝑢 and 𝑣 are in different trees: 7 create an edge between 𝑢 and 𝑣 8 return forest[1] Sort? 𝑂(|𝐸| log|𝐸|) Loop? 𝑂(|𝐸|) × check + 𝑂(|𝑉 |) × merge\n7/26 DisjointSet Three functions: • create_djset(𝑛): Creates a disjoint set with integers 1..𝑛, each as its own set • find(𝑥): Finds the representative of the set containing 𝑥, such that find(𝑥) = find(𝑦) if and only if 𝑥 and 𝑦 are in the same set. • union(𝑥, 𝑦): 𝑥 and 𝑦 are representatives of two sets. This merges them into one set. s := create_djset(5) set_a := s.find(3) set_b := s.find(4) if set_a != set_b: s.union(set_a, set_b)\n8/26 Kruskal's Algorithm 2 1 define kruskals(𝐺): edges ≔ sorted_by_cost(𝐺.𝐸) 2 trees ≔ create_djset(|𝐺.𝑉 |) 3 tree_edges ≔ [] 4 5 while trees.count > 1: (𝑢, 𝑣) ≔ pop_head(edges) 6 u_set ≔ trees.find(𝑢) 7 v_set ≔ trees.find(𝑣) 8 9 if u_set does not equal v_set: trees.union(u_set, v_set) 10 tree_edges.append((𝑢, 𝑣)) 11 12 return trees_edges\n9/26 Disjoint Set s := create_djset(5) set_a := s.find(3) set_b := s.find(4) if set_a != set_b: s.union(set_a, set_b) s.union(s.find(3),s.find(2))\n10/26 Disjoint Set 1 define create_djset(𝑛): parents ≔ new array(𝑛) 2 3 forall i in 1..n: parents[𝑖] ≔ 𝑖 4 count ≔ 𝑛 5 6 return (parents, count) 1 define find(𝑠, 𝑥): 1 define union(𝑠, x_root, y_root): 2 while 𝑥 does not equal 𝑠.parents[𝑥]: 𝑠.parents[x_root] ≔ y_root 2 𝑥 ≔ 𝑠.parents[𝑥] 3 𝑠.count ≔ 𝑠.count − 1 3 4 return 𝑥\n11/26 Path Compression A bushy tree is so much better find(1) than a linked list… 1 define find(𝑠, 𝑥): parent ≔ 𝑠.parents[𝑥] 2 3 if 𝑥 = parent, return 𝑥 root ≔ find(𝑠, parent) 4 𝑠.parents[𝑥] ≔ root 5 6 return root\n12/26 How fast is a disjoint set? With a little bookkeeping to guarantee the tree is bushy… Insert: 𝑂(1) Union: 𝑂(1) Find: 𝑂(𝛼(𝑛)) < 𝑂(log log 𝑛) (amortized)\n13/26 Complexity of Kruskal's 1 define kruskals(𝐺): edges ≔ sorted_by_cost(𝐺.𝐸) 2 trees ≔ create_djset(|𝐺.𝑉 |) 3 tree_edges ≔ [] 4 5 while trees.count > 1: (𝑢, 𝑣) ≔ pop_head(edges) 6 𝑂(|𝐸| log|𝐸|) u_set ≔ trees.find(𝑢) 7 v_set ≔ trees.find(𝑣) 8 9 if u_set does not equal v_set: trees.union(u_set, v_set) 10 tree_edges.append((𝑢, 𝑣)) 11 12 return trees_edges\nIs there another way to find the minimum spanning tree?\n15/26 Kruskal's vs Prim's Kruskal: “Make each vertex a tree. Merge trees that are close to each other until there is one tree.” Uses Disjoint Set. Prim: “Start with a single vertex as a tree. Add closest vertex to the tree until all vertices are in the tree.” Uses Priority Queue.\n16/26 Prim's Example\n17/26 Prim's Algorithm 1 define prims(𝐺): 2 𝑣 ≔ some vertex in 𝐺.𝑉 3 edges ≔ [], 𝑞 ≔ priority queue 4 closest ≔ new map 5 for 𝑢 in (𝐺.𝑉 − 𝑣), closest[𝑢] ≔ 𝑣 6 while closest.count > 0: 7 for ({𝑣,child},cost) in 𝐺.𝐸: 8 if child ∈ closest and 𝑞.decrease(child,cost): closest[child] ≔ 𝑣 9 𝑣 ≔ 𝑞.pop_min() 10 edges.append({closest[𝑣],𝑣}) 11 12 delete 𝑣 from closest 13 return edges\n18/26 Complexity of Prim's 1 define prims(𝐺): 2 𝑣 ≔ some vertex in 𝐺.𝑉 3 edges ≔ [], 𝑞 ≔ priority queue 4 closest ≔ new map 5 for 𝑢 in (𝐺.𝑉 − 𝑣), closest[𝑢] ≔ 𝑣 6 while closest.count > 0: 𝑂(|𝐸| log|𝑉 |) 7 for ({𝑣,child},cost) in 𝐺.𝐸: 8 if child ∈ closest and 𝑞.decrease(child,cost): closest[child] ≔ 𝑣 9 𝑣 ≔ 𝑞.pop_min() 10 edges.append({closest[𝑣],𝑣}) 11 12 delete 𝑣 from closest 13 return edges\n𝐺 Imagine is not connected. What happens when you run Prim’s? Kruskal’s?\n20/26 Always be suspicious of greedy algorithms! • Most greedy algorithms always get you a solution. • Few greedy algorithms are guaranteed to get you an optimal solution! • But I claim Kruskal’s and Prim’s will. Better check it.\n21/26 Cuts Given an undirected graph 𝐺 = (𝐸, 𝑉 ). A cut is a partition of 𝑉 into two non-empty disjoint sets 𝐴 and 𝐵 such that 𝐴 ∪ 𝐵 = 𝑉 and 𝐴 ∩ 𝐵 = ∅. The cut-set of 𝐴 and 𝐵 is defined as the set of edges that have one endpoint in 𝐴 and the other in 𝐵. Given a spanning tree 𝑇 and a cut, at least one edge of 𝑇 is in the cut-set.\n22/26 The Cut Property Let 𝑋 be some edges from a minimum spanning tree of 𝐺. Let (𝐴, 𝐵) be a cut such that no edges of 𝑋 are in the cut-set. Let 𝑒 be the cheapest edge in the cut-set. Then 𝑋 ∪ {𝑒} is part of some minimum spanning tree of 𝐺\n23/26 Cut Property Proof Let 𝑋 ⊂ 𝑇, where 𝑇 is a MST of 𝐺. If 𝑒 is in 𝑇, done. So assume 𝑒 is not in 𝑇. Add 𝑒 to 𝑇 – introduces a cycle across the cut. Call the other edge involved 𝑒′ Create 𝑇′ by adding 𝑒 to and removing 𝑒′ from 𝑇. Removing an edge from a cycle doesn’t disconnect a graph. So 𝑇′ is a tree and has the same number of edges as 𝑇, so it is a spanning tree. Is it minimal? 𝑤(𝑇′) = 𝑤(𝑇) − 𝑤(𝑒′) + 𝑤(𝑒) ≤ 𝑤(𝑇). 𝑇 was minimal, so 𝑇′ must be.\n24/26 Proof of Prim's Prim’s starts with a single vertex 𝑣 and adds new vertices 𝑣 , 𝑣 , …, 𝑣 . 0 1 2 𝑛 Inductive proof: The single vertex with no edges is obviously a subset of an MST. At each step 𝑖, the cut is 𝐴 = {𝑣 , …, 𝑣 } and 𝐵 = 𝑉 − 𝐴. 0 𝑖 If we assume the edges gathered are part of an MST, when we add the cheapest edge 𝑒 in the cut, the result is part of an MST. When all the vertices are in 𝐴, the edges are spanning. Thus we have an MST.\n25/26 Proof of Kruskal's Kruskal’s starts with the empty set and adds the shortest edge that connects two trees. Inductive proof: The empty set is obviously a subset of an MST. Every step in Kruskal’s adds an edge that connects two trees, 𝑇 and 𝑇 . 1 2 Assume that before the step the edges gathered are part of an MST. The edge we add is the cheapest edge in the cut-set for (𝑇 , 𝑉 − 𝑇 ). 1 1 Thus the new set of edges is also part of an MST. We continue until all the vertices are in a single tree. Thus it spanning, thus an MST.\nQuestions? Slides by Aaron Hillegass"}
]